<!DOCTYPE html>
<html>
<head>
<title>Thu May 6, 2021 : #software (osg)</title>
</head>
<body>
<h3>Thu May 6, 2021 : #software (osg)</h3>
<span style="color: #235e5b"><span style="font-size: small">(10:22:06)</span> <b>dweitzel:</b></span> I'm considering dropping python2 support for Scitokens library.  Anyone have an opinion on that?<br/>
<span style="color: #43761b"><span style="font-size: small">(10:22:57)</span> <b>blin:</b></span> i'm all for it<br/>
<span style="color: #73769d"><span style="font-size: small">(10:31:04)</span> <b>tim.theisen:</b></span> @dweitzel Since you are working on sci-tokens cpp, can you merge my pull request too.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:31:51)</span> <b>dweitzel:</b></span> merged!<br/>
<span style="color: #73769d"><span style="font-size: small">(10:32:15)</span> <b>tim.theisen:</b></span> Any idea when you might cut a new release? I think we need some patches for Debian 11 (bullseye)<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:33:08)</span> <b>dweitzel:</b></span> Duncan also added some scitokens-cpp.  I could cut a release soon.<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:21:04)</span> <b>justas.balcas:</b></span> Hi,<br/>I have a bit unclear bug/issue in HTCondor. Some background:<br/>1. We have cronjob in HTCondor to run bash script. Bash script is pretty simple, check if hdfs, ceph, cvmfs is ok and declare node good/bad. I use this information for matching - so bad nodes are out of game.<br/>2. Also there is system cron - to look at all jobs .job_ad file and to know exactly what WN is running; This one mainly to monitor what jobs are running and what they read/access.<br/>And we expect all this output in WN ClassAds. It works for some time (some nodes do not have issue for several months) - but some (big nodes with many cores- like 160cores) - fail to update classads after some time. From HTCondor StartLog I see this:<br/><pre>[root@blade-03-01 condor]# cat StartLog | grep -i 'CronJob'<br/>05/03/21 12:23:06 CronJob: Job 'LOCALCHECK' is still running!<br/>05/03/21 13:17:07 CronJob: Job 'LOCALCHECK' is still running!<br/>05/03/21 13:29:07 CronJob: Job 'LOCALCHECK' is still running!<br/>05/03/21 18:37:17 CronJob: Job 'LOCALCHECK' is still running!<br/>05/04/21 07:15:25 CronJob: Job 'LOCALCHECK' is still running!<br/>05/04/21 07:23:25 CronJob: Job 'LOCALCHECK' is still running!<br/>05/04/21 08:09:26 CronJob: Job 'LOCALCHECK' is still running!<br/>05/04/21 08:21:26 CronJob: Job 'LOCALCHECK' is still running!<br/>05/05/21 02:29:41 CronJob: Job 'LOCALCHECK' is still running!<br/>05/05/21 02:33:41 CronJob: Job 'LOCALCHECK' is still running!<br/>05/05/21 06:09:43 CronJob: Job 'LOCALCHECK' is still running!<br/>05/05/21 22:03:48 CronJob: Job 'LOCALCHECK' is still running!<br/>05/06/21 07:33:52 CronJob: Job 'LOCALCHECK' is still running!<br/>05/06/21 07:35:52 CronJob: Job 'LOCALCHECK' is still running!</pre><br/>And If I check StartD ClassAds, last time classads were added was right before the first error. <br/>LOCALCHECK_CMS_CONDOR_SCAN = 1620025322 #Monday, May 3, 2021 12:02:02 AM <a href="https://www.epochconverter.com/timezones?q=1620025322">GMT-07:00</a> DST<br/>StartLog:<br/><pre>05/03/21 08:12:54 Unable to calculate keyboard/mouse idle time due to them both being USB or not present, assuming infinite idle time for these devices.<br/>05/03/21 09:12:56 Unable to calculate keyboard/mouse idle time due to them both being USB or not present, assuming infinite idle time for these devices.<br/>05/03/21 10:12:59 Unable to calculate keyboard/mouse idle time due to them both being USB or not present, assuming infinite idle time for these devices.<br/>05/03/21 11:13:01 Unable to calculate keyboard/mouse idle time due to them both being USB or not present, assuming infinite idle time for these devices.<br/>05/03/21 12:13:06 Unable to calculate keyboard/mouse idle time due to them both being USB or not present, assuming infinite idle time for these devices.<br/>05/03/21 12:23:06 CronJob: Job 'LOCALCHECK' is still running!<br/>05/03/21 13:13:10 Unable to calculate keyboard/mouse idle time due to them both being USB or not present, assuming infinite idle time for these devices.<br/>05/03/21 13:17:07 CronJob: Job 'LOCALCHECK' is still running!<br/>05/03/21 13:29:07 CronJob: Job 'LOCALCHECK' is still running!<br/>05/03/21 14:13:13 Unable to calculate keyboard/mouse idle time due to them both being USB or not present, assuming infinite idle time for these devices.<br/>05/03/21 14:14:29 slot1_1: Called deactivate_claim()</pre><br/>So nothing really in the logs…<br/>I also from both crons write to system file under /tmp/ (not only echo to HTCondor) and I can see that logs are update on file, but do not appear on HTCondor classads.<br/>I also tried to make htcondor cron script to fail (permission error) and HTCondor keeps running cron, but not outputing anything into StartD Classads:<br/><pre>05/06/21 09:23:53 Can't insert 'LOCALCHECK_rm: cannot remove '/tmp/CONDOR_STORAGE_TEST.ROOT': Operation not permitted' into 'LOCALCHECK' ClassAd<br/>05/06/21 09:23:53 Can't insert 'LOCALCHECK_rm: cannot remove '/tmp/CONDOR_STORAGE_TEST.ROOT': Operation not permitted' into 'LOCALCHECK' ClassAd<br/>05/06/21 09:25:53 Can't insert 'LOCALCHECK_rm: cannot remove '/tmp/CONDOR_STORAGE_TEST.ROOT': Operation not permitted' into 'LOCALCHECK' ClassAd<br/>05/06/21 09:25:53 Can't insert 'LOCALCHECK_rm: cannot remove '/tmp/CONDOR_STORAGE_TEST.ROOT': Operation not permitted' into 'LOCALCHECK' ClassAd</pre><br/><br/>So the questions are:<br/>Why HTCondor is not updating classads? Who keeps the lock and how to check it?<br/>Can it be due to very long ClassAds? What is the character limit for a ClassAd and can this be increased? There are errors like this in StartLog:<br/>Can’t insert ‘LOCALCHECK_DESIRED_CMSDataset = “[’/QCD_Pt-1000toInf_MuEnrichedPt5_TuneC………… into ‘LOCALCHECK’ ClassAd<br/><br/>HTCondor Cron config:<br/><pre>STARTD_CRON_JOBLIST = $(STARTD_CRON_JOBLIST) LOCALCHECK<br/>STARTD_CRON_LOCALCHECK_EXECUTABLE = $(MODULES)/condor_cron_script.sh<br/>STARTD_CRON_LOCALCHECK_PREFIX = LOCALCHECK_<br/>STARTD_CRON_LOCALCHECK_MODE = Periodic<br/>STARTD_CRON_LOCALCHECK_PERIOD = 2m<br/>STARTD_CRON_LOCALCHECK_JOB_LOAD = 0.2</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(12:23:59)</span> <b>blin:</b></span> on the problematic nodes, do you see any old <tt>condor_cron_script.sh</tt> processes running?<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:27:26)</span> <b>justas.balcas:</b></span> None old processes:<br/><pre>root@admin-2 downtime]# pssh -h stale-nodes -l root -A -i "ps auxf | grep condor_cron_*"<br/>Warning: do not enter your password if anyone else has superuser<br/>privileges or access to your account.<br/>Password:<br/>[1] 10:26:49 [SUCCESS] compute-03-274.tier2<br/>root      625208  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root      625226  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[2] 10:26:49 [SUCCESS] compute-13-151.tier2<br/>root     3018136  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     3018157  0.0  0.0 112812   956 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[3] 10:26:49 [SUCCESS] compute-10-272.tier2<br/>root     1772730  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     1772748  0.0  0.0 112812   956 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[4] 10:26:49 [SUCCESS] compute-10-274.tier2<br/>root     3449654  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     3449672  0.0  0.0 112812   956 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[5] 10:26:49 [SUCCESS] compute-03-272.tier2<br/>root     3225837  0.0  0.0 113284  1560 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     3225855  0.0  0.0 112812   956 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[6] 10:26:49 [SUCCESS] compute-03-271.tier2<br/>root     2874506  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     2874524  0.0  0.0 112812   956 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[7] 10:26:49 [SUCCESS] compute-13-172.tier2<br/>root     1462881  0.0  0.0 113284  1560 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     1462901  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[8] 10:26:49 [SUCCESS] compute-03-273.tier2<br/>root     3123836  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     3123855  0.0  0.0 112812   956 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>condor   3123824  0.0  0.0 113500  1440 ?        S    10:26   0:00      \_ /bin/bash /opt/dist/compute/condor_cron_script.sh<br/>condor   3123826  0.0  0.0 113500   604 ?        S    10:26   0:00          \_ /bin/bash /opt/dist/compute/condor_cron_script.sh<br/>[9] 10:26:49 [SUCCESS] compute-10-273.tier2<br/>root     1947680  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     1947698  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[10] 10:26:49 [SUCCESS] compute-13-153.tier2<br/>root     1993529  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     1993549  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[11] 10:26:49 [SUCCESS] compute-13-173.tier2<br/>root     2409772  0.0  0.0 113284  1560 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     2409792  0.0  0.0 112812   956 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[12] 10:26:49 [SUCCESS] compute-13-171.tier2<br/>root     4013605  0.0  0.0 113284  1560 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     4013626  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[13] 10:26:49 [SUCCESS] compute-13-252.tier2<br/>root      580544  0.0  0.0 113284  1556 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root      580562  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[14] 10:26:49 [SUCCESS] compute-13-254.tier2<br/>root     1090719  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     1090737  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[15] 10:26:49 [SUCCESS] compute-13-253.tier2<br/>root      988296  0.0  0.0 113284  1560 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root      988314  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[16] 10:26:49 [SUCCESS] compute-13-251.tier2<br/>root      738014  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root      738032  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[17] 10:26:49 [SUCCESS] blade-03-02.tier2<br/>root     1556738  0.0  0.0 113284  1560 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     1556756  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[18] 10:26:49 [SUCCESS] blade-03-01.tier2<br/>root     1060671  0.0  0.0 113284  1560 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     1060689  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[19] 10:26:49 [SUCCESS] blade-03-04.tier2<br/>root     1396794  0.0  0.0 113284  1560 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     1396812  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[20] 10:26:49 [SUCCESS] blade-03-03.tier2<br/>root     3716000  0.0  0.0 113284  1556 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     3716019  0.0  0.0 112812   960 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[21] 10:26:49 [SUCCESS] compute-11-21.tier2<br/>root     2869438  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     2869463  0.0  0.0 112812   968 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[22] 10:26:49 [SUCCESS] compute-7-23.tier2<br/>root     3038969  0.0  0.0 113176  1588 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     3038994  0.0  0.0 112708   968 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[23] 10:26:50 [SUCCESS] compute-11-18.tier2<br/>root     2144649  0.0  0.0 113284  1568 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     2144679  0.0  0.0 112812   968 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[24] 10:26:50 [SUCCESS] compute-11-12.tier2<br/>root     1214949  0.0  0.0 113284  1568 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     1214974  0.0  0.0 112812   964 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[25] 10:26:50 [SUCCESS] compute-11-11.tier2<br/>root     2781235  0.0  0.0 113284  1568 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     2781260  0.0  0.0 112812   968 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[26] 10:26:50 [SUCCESS] compute-11-15.tier2<br/>root      758013  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root      758039  0.0  0.0 112812   968 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[27] 10:26:50 [SUCCESS] compute-11-19.tier2<br/>root     2047463  0.0  0.0 113284  1572 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     2047490  0.0  0.0 112812   968 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[28] 10:26:50 [SUCCESS] compute-11-16.tier2<br/>root     2403727  0.0  0.0 113284  1568 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root     2403754  0.0  0.0 112812   968 ?        S    10:26   0:00          \_ grep condor_cron_*<br/>[29] 10:26:50 [SUCCESS] compute-11-9.tier2<br/>root      159102  0.0  0.0 113284  1564 ?        Ss   10:26   0:00      \_ bash -c ps auxf | grep condor_cron_*<br/>root      159128  0.0  0.0 112812   968 ?        S    10:26   0:00          \_ grep condor_cron_*</pre><br/>
<span style="color: #de5f24"><span style="font-size: small">(12:27:35)</span> <b>justas.balcas:</b></span> Only 1 recent<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:28:33)</span> <b>justas.balcas:</b></span> And it is stale on all big machines - all these machines are &gt;80 cores<br/>
<span style="color: #43761b"><span style="font-size: small">(12:39:55)</span> <b>blin:</b></span> hrm, what version of condor? and do you have a machine that has a recent "is still running" error? i think that's where the process check would be most relevant<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:47:46)</span> <b>justas.balcas:</b></span> 8.8.11-1.osg35.el7 on one of the nodes (but I have seen this in the past with older releases). The latest error form ‘is still running’ is at ~07:50 PST<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:48:11)</span> <b>justas.balcas:</b></span> so for 3hrs no complain - but classads are still old on StartD<br/>
<span style="color: #43761b"><span style="font-size: small">(12:49:23)</span> <b>blin:</b></span> and the cron has run since then?<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:52:28)</span> <b>justas.balcas:</b></span> <pre>Most yes, except 2 nodes (compute-13-172.tier2 and blade-03-04.tier2):<br/>[1] 10:50:19 [SUCCESS] compute-13-172.tier2<br/>CMS_CONDOR_SCAN = 1605306602<br/>[2] 10:50:19 [SUCCESS] compute-13-151.tier2<br/>CMS_CONDOR_SCAN = 1620323281<br/>[3] 10:50:19 [SUCCESS] compute-13-171.tier2<br/>CMS_CONDOR_SCAN = 1620323281<br/>[4] 10:50:19 [SUCCESS] compute-13-173.tier2<br/>CMS_CONDOR_SCAN = 1620323281<br/>[5] 10:50:19 [SUCCESS] compute-13-251.tier2<br/>CMS_CONDOR_SCAN = 1620323281<br/>[6] 10:50:19 [SUCCESS] compute-10-274.tier2<br/>CMS_CONDOR_SCAN = 1620323401<br/>[7] 10:50:19 [SUCCESS] compute-10-273.tier2<br/>CMS_CONDOR_SCAN = 1620323401<br/>[8] 10:50:19 [SUCCESS] compute-03-274.tier2<br/>CMS_CONDOR_SCAN = 1620323401<br/>[9] 10:50:19 [SUCCESS] compute-10-272.tier2<br/>CMS_CONDOR_SCAN = 1620323401<br/>[10] 10:50:19 [SUCCESS] compute-13-153.tier2<br/>CMS_CONDOR_SCAN = 1620323281<br/>[11] 10:50:19 [SUCCESS] compute-03-271.tier2<br/>CMS_CONDOR_SCAN = 1620323401<br/>[12] 10:50:19 [SUCCESS] compute-03-272.tier2<br/>CMS_CONDOR_SCAN = 1620323401<br/>[13] 10:50:19 [SUCCESS] compute-13-252.tier2<br/>CMS_CONDOR_SCAN = 1620323402<br/>[14] 10:50:20 [SUCCESS] compute-13-253.tier2<br/>CMS_CONDOR_SCAN = 1620323281<br/>[15] 10:50:20 [SUCCESS] compute-13-254.tier2<br/>CMS_CONDOR_SCAN = 1620323401<br/>[16] 10:50:20 [SUCCESS] blade-03-02.tier2<br/>CMS_CONDOR_SCAN = 1620323401<br/>[17] 10:50:20 [SUCCESS] compute-03-273.tier2<br/>CMS_CONDOR_SCAN = 1620323282<br/>[18] 10:50:20 [SUCCESS] blade-03-04.tier2<br/>CMS_CONDOR_SCAN = 1613089562<br/>[19] 10:50:20 [SUCCESS] blade-03-01.tier2<br/>CMS_CONDOR_SCAN = 1620323281<br/>[20] 10:50:20 [SUCCESS] blade-03-03.tier2<br/>CMS_CONDOR_SCAN = 1620323282<br/>[21] 10:50:20 [SUCCESS] compute-7-23.tier2<br/>CMS_CONDOR_SCAN = 1606864562<br/>[22] 10:50:20 [SUCCESS] compute-11-21.tier2<br/>CMS_CONDOR_SCAN = 1620323402<br/>[23] 10:50:20 [SUCCESS] compute-11-18.tier2<br/>CMS_CONDOR_SCAN = 1620323282<br/>[24] 10:50:20 [SUCCESS] compute-11-9.tier2<br/>CMS_CONDOR_SCAN = 1620323282<br/>[25] 10:50:20 [SUCCESS] compute-11-19.tier2<br/>CMS_CONDOR_SCAN = 1620323402<br/>[26] 10:50:20 [SUCCESS] compute-11-15.tier2<br/>CMS_CONDOR_SCAN = 1620323402<br/>[27] 10:50:20 [SUCCESS] compute-11-16.tier2<br/>CMS_CONDOR_SCAN = 1614642242<br/>[28] 10:50:20 [SUCCESS] compute-11-11.tier2<br/>CMS_CONDOR_SCAN = 1620323402<br/>[29] 10:50:20 [SUCCESS] compute-11-12.tier2<br/>CMS_CONDOR_SCAN = 1620323403</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(12:56:09)</span> <b>blin:</b></span> hrm, i don't have any immediate thoughts. could you open a ticket for this? then we can rope in the condor team<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:56:23)</span> <b>justas.balcas:</b></span> Can I increase just CRON logging without restart? (Even with restart I guess it would fail with-in next 24hrs)<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:59:37)</span> <b>justas.balcas:</b></span> Ticket sent: 67224<br/>
<span style="color: #43761b"><span style="font-size: small">(13:03:44)</span> <b>blin:</b></span> got it, thanks<br/>
</body>
</html>

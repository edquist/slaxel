<!DOCTYPE html>
<html>
<head>
<title>Mon Mar 1, 2021 : #software (osg)</title>
</head>
<body>
<h3>Mon Mar 1, 2021 : #software (osg)</h3>
<span style="color: #9f69e7"><span style="font-size: small">(08:53:44)</span> <b>mtwest2718:</b></span> Hi @efajardo, I saw in #general that certain channels are being reorganized. Is this one of them slated for demolition?<br/>
<blockquote>
<span style="color: #43761b"><span style="font-size: small">(09:01:14)</span> <b>blin:</b></span> Hi @mtwest2718 we don't currently have any plans to reorganize this channel!<br/>
<span style="color: #9f69e7"><span style="font-size: small">(09:01:51)</span> <b>mtwest2718:</b></span> Understood.<br/>
<span style="color: #43761b"><span style="font-size: small">(09:06:06)</span> <b>blin:</b></span> also it's worth noting that last Friday was Edgar's last day with the OSG<br/>
<span style="color: #9f69e7"><span style="font-size: small">(09:26:05)</span> <b>mtwest2718:</b></span> Which was all the more odd because I registered for the Slack workspace an hour ago. Maybe Edgar meant signing up for the meeting itself, just odd coincidence it would be the first post I would see upon entering here.<br/>
</blockquote>
<span style="color: #9d8eee"><span style="font-size: small">(09:01:15)</span> <b>cat:</b></span> @mtwest2718, we are not performing a wholesale reorganization at this time. There were a few limited changes last week.<br/>
<span style="color: #c386df"><span style="font-size: small">(13:28:01)</span> <b>khurtado:</b></span> Hello, it seems CRL certs in OASIS are not up to date (latest files come from Feb 25)<br/><pre>[khurtado@login-el7 certificates]$ pwd<br/>/cvmfs/oasis.opensciencegrid.org/mis/certificates<br/>[khurtado@login-el7 certificates]$ ls -lstrh | tail<br/>1.0K -rw-rw-r-- 1 cvmfs cvmfs  694 Feb 25 00:21 7ed47087.r0<br/> 11K -rw-rw-r-- 1 cvmfs cvmfs  11K Feb 25 00:21 24c3ccde.r0<br/>1.5K -rw-r--r-- 1 cvmfs cvmfs 1.4K Feb 25 00:21 fc5a8f99.r0<br/>1.0K -rw-r--r-- 1 cvmfs cvmfs  739 Feb 25 00:21 f30dd6ad.r0<br/>1.0K -rw-r--r-- 1 cvmfs cvmfs  987 Feb 25 00:21 95b96af1.r0<br/>1.0K -rw-r--r-- 1 cvmfs cvmfs 1019 Feb 25 00:21 6187b673.r0<br/>1.0K -rw-rw-r-- 1 cvmfs cvmfs  987 Feb 25 00:21 522b8537.r0<br/>1.0K -rw-r--r-- 1 cvmfs cvmfs 1019 Feb 25 00:21 4042bcee.r0<br/>1.5K -rw-r--r-- 1 cvmfs cvmfs 1.4K Feb 25 00:21 35105088.r0<br/>1.0K -rw-r--r-- 1 cvmfs cvmfs  739 Feb 25 00:21 04f60c28.r0</pre><br/><br/>
<span style="color: #c386df"><span style="font-size: small">(13:29:17)</span> <b>khurtado:</b></span> So, e.g.: I get the following message when trying to get a cms proxy (which uses :<br/><pre>$ voms-proxy-init -voms cms -valid 192:00<br/>Error: Error during SSL handshake:error:80066405:lib(128):proxy_verify_callback:outdated CRL found, revoking all certs till you get new CRL:sslutils.c:1915<br/>outdated CRL found, revoking all certs till you get new CRL<br/>Function: proxy_verify_callback<br/>error:80066411:lib(128):proxy_verify_callback:certificate validation error:sslutils.c:2110: CRL has expired [subject=/DC=ch/DC=cern/OU=computers/CN=<a href="http://lcg-voms2.cern.ch">lcg-voms2.cern.ch</a>,issuer=/DC=ch/DC=cern/CN=CERN Grid Certification Authority]<br/>certificate validation error: CRL has expired [subject=/DC=ch/DC=cern/OU=computers/CN=<a href="http://lcg-voms2.cern.ch">lcg-voms2.cern.ch</a>,issuer=/DC=ch/DC=cern/CN=CERN Grid Certification Authority]<br/>Function: proxy_verify_callback<br/>error:14090086:SSL routines:ssl3_get_server_certificate:certificate verify failed:s3_clnt.c:1264<br/>certificate verify failed<br/>Function: ssl3_get_server_certificate<br/>[khurtado@login-el7 ~]$ which voms-proxy-init<br/>/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.5/3.5.31-1/el7-x86_64/usr/bin/voms-proxy-init<br/>[khurtado@login-el7 ~]$ ls -l /etc/grid-security/certificates<br/>lrwxrwxrwx 1 root root 19 Feb 14  2020 /etc/grid-security/certificates -&gt; /share/certificates<br/>[khurtado@login-el7 ~]$ ls -l /share/certificates<br/>lrwxrwxrwx 1 root root 72 Feb 14  2020 /share/certificates -&gt; /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/certificates<br/>[khurtado@login-el7 ~]$</pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(13:37:16)</span> <b>bbockelm:</b></span> @matyas ^^^ can you take a look at the above?<br/>
<span style="color: #e06b56"><span style="font-size: small">(13:37:58)</span> <b>jthiltges:</b></span> Looks like it's perhaps the local system mount, rather than CVMFS.<br/>
<span style="color: #43761b"><span style="font-size: small">(13:38:26)</span> <b>blin:</b></span> yeah, CVMFS looks fine on the CHTC:<br/><pre>$ ls -l /cvmfs/oasis.opensciencegrid.org/mis/certificates-1.94NEW/ | grep r0 | tail<br/>-rw-r--r-- 1 cvmfs cvmfs   743 Mar  1 12:21 f30dd6ad.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs   849 Mar  1 12:20 f4401b90.r0<br/>-rw-r--r-- 1 cvmfs cvmfs  3757 Mar  1 12:20 f4cf8fb6.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs  3250 Mar  1 12:20 f5ead794.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs  3380 Mar  1 12:20 f5f0dfc2.r0<br/>-rw-r--r-- 1 cvmfs cvmfs  1653 Mar  1 12:20 f8598272.r0<br/>-rw-r--r-- 1 cvmfs cvmfs  1373 Mar  1 12:21 fc5a8f99.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs  2425 Mar  1 12:20 fdf90b95.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs  3376 Mar  1 12:21 ff94d436.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs 89302 Mar  1 12:21 ffc3d59b.r0</pre><br/>
<span style="color: #e96699"><span style="font-size: small">(13:50:30)</span> <b>lincoln:</b></span> hrmm<br/>
<span style="color: #e96699"><span style="font-size: small">(13:50:37)</span> <b>lincoln:</b></span> <pre><br/>[root@login-el7 certificates]#  ls -l /cvmfs/oasis.opensciencegrid.org/mis/certificates-1.94NEW/ | grep r0 | tail<br/>-rw-r--r-- 1 cvmfs cvmfs   739 Feb 25 00:21 f30dd6ad.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs   849 Feb 25 00:20 f4401b90.r0<br/>-rw-r--r-- 1 cvmfs cvmfs  3757 Feb 25 00:20 f4cf8fb6.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs  3250 Feb 25 00:20 f5ead794.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs  3238 Feb 25 00:20 f5f0dfc2.r0<br/>-rw-r--r-- 1 cvmfs cvmfs  1653 Feb 25 00:20 f8598272.r0<br/>-rw-r--r-- 1 cvmfs cvmfs  1373 Feb 25 00:21 fc5a8f99.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs  2425 Feb 25 00:20 fdf90b95.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs  3376 Feb 25 00:21 ff94d436.r0<br/>-rw-rw-r-- 1 cvmfs cvmfs 89212 Feb 25 00:21 ffc3d59b.r0</pre><br/>
<span style="color: #e96699"><span style="font-size: small">(13:50:43)</span> <b>lincoln:</b></span> we can try flushing the CVMFS cache<br/>
<span style="color: #e96699"><span style="font-size: small">(13:51:13)</span> <b>lincoln:</b></span> this is probably the same issue that we saw on MWT2 hosts<br/>
<span style="color: #e96699"><span style="font-size: small">(13:51:16)</span> <b>lincoln:</b></span> whatever it is.<br/>
<span style="color: #99a949"><span style="font-size: small">(13:57:04)</span> <b>dwd:</b></span> Let’s do a little debugging first<br/>
<span style="color: #99a949"><span style="font-size: small">(13:58:33)</span> <b>dwd:</b></span> What does ‘attr -qg revision /cvmfs/oasis.opensciencegrid.org’ say?  I see now 16309<br/>
<span style="color: #99a949"><span style="font-size: small">(13:59:44)</span> <b>dwd:</b></span> I also see an update date of Mar 1 on my test machine<br/>
<span style="color: #99a949"><span style="font-size: small">(13:59:52)</span> <b>dwd:</b></span> @lincoln<br/>
<span style="color: #e96699"><span style="font-size: small">(14:00:05)</span> <b>lincoln:</b></span> checking<br/>
<span style="color: #e96699"><span style="font-size: small">(14:00:22)</span> <b>lincoln:</b></span> <pre># attr -qg revision /cvmfs/oasis.opensciencegrid.org<br/>16267</pre><br/>
<span style="color: #99a949"><span style="font-size: small">(14:00:59)</span> <b>dwd:</b></span> Ok then next look for messages relaged to <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a> in /var/log/messages<br/>
<span style="color: #e96699"><span style="font-size: small">(14:01:22)</span> <b>lincoln:</b></span> <pre>Mar  1 13:43:11 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) warning, could not apply updated catalog revision, entering offline mode<br/>Mar  1 13:46:12 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) recovered from offline mode<br/>Mar  1 13:46:16 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) warning, could not apply updated catalog revision, entering offline mode<br/>Mar  1 13:49:16 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) recovered from offline mode<br/>Mar  1 13:49:23 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) warning, could not apply updated catalog revision, entering offline mode<br/>Mar  1 13:52:24 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) recovered from offline mode<br/>Mar  1 13:53:04 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) warning, could not apply updated catalog revision, entering offline mode<br/>Mar  1 13:56:04 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) recovered from offline mode<br/>Mar  1 13:56:12 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) warning, could not apply updated catalog revision, entering offline mode<br/>Mar  1 13:59:12 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) recovered from offline mode<br/>Mar  1 13:59:29 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) warning, could not apply updated catalog revision, entering offline mode</pre><br/>
<span style="color: #e96699"><span style="font-size: small">(14:01:32)</span> <b>lincoln:</b></span> those messages every 5-10min<br/>
<span style="color: #e96699"><span style="font-size: small">(14:01:38)</span> <b>lincoln:</b></span> for quite some time it appears.<br/>
<span style="color: #99a949"><span style="font-size: small">(14:01:46)</span> <b>dwd:</b></span> Ah ha, we are getting somewhere<br/>
<span style="color: #99a949"><span style="font-size: small">(14:02:16)</span> <b>dwd:</b></span> I will look up what that error means<br/>
<span style="color: #99a949"><span style="font-size: small">(14:02:45)</span> <b>dwd:</b></span> Meanwhile do df to see if you have any full filesystems<br/>
<span style="color: #e96699"><span style="font-size: small">(14:04:17)</span> <b>lincoln:</b></span> <pre>[root@login-el7 ~]# df -h<br/>Filesystem                                  Size  Used Avail Use% Mounted on<br/>devtmpfs                                     94G     0   94G   0% /dev<br/>tmpfs                                        94G   16K   94G   1% /dev/shm<br/>tmpfs                                        94G  4.1G   90G   5% /run<br/>tmpfs                                        94G     0   94G   0% /sys/fs/cgroup<br/>/dev/sdb3                                   215G   45G  160G  22% /<br/>/dev/sdb2                                   976M  188M  738M  21% /boot<br/>/dev/sda                                    8.8T  1.3T  7.5T  15% /scratch<br/><a href="http://nfs.grid.uchicago.edu:/tank/export/connect">nfs.grid.uchicago.edu:/tank/export/connect</a>   17T  4.0T   13T  24% /home<br/>cvmfs2                                       50G   43G  7.7G  85% /cvmfs/oasis.opensciencegrid.org<br/>cvmfs2                                       50G   43G  7.7G  85% /cvmfs/cms.cern.ch<br/>tmpfs                                        19G     0   19G   0% /run/user/100814<br/>tmpfs                                        19G     0   19G   0% /run/user/15148<br/>cvmfs2                                       50G   43G  7.7G  85% /cvmfs/config-osg.opensciencegrid.org<br/>tmpfs                                        19G     0   19G   0% /run/user/12773<br/>tmpfs                                        19G     0   19G   0% /run/user/100538<br/>tmpfs                                        19G     0   19G   0% /run/user/12619<br/>tmpfs                                        19G     0   19G   0% /run/user/58006<br/>tmpfs                                        19G     0   19G   0% /run/user/12340<br/>tmpfs                                        19G     0   19G   0% /run/user/24239<br/>tmpfs                                        19G     0   19G   0% /run/user/0</pre><br/>
<span style="color: #99a949"><span style="font-size: small">(14:05:38)</span> <b>dwd:</b></span> See if you can search back through /var/log/messages for the first mention of offline mode.  This message appears to happen whenever offline mode is being continued<br/>
<span style="color: #e96699"><span style="font-size: small">(14:10:08)</span> <b>lincoln:</b></span> ok<br/>
<span style="color: #e96699"><span style="font-size: small">(14:10:41)</span> <b>lincoln:</b></span> first time it happens, it's surrounded by some messages from another repo<br/>
<span style="color: #e96699"><span style="font-size: small">(14:10:42)</span> <b>lincoln:</b></span> <pre>Feb 28 03:14:25 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 64444876, CAS key a86d4c5dd98fb9204688d03a2a2ea1213a1a757d, error code 2<br/>Feb 28 03:14:26 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 64444761, CAS key e64d33894d496efcf2f60f6e7ea71c43d2eb61ce, error code 2<br/>Feb 28 03:14:26 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 64444761, CAS key e64d33894d496efcf2f60f6e7ea71c43d2eb61ce, error code 2<br/>Feb 28 03:14:26 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 64444874, CAS key 80b445864a98eeb9b5d2b860c63ffa7a99449f7b, error code 2<br/>Feb 28 03:14:27 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 64444874, CAS key 80b445864a98eeb9b5d2b860c63ffa7a99449f7b, error code 2<br/>Feb 28 03:14:28 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) recovered from offline mode<br/>Feb 28 03:14:28 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) warning, could not apply updated catalog revision, entering offline mode<br/>Feb 28 03:14:28 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 64444926, CAS key 8d50b8319c330bf8b0df9d9007f17d4689fd3172, error code 2<br/>Feb 28 03:14:28 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 64444926, CAS key 8d50b8319c330bf8b0df9d9007f17d4689fd3172, error code 2<br/>Feb 28 03:14:29 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 64444878, CAS key 63add2e4a0224cfce1ad27efeb9b53438676a68c, error code 2</pre><br/>
<span style="color: #e96699"><span style="font-size: small">(14:11:14)</span> <b>lincoln:</b></span> let me check older messages<br/>
<span style="color: #99a949"><span style="font-size: small">(14:11:33)</span> <b>dwd:</b></span> Those are a different repository so I’m not sure they’re relevant<br/>
<span style="color: #e96699"><span style="font-size: small">(14:12:01)</span> <b>lincoln:</b></span> the first occurance is here<br/>Feb 25 03:24:18 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) warning, could not apply updated catalog revision, entering offline mode<br/>
<span style="color: #e96699"><span style="font-size: small">(14:12:07)</span> <b>lincoln:</b></span> again surrounded by cms repo messages<br/>
<span style="color: #e96699"><span style="font-size: small">(14:12:18)</span> <b>lincoln:</b></span> nothing else interesting in the log as far as i can tell<br/>
<span style="color: #e96699"><span style="font-size: small">(14:12:41)</span> <b>lincoln:</b></span> oh, that's for the cms repo<br/>
<span style="color: #e96699"><span style="font-size: small">(14:12:46)</span> <b>lincoln:</b></span> let me find the first occurance for the oasis repo<br/>
<span style="color: #99a949"><span style="font-size: small">(14:13:02)</span> <b>dwd:</b></span> Also I may be wrong about it being continuation of the condition.  The code is here: <a href="https://github.com/cvmfs/cvmfs/blob/devel/cvmfs/fuse_remount.cc#L264">https://github.com/cvmfs/cvmfs/blob/devel/cvmfs/fuse_remount.cc#L264</a><br/>
<span style="color: #99a949"><span style="font-size: small">(14:14:30)</span> <b>dwd:</b></span> So in that <a href="http://cms.cern.ch">cms.cern.ch</a> case having surrounding messages for the same repo could be relevant<br/>
<span style="color: #e96699"><span style="font-size: small">(14:14:46)</span> <b>lincoln:</b></span> yeah<br/>
<span style="color: #e96699"><span style="font-size: small">(14:15:05)</span> <b>lincoln:</b></span> here's the very first time it happens, as far as I can tell:<br/><pre>Feb 25 03:30:10 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 4741900, CAS key e64d33894d496efcf2f60f6e7ea71c43d2eb61ce, error code 2<br/>Feb 25 03:30:10 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 4742015, CAS key a86d4c5dd98fb9204688d03a2a2ea1213a1a757d, error code 2<br/>Feb 25 03:30:10 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) warning, could not apply updated catalog revision, entering offline mode<br/>Feb 25 03:30:10 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) failed to open inode: 4742065, CAS key 8d50b8319c330bf8b0df9d9007f17d4689fd3172, error code 2</pre><br/><br/>
<span style="color: #e96699"><span style="font-size: small">(14:15:50)</span> <b>lincoln:</b></span> There were some spurious "Failed to open inode" errors for the Oasis repo too:<br/><pre>messages-20210228:Feb 25 10:33:14 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) failed to open inode: 273389, CAS key 6b78d206fa01a86b38229c6af8c3776da4d2a69e, error code 2<br/>messages-20210228:Feb 25 10:37:52 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) failed to open inode: 273389, CAS key 6b78d206fa01a86b38229c6af8c3776da4d2a69e, error code 2<br/>messages-20210228:Feb 25 23:10:19 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) failed to open inode: 273389, CAS key 6b78d206fa01a86b38229c6af8c3776da4d2a69e, error code 2<br/>messages-20210228:Feb 26 01:45:50 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) failed to open inode: 273389, CAS key 6b78d206fa01a86b38229c6af8c3776da4d2a69e, error code 2<br/>messages-20210228:Feb 26 02:46:29 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) failed to open inode: 273389, CAS key 6b78d206fa01a86b38229c6af8c3776da4d2a69e, error code 2<br/>messages-20210228:Feb 26 06:39:14 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) failed to open inode: 273389, CAS key 6b78d206fa01a86b38229c6af8c3776da4d2a69e, error code 2<br/>messages-20210228:Feb 26 06:42:44 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) failed to open inode: 273389, CAS key 6b78d206fa01a86b38229c6af8c3776da4d2a69e, error code 2</pre><br/>
<span style="color: #99a949"><span style="font-size: small">(14:17:15)</span> <b>dwd:</b></span> The above function I point to has a different type of message “kLogSyslog” for the recovered message, different than “kLogSyslogWarn” for the entering offline message so it’s possible that the recovered message is not showing up.  The function also does nothing if the mode is not changing, so that makes me think that it is switching back and forth between online and offline.<br/>
<span style="color: #99a949"><span style="font-size: small">(14:18:41)</span> <b>dwd:</b></span> SetOfflineMOde(true) appears to come from errors either catalog::kLoadFail or atalog::kLoadNoSpace<br/>
<span style="color: #99a949"><span style="font-size: small">(14:19:32)</span> <b>dwd:</b></span> Is your cache in the default location, /var/cache/cvmfs?  There’s plenty of space in the root filesystem<br/>
<span style="color: #e96699"><span style="font-size: small">(14:22:34)</span> <b>lincoln:</b></span> It's in /scratch<br/>
<span style="color: #e96699"><span style="font-size: small">(14:22:41)</span> <b>lincoln:</b></span> which also has plenty of space<br/>
<span style="color: #e96699"><span style="font-size: small">(14:22:46)</span> <b>lincoln:</b></span> <pre>[root@login-el7 scratch]# df -h /scratch/<br/>Filesystem      Size  Used Avail Use% Mounted on<br/>/dev/sda        8.8T  1.3T  7.5T  15% /scratch</pre><br/>
<span style="color: #99a949"><span style="font-size: small">(14:25:52)</span> <b>dwd:</b></span> It is risky to put it into a filesystem that users can fill up (can they?), although that doesn’t appear to be the problem here.  Can you check to see if there are any files under the cvmfs cache that are not owned by the cvmfs user<br/>
<span style="color: #99a949"><span style="font-size: small">(14:26:09)</span> <b>dwd:</b></span> Might the disk be going bad?<br/>
<span style="color: #e96699"><span style="font-size: small">(14:26:19)</span> <b>lincoln:</b></span> it's a RAID6 but always possible<br/>
<span style="color: #e96699"><span style="font-size: small">(14:26:27)</span> <b>lincoln:</b></span> let me check the ownership<br/>
<span style="color: #e96699"><span style="font-size: small">(14:28:31)</span> <b>lincoln:</b></span> <pre>[root@login-el7 cvmfs]# find \! -user cvmfs<br/>./cache<br/>./cache/osgstorage</pre><br/>
<span style="color: #99a949"><span style="font-size: small">(14:37:36)</span> <b>dwd:</b></span> Did you say this happens on multiple different cvmfs clients over time?<br/>
<span style="color: #e96699"><span style="font-size: small">(14:42:08)</span> <b>lincoln:</b></span> I haven't seen this particular issue, but we have seen CRLs seemingly out of date on the MWT2 hosts. We ended up just moving them to RPM installs, but I don't think it's the exact same issue here.<br/>
<span style="color: #e96699"><span style="font-size: small">(14:42:36)</span> <b>lincoln:</b></span> in this case, the catalog is clearly out of date and noisy in /var/log/messags.<br/>
<span style="color: #99a949"><span style="font-size: small">(14:44:05)</span> <b>dwd:</b></span> so it is possible that it’s a hardware issue, or some one-time corruption in the cache<br/>
<span style="color: #e96699"><span style="font-size: small">(14:46:37)</span> <b>lincoln:</b></span> it sounds like we ought to try wiping the cache and see if things recover unless you have some other debugging in mind<br/>
<span style="color: #99a949"><span style="font-size: small">(14:48:34)</span> <b>dwd:</b></span> Yes I think the next thing to do is to clean the cache, but I’d like to see if it can be done without unmounting.   First, please do <tt>sudo cvmfs_talk -i <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a> remount</tt> to see if we got the same error message in /var/log/messages (I expect we will)<br/>
<span style="color: #e96699"><span style="font-size: small">(14:50:10)</span> <b>lincoln:</b></span> <pre># sudo cvmfs_talk -i <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a> remount<br/>New revision applied</pre><br/>
<span style="color: #99a949"><span style="font-size: small">(14:56:53)</span> <b>dwd:</b></span> did that actually catch it up?<br/>
<span style="color: #e96699"><span style="font-size: small">(14:56:59)</span> <b>lincoln:</b></span> checking<br/>
<span style="color: #e96699"><span style="font-size: small">(14:57:15)</span> <b>lincoln:</b></span> <pre># attr -qg revision /cvmfs/oasis.opensciencegrid.org<br/>16267</pre><br/>
<span style="color: #e96699"><span style="font-size: small">(14:57:16)</span> <b>lincoln:</b></span> did not<br/>
<span style="color: #99a949"><span style="font-size: small">(14:57:46)</span> <b>dwd:</b></span> ok and there’s probably also the same warning in /var/log/messages in response to the command<br/>
<span style="color: #e96699"><span style="font-size: small">(14:58:50)</span> <b>lincoln:</b></span> i just ran it again because there was a bunch of junk in /var/log/messages, here's it again:<br/><pre>Mar  1 14:58:33 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) recovered from offline mode<br/>Mar  1 14:58:35 login-el7 cvmfs2: (<a href="http://cms.cern.ch">cms.cern.ch</a>) warning, could not apply updated catalog revision, entering offline mode<br/>Mar  1 14:58:36 login-el7 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) warning, could not apply updated catalog revision, entering offline mode</pre><br/>
<span style="color: #e96699"><span style="font-size: small">(14:58:52)</span> <b>lincoln:</b></span> immediately after i ran it<br/>
<span style="color: #99a949"><span style="font-size: small">(15:03:14)</span> <b>dwd:</b></span> Oh I forgot that we were seeing messages ‘recovered from offline mode’.  It’s clear that it’s not really recovering though. It would be nice if we could get debug messages, but the only way I know to enable that is to unmount.  Could you try doing umount /cvmfs/oasis.opensciencegrid.org?  Most likely it will be busy and not work.<br/>
<span style="color: #e96699"><span style="font-size: small">(15:04:07)</span> <b>lincoln:</b></span> sure<br/>
<span style="color: #e96699"><span style="font-size: small">(15:04:22)</span> <b>lincoln:</b></span> indeed, busy<br/>
<span style="color: #99a949"><span style="font-size: small">(15:05:37)</span> <b>dwd:</b></span> Ok so the next thing to do to wipe cache without unmounting is <tt>sudo cvmfs_talk -i <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a> cleanup 0</tt><br/>
<span style="color: #e96699"><span style="font-size: small">(15:11:15)</span> <b>lincoln:</b></span> running it now..<br/>
<span style="color: #e96699"><span style="font-size: small">(15:11:39)</span> <b>lincoln:</b></span> <pre>[root@login-el7 ~]# cvmfs_talk -i <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a> cleanup 0<br/>Not fully cleaned (there might be pinned chunks)</pre><br/>
<span style="color: #99a949"><span style="font-size: small">(15:11:56)</span> <b>dwd:</b></span> Ok any difference if you do a remount?<br/>
<span style="color: #e96699"><span style="font-size: small">(15:12:48)</span> <b>lincoln:</b></span> <pre>[root@login-el7 ~]# cvmfs_talk -i <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a> remount<br/><br/><br/>New revision applied<br/>[root@login-el7 ~]#<br/>[root@login-el7 ~]#<br/>[root@login-el7 ~]#  attr -qg revision /cvmfs/oasis.opensciencegrid.org<br/>16267[</pre><br/>
<span style="color: #e96699"><span style="font-size: small">(15:12:51)</span> <b>lincoln:</b></span> still the same<br/>
<span style="color: #99a949"><span style="font-size: small">(15:14:11)</span> <b>dwd:</b></span> One problem with the online cache cleanup is that I think catalogs are the files that get pinned and therefore not cleaned out, but it’s catalogs that we’re having a problem with<br/>
<span style="color: #e96699"><span style="font-size: small">(15:14:21)</span> <b>lincoln:</b></span> ah<br/>
<span style="color: #99a949"><span style="font-size: small">(15:17:38)</span> <b>dwd:</b></span> Ok the next more drastic thing to try is <tt>sudo cvmfs_config reload <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a></tt> which should pause accesses for a minute<br/>
<span style="color: #e96699"><span style="font-size: small">(15:18:10)</span> <b>lincoln:</b></span> doing it<br/>
<span style="color: #e96699"><span style="font-size: small">(15:18:28)</span> <b>lincoln:</b></span> <pre>Connecting to CernVM-FS loader... done<br/>Entering maintenance mode<br/>Draining out kernel caches (up to 60s)<br/>Blocking new file system calls<br/>Waiting for active file system calls<br/>Saving inode tracker<br/>Saving negative entry cache<br/>Saving chunk tables<br/>Saving inode generation<br/>Saving open files table<br/>Unloading Fuse module<br/>Re-Loading Fuse module<br/>failed to load blacklist from config repository (15)<br/>Reload FAILED! CernVM-FS mountpoints unusable.</pre><br/>
<span style="color: #e96699"><span style="font-size: small">(15:18:34)</span> <b>lincoln:</b></span> whomp whomp<br/>
<span style="color: #99a949"><span style="font-size: small">(15:19:11)</span> <b>dwd:</b></span> Next try <tt>sudo cvmfs_config reload</tt> which pauses all repositories and reloads<br/>
<span style="color: #e96699"><span style="font-size: small">(15:20:59)</span> <b>lincoln:</b></span> <pre>[root@login-el7 ~]# cvmfs_config reload<br/>Pausing <a href="http://cms.cern.ch">cms.cern.ch</a> on /cvmfs/cms.cern.ch<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Connecting to CernVM-FS loader... done<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Entering maintenance mode<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Draining out kernel caches (up to 60s)<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Blocking new file system calls<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Waiting for active file system calls<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Saving inode tracker<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Saving negative entry cache<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Saving chunk tables<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Saving inode generation<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Saving open files table<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Unloading Fuse module<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Waiting for the delivery of SIGUSR1...<br/>Pausing <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a> on /cvmfs/config-osg.opensciencegrid.org<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Connecting to CernVM-FS loader... done<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Entering maintenance mode<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Draining out kernel caches (up to 60s)<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Blocking new file system calls<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Waiting for active file system calls<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Saving inode tracker<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Saving negative entry cache<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Saving chunk tables<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Saving inode generation<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Saving open files table<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Unloading Fuse module<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Waiting for the delivery of SIGUSR1...<br/><a href="http://cms.cern.ch">cms.cern.ch</a>: Re-Loading Fuse module<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Re-Loading Fuse module<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Restoring inode tracker...  done<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Restoring negative entry cache...  done<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Restoring chunk tables...  done<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Restoring inode generation...  done<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Restoring open files table... done<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Restoring open files counter...  done<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Releasing saved glue buffer<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Releasing saved negative entry cache<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Releasing chunk tables<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Releasing saved inode generation info<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Releasing saved open files table<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Releasing open files counter<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>: Activating Fuse module<br/>Reload FAILED! CernVM-FS mountpoints <a href="http://unusable.cms.cern.ch">unusable.cms.cern.ch</a>: failed to load blacklist from config repository (15)</pre><br/>
<span style="color: #99a949"><span style="font-size: small">(15:21:53)</span> <b>dwd:</b></span> How about <tt>sudo cvmfs_config reload -c</tt><br/>
<span style="color: #99a949"><span style="font-size: small">(15:22:05)</span> <b>dwd:</b></span> That’s supposed to clean cache.<br/>
<span style="color: #e96699"><span style="font-size: small">(15:22:20)</span> <b>lincoln:</b></span> running<br/>
<span style="color: #99a949"><span style="font-size: small">(15:22:41)</span> <b>dwd:</b></span> But it sounds like it can’t mount the config repo at all<br/>
<span style="color: #e96699"><span style="font-size: small">(15:25:26)</span> <b>lincoln:</b></span> <pre>[root@login-el7 ~]#  cvmfs_config reload -c<br/><br/><br/>Wiping out /scratch/cvmfs/cache/atlas-condb.cern.ch ... OK<br/>Wiping out /scratch/cvmfs/cache/atlas-nightlies.cern.ch ... OK<br/>Wiping out /scratch/cvmfs/cache/atlas.cern.ch ... OK<br/>Wiping out /scratch/cvmfs/cache/cms.cern.ch ... OK<br/>Wiping out /scratch/cvmfs/cache/geant4.cern.ch ... OK<br/>Wiping out /scratch/cvmfs/cache/grid.cern.ch ... OK<br/>Wiping out /scratch/cvmfs/cache/oasis.opensciencegrid.org ... OK<br/>Wiping out /scratch/cvmfs/cache/osg.mwt2.org ... OK<br/>Wiping out /scratch/cvmfs/cache/sft.cern.ch ... OK<br/>Wiping out /scratch/cvmfs/cache/shared ... OK<br/>Wiping out /scratch/cvmfs/cache/unpacked.cern.ch ... OK<br/>[root@login-el7 ~]#</pre><br/><br/>
<span style="color: #99a949"><span style="font-size: small">(15:26:32)</span> <b>dwd:</b></span> Does it help?<br/>
<span style="color: #99a949"><span style="font-size: small">(15:27:28)</span> <b>dwd:</b></span> I’m surprised it wasn’t surrounded by pausing and releasing messages<br/>
<span style="color: #e96699"><span style="font-size: small">(15:31:57)</span> <b>lincoln:</b></span> now I'm getting transport endpoint not connected<br/>
<span style="color: #e96699"><span style="font-size: small">(15:32:02)</span> <b>lincoln:</b></span> so it seems like the process has disappeared<br/>
<span style="color: #99a949"><span style="font-size: small">(15:34:03)</span> <b>dwd:</b></span> I guess so.  The system was already quite messed up, but we have made it worse.  To get rid of the Transport endpoint not connected without rebooting you’ll need to get the endpoints umounted, which probably means killing processess holding them with fuser -k -m<br/>
<span style="color: #e06b56"><span style="font-size: small">(15:35:11)</span> <b>jthiltges:</b></span> Been a while, but I want to believe a lazy (<tt>-l</tt>) unmount will then allow remounting.<br/>
<span style="color: #99a949"><span style="font-size: small">(15:36:19)</span> <b>dwd:</b></span> will allow umount but I’m not sure about remount<br/>
<span style="color: #e96699"><span style="font-size: small">(15:38:11)</span> <b>lincoln:</b></span> i just did <tt>cvmfs_config killall</tt> and things came back<br/>
<span style="color: #e96699"><span style="font-size: small">(15:38:21)</span> <b>lincoln:</b></span> <pre> attr -qg revision /cvmfs/oasis.opensciencegrid.org<br/>16309 </pre><br/>seems ok now<br/>
<span style="color: #99a949"><span style="font-size: small">(16:02:09)</span> <b>dwd:</b></span> I wasn’t remembering cvmfs_config killall, but it’s a nice overall cleanup function.  I see it will do umount -l if umount fails, as John suggested.  It does not do fuser -k -m as I suggested. I should remember it.<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:12:33)</span> <b>bbockelm:</b></span> So - is the conclusion that perhaps the problem was in the config repo in the end?<br/>
<span style="color: #e96699"><span style="font-size: small">(16:31:45)</span> <b>lincoln:</b></span> well, I don't know what the problem ended up being in the end, but a killall restored the repo and it is now the latest version.<br/>
</body>
</html>

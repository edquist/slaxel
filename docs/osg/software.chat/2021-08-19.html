<!DOCTYPE html>
<html>
<head>
<title>Thu Aug 19, 2021 : #software (osg)</title>
</head>
<body>
<h3>Thu Aug 19, 2021 : #software (osg)</h3>
<span style="color: #e7392d"><span style="font-size: small">(08:51:52)</span> <b>xizhouf:</b></span> The Clemson site finds some OSG jobs were killed by pbs/cgroup because those jobs requested only 1MB memory. Is it possible to set a default memory request to 1GB for those jobs without memory request or with wrong memory request?<br/>
<span style="color: #9e3997"><span style="font-size: small">(08:52:54)</span> <b>bbockelm:</b></span> Hi Xizhou!  Sounds like a misconfiguration of the OSG CE (or potentially a bug, I suppose) - it should always put reasonable defaults in the jobs.<br/>
<span style="color: #9e3997"><span style="font-size: small">(08:53:08)</span> <b>bbockelm:</b></span> Do you know which Clemson resource this was for?<br/>
<span style="color: #e7392d"><span style="font-size: small">(08:55:25)</span> <b>xizhouf:</b></span> <a href="http://osg-ce.clemson.edu">osg-ce.clemson.edu</a><br/>
<span style="color: #e7392d"><span style="font-size: small">(08:56:25)</span> <b>xizhouf:</b></span> Here is a snippet of the log.<br/><br/>Job Id: 3319243.pbs02<br/>   Job_Name = bl_ykngN9<br/>   Job_Owner = <a href="mailto:glow@osg002.palmetto.clemson.edu">glow@osg002.palmetto.clemson.edu</a><br/>   resources_used.cpupercent = 0<br/>   resources_used.cput = 00:00:00<br/>   resources_used.mem = 1024kb<br/>   resources_used.ncpus = 8<br/>   resources_used.vmem = 28016kb<br/>   resources_used.walltime = 00:00:01<br/>   job_state = F<br/>   queue = osg_e<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:39:38)</span> <b>bbockelm:</b></span> Yes, that definitely looks suspicious!  If I look at the settings on the remote side, they should be asking for 2GB.<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:48:01)</span> <b>bbockelm:</b></span> @blin - can you look at the routed job?  here's an example:<br/><pre>condor_history -limit 1 -pool <a href="http://collector.opensciencegrid.org:9619">collector.opensciencegrid.org:9619</a> -name <a href="http://osg-ce.clemson.edu">osg-ce.clemson.edu</a> -const 'GlideinEntryName=?="OSG_US_Clemson-Palmetto_condce"' -l 170435.0</pre><br/>I just can't recall which attributes get passed through for memory these days.<br/>
<span style="color: #43761b"><span style="font-size: small">(10:13:56)</span> <b>blin:</b></span> <tt>maxMemory = 2048</tt> looks reasonable to me<br/>
<span style="color: #43761b"><span style="font-size: small">(10:14:37)</span> <b>blin:</b></span> this doesn't look right, though<br/><pre>RequestMemory = ifthenelse(MemoryUsage =!= undefined,MemoryUsage,(ImageSize + 1023) / 1024)</pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(10:22:03)</span> <b>bbockelm:</b></span> That's just the default nonsense, should get ignored, no?<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:23:15)</span> <b>bbockelm:</b></span> So it's possibly in the blahp submission from HTCondor -&gt; PBS?<br/>
<span style="color: #43761b"><span style="font-size: small">(10:23:27)</span> <b>blin:</b></span> it's a routed job so we <tt>set_RequestMemory</tt> to an expression in the JR defaults<br/>
<span style="color: #43761b"><span style="font-size: small">(10:23:35)</span> <b>blin:</b></span> <pre>    set_RequestMemory = ifThenElse(WantWholeNode is true,<br/>                                   !isUndefined(TotalMemory) ? TotalMemory*95/100 : JobMemory,<br/>                                   OriginalMemory);</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(10:24:03)</span> <b>blin:</b></span> i'm checking config remotely<br/>
<span style="color: #43761b"><span style="font-size: small">(10:45:22)</span> <b>blin:</b></span> ^^ @johnkn this is a bug in the post route transforms<br/>
<span style="color: #43761b"><span style="font-size: small">(10:48:33)</span> <b>blin:</b></span> we use <tt>DEFAULT RequestMemory</tt> to the CE default expression but that will never trigger since condor will always set a <tt>RequestMemory = ifthenelse(MemoryUsage =!= undefined,MemoryUsage,(ImageSize + 1023) / 1024)</tt><br/>
<span style="color: #43761b"><span style="font-size: small">(10:48:37)</span> <b>blin:</b></span> <a href="https://github.com/htcondor/htcondor-ce/blob/V5-branch/config/01-ce-router-defaults.conf#L236">https://github.com/htcondor/htcondor-ce/blob/V5-branch/config/01-ce-router-defaults.conf#L236</a><br/>
<span style="color: #43761b"><span style="font-size: small">(10:49:22)</span> <b>blin:</b></span> CE clients submit memory requests with <tt>maxMemory</tt> <a href="https://htcondor.github.io/htcondor-ce/v5/remote-job-submission/#submit-file-commands">https://htcondor.github.io/htcondor-ce/v5/remote-job-submission/#submit-file-commands</a><br/>
<span style="color: #43761b"><span style="font-size: small">(10:58:18)</span> <b>blin:</b></span> i don't think we can really use <tt>DEFAULT</tt> for the resource requests<br/>
<span style="color: #e06b56"><span style="font-size: small">(11:04:04)</span> <b>johnkn:</b></span> "condor" doesn't supply a default,  condor_submit does.   not all jobs are submitted by condor_submit.<br/>
<span style="color: #e06b56"><span style="font-size: small">(11:04:59)</span> <b>johnkn:</b></span> if a job does already have a Request then there is no need for he router to set one, and it would be wrong to overwrite it, hence the use of DEFAULT<br/>
<span style="color: #e06b56"><span style="font-size: small">(11:05:53)</span> <b>johnkn:</b></span> You could also delete the request in a pre-transform, if you want to just ignore what the incoming job has<br/>
<span style="color: #43761b"><span style="font-size: small">(11:07:36)</span> <b>blin:</b></span> we should add the logic back the post route transforms to prefer the <tt>maxMemory</tt>, <tt>xcount</tt>, etc attributes vs the <tt>Request*</tt> attrs<br/>
<span style="color: #43761b"><span style="font-size: small">(11:08:05)</span> <b>blin:</b></span> i don't think we want to ignore the <tt>Request*</tt> attr, especially if we think we want clients to transition to them eventually!<br/>
<span style="color: #e06b56"><span style="font-size: small">(11:09:23)</span> <b>johnkn:</b></span> I personally would never prefer foreign names for the resource requests over the standard condor names, but it's entirely your call.<br/>
<span style="color: #e7392d"><span style="font-size: small">(11:10:50)</span> <b>xizhouf:</b></span> Hi Brian, what change should I make on the osg-ce node?<br/>
<blockquote>
<span style="color: #43761b"><span style="font-size: small">(11:12:25)</span> <b>blin:</b></span> yeah, in <tt>/etc/condor-ce/config.d/</tt> set<br/><pre>JOB_ROUTER_ROUTE_Pbs_Cluster = GridResource = "batch pbs"<br/>default_queue = osg<br/>SET RequestMemory &lt;MEMORY&gt;</pre><br/>replacing <tt>&lt;MEMORY&gt;</tt> with the amount of memory that the jobs should request in MB<br/>
<span style="color: #e7392d"><span style="font-size: small">(11:15:58)</span> <b>xizhouf:</b></span> Is this /etc/condor-ce/config.d/99-local.conf file good?<br/>
<span style="color: #43761b"><span style="font-size: small">(11:16:30)</span> <b>blin:</b></span> that should be fine. it appears that you're already setting <tt>JOB_ROUTER_ROUTE_Pbs_Cluster</tt> somewhere so you may as well fix it wherever you set it<br/>
<span style="color: #43761b"><span style="font-size: small">(11:16:41)</span> <b>blin:</b></span> you can find the file with <tt>condor_ce_config_val -v JOB_ROUTER_ROUTE_Pbs_Cluster</tt><br/>
<span style="color: #9e3997"><span style="font-size: small">(11:17:15)</span> <b>bbockelm:</b></span> &gt;  replacing <tt>&lt;MEMORY&gt;</tt> with the amount of memory that the jobs should request in MB<br/>Brian, be careful with that - Xizhou is getting both single core and multicore pilots.<br/>
<span style="color: #43761b"><span style="font-size: small">(11:17:38)</span> <b>blin:</b></span> argh<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:17:53)</span> <b>bbockelm:</b></span> It looks like both are setting the <tt>maxMemory</tt> attribute, however.<br/>
<span style="color: #43761b"><span style="font-size: small">(11:18:35)</span> <b>blin:</b></span> @xizhouf ah ok, then instead of <tt>SET RequestMemory &lt;MEMORY&gt;</tt> , <tt>SET RequestMemory maxMemory</tt><br/>
<span style="color: #e7392d"><span style="font-size: small">(11:20:24)</span> <b>xizhouf:</b></span> Here is the current configuration:<br/>#JOB_QUEUE_LOG=/osg/condor/job_queue.log<br/>GSS_ASSIST_GRIDMAP_CACHE_EXPIRATION = 0<br/>JOB_ROUTER_ROUTE_Pbs_Cluster @=jrt<br/>  GridResource = "batch pbs"<br/>  default_queue = osg<br/>  SET RequestMemory maxMemory<br/>@jrt<br/>JOB_ROUTER_ROUTE_NAMES = Pbs_Cluster<br/>
<span style="color: #43761b"><span style="font-size: small">(11:21:42)</span> <b>blin:</b></span> that looks good to me!<br/>
<span style="color: #43761b"><span style="font-size: small">(11:22:08)</span> <b>blin:</b></span> oh wait, if you're also getting multi core pilot requests, we should do something for <tt>RequestCpus</tt> too<br/>
<span style="color: #43761b"><span style="font-size: small">(11:22:49)</span> <b>blin:</b></span> add <tt>SET RequestCpus xcount</tt> to the route as well<br/>
<span style="color: #43761b"><span style="font-size: small">(11:22:56)</span> <b>blin:</b></span> then run <tt>condor_ce_reconfig</tt><br/>
<span style="color: #e7392d"><span style="font-size: small">(11:25:00)</span> <b>xizhouf:</b></span> Done.<br/>
<span style="color: #e7392d"><span style="font-size: small">(11:25:44)</span> <b>xizhouf:</b></span> By the way, below is our current osg queue setting on the PBS side:<br/># Create and define queue osg<br/>create queue osg<br/>set queue osg queue_type = Route<br/>set queue osg Priority = 100<br/>set queue osg max_queued = [o:PBS_ALL=500]<br/>set queue osg acl_group_enable = True<br/>set queue osg acl_groups = osg<br/>set queue osg route_destinations = osg_e<br/>set queue osg enabled = True<br/>set queue osg started = True<br/># Create and define queue osg_e<br/>create queue osg_e<br/>set queue osg_e queue_type = Execution<br/>set queue osg_e Priority = 100<br/>set queue osg_e max_queued = [o:PBS_ALL=500]<br/>set queue osg_e max_queued += [u:PBS_GENERIC=500]<br/>set queue osg_e max_queued_res.mem = [o:PBS_ALL=10000gb]<br/>set queue osg_e max_queued_res.ncpus = [o:PBS_ALL=1000]<br/>set queue osg_e max_queued_res.mem += [u:PBS_GENERIC=10000gb]<br/>set queue osg_e max_queued_res.ncpus += [u:PBS_GENERIC=1000]<br/>set queue osg_e from_route_only = True<br/>set queue osg_e resources_max.mem = 100gb<br/>set queue osg_e resources_max.ncpus = 16<br/>set queue osg_e resources_max.walltime = 24:00:00<br/>set queue osg_e resources_default.qcat = osg_qcat<br/>set queue osg_e resources_default.walltime = 24:00:00<br/>set queue osg_e acl_group_enable = True<br/>set queue osg_e acl_groups = osg<br/>set queue osg_e default_chunk.mem = 1gb<br/>set queue osg_e default_chunk.qcat = osg_qcat<br/>set queue osg_e resources_available.mem = 10000gb<br/>set queue osg_e resources_available.ncpus = 500<br/>set queue osg_e resources_available.qcat = osg_qcat<br/>set queue osg_e kill_delay = 120<br/>set queue osg_e max_run = [o:PBS_ALL=500]<br/>set queue osg_e max_run += [u:PBS_GENERIC=500]<br/>set queue osg_e max_run_res.mem = [o:PBS_ALL=10000gb]<br/>set queue osg_e max_run_res.ncpus = [o:PBS_ALL=1000]<br/>set queue osg_e max_run_res.mem += [u:PBS_GENERIC=10000gb]<br/>set queue osg_e max_run_res.ncpus += [u:PBS_GENERIC=1000]<br/>set queue osg_e enabled = True<br/>set queue osg_e started = True<br/>
<span style="color: #e7392d"><span style="font-size: small">(11:28:43)</span> <b>xizhouf:</b></span> The change is effective. Here is the qstat results.<br/>...<br/>    schedselect = 1:mem=1mb:ncpus=1:qcat=osg_qcat:ngpus=0:nphis=0<br/>    schedselect = 1:mem=1024mb:ncpus=1:qcat=osg_qcat:ngpus=0:nphis=0<br/>    schedselect = 1:mem=1024mb:ncpus=8:qcat=osg_qcat:ngpus=0:nphis=0<br/>    schedselect = 1:mem=1024mb:ncpus=1:qcat=osg_qcat:ngpus=0:nphis=0<br/>...<br/>
</blockquote>
<span style="color: #43761b"><span style="font-size: small">(11:11:09)</span> <b>blin:</b></span> maxMemory etc are CE native, <tt>Request*</tt>  are "foreign" from this perspective :slightly_smiling_face:<br/>
<span style="color: #e06b56"><span style="font-size: small">(11:35:03)</span> <b>johnkn:</b></span> Perhaps we want to have a pre transform that deletes a RequestMemory that we think is unreasonably small.<br/><pre>EVALMACRO small_mem_request (RequestMemory?:0) &lt; 100<br/>if $(small_mem_request)<br/>   DELETE RequestMemory<br/>endif</pre><br/>
<span style="color: #16569E"><span style="font-size: small">(11:56:58)</span> <b>edquist:</b></span> GRIDFTP experts! (@bbockelm ?) does anybody know if we have a recommended way to find the ideal values for the <tt>GRIDFTP_BUFFER_COUNT</tt> and <tt>GRIDFTP_FILE_BUFFER_COUNT</tt> knobs?<br/>
<span style="color: #16569E"><span style="font-size: small">(12:02:38)</span> <b>edquist:</b></span> our docs tell us they refer to the number of 1M memory &amp; file-based buffers, but i can't find anything in terms of tuning these parameters<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:10:07)</span> <b>bbockelm:</b></span> I'd maybe just ask an existing site (@gattebury?).  They aren't as relevant anymore as modern GridFTP versions Should Mostly do the reordering.<br/>
<span style="color: #16569E"><span style="font-size: small">(12:23:26)</span> <b>edquist:</b></span> ok thanks<br/>
<span style="color: #d1707d"><span style="font-size: small">(12:25:30)</span> <b>gattebury:</b></span> FWIW I don’t think we’ve ever actually adjusted those knobs at Nebraska — certainly they’ve been commented out in <tt>/etc/sysconfig/gridftp-hdfs</tt> for a long while. Looks like the defaults are 200 and 1500.<br/>
<span style="color: #16569E"><span style="font-size: small">(12:28:58)</span> <b>edquist:</b></span> ok thanks!<br/>
<span style="color: #16569E"><span style="font-size: small">(12:28:59)</span> <b>edquist:</b></span> and,<br/>
<span style="color: #16569E"><span style="font-size: small">(12:29:07)</span> <b>edquist:</b></span> &gt;Looks like the defaults are 200 and 1500<br/>
<span style="color: #16569E"><span style="font-size: small">(12:29:19)</span> <b>edquist:</b></span> i agree that's how it looks but it's a lie<br/>
<span style="color: #16569E"><span style="font-size: small">(12:30:02)</span> <b>edquist:</b></span> the defaults are 200 for the memory buffers, and for the file buffers the default is 3x the memory buffer number<br/>
<span style="color: #16569E"><span style="font-size: small">(12:30:13)</span> <b>edquist:</b></span> But<br/>
<span style="color: #16569E"><span style="font-size: small">(12:30:29)</span> <b>edquist:</b></span> the mem buffer number resets to its default if you try to set it above 1000<br/>
<span style="color: #16569E"><span style="font-size: small">(12:30:49)</span> <b>edquist:</b></span> and the file buffer number resets to its default if you try to set it less than the mem buffer number ;)<br/>
<span style="color: #16569E"><span style="font-size: small">(12:31:14)</span> <b>edquist:</b></span> <a href="https://github.com/gridcf/gct/blob/master/gridftp/hdfs/conf/gridftp-hdfs#L8-L14">https://github.com/gridcf/gct/blob/master/gridftp/hdfs/conf/gridftp-hdfs#L8-L14</a><br/>
<span style="color: #16569E"><span style="font-size: small">(12:31:18)</span> <b>edquist:</b></span> <a href="https://github.com/gridcf/gct/blob/master/gridftp/hdfs/src/gridftp_hdfs.c#L505-L521">https://github.com/gridcf/gct/blob/master/gridftp/hdfs/src/gridftp_hdfs.c#L505-L521</a><br/>
</body>
</html>

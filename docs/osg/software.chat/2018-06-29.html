<!DOCTYPE html>
<html>
<head>
<title>Fri Jun 29, 2018 : #software (osg)</title>
</head>
<body>
<h3>Fri Jun 29, 2018 : #software (osg)</h3>
<span style="color: #a72f79"><span style="font-size: small">(09:58:28)</span> <b>andrew.melo:</b></span> As a thought experiment, what would happen if the factory submits a pilot job, the CE internally replicated it 100x as a SLURM job array and ran the same pilot multiple times? Our local SLURM guy claims that SLURM can schedule those much more quickly (since they have the same requirements, etc...), but I know that none of the interfaces (importantly BLAH) support that<br/>
<span style="color: #43761b"><span style="font-size: small">(10:00:42)</span> <b>blin:</b></span> what would happen if a few of the replicated pilots failed?<br/>
<span style="color: #e96699"><span style="font-size: small">(10:05:57)</span> <b>ian_cancercomputer:</b></span> There would need to be a lot of changes to allow a 1:n relationship in the pilot jobs running on HTCondor-CE vs the local-batch, wouldn't there..<br/>
<span style="color: #43761b"><span style="font-size: small">(10:07:15)</span> <b>blin:</b></span> yea, i don't think glideinwms would be able to handle it at all<br/>
<span style="color: #43761b"><span style="font-size: small">(10:07:45)</span> <b>blin:</b></span> the blahp could certainly be modified to do it but there'd have to be a lot of thought that goes into what constitutes success/failure<br/>
<span style="color: #e96699"><span style="font-size: small">(10:10:53)</span> <b>ian_cancercomputer:</b></span> blahp w/ SLURM launches a sub-condor to handle the work unit management, same as on PBS, right?<br/>
<span style="color: #e96699"><span style="font-size: small">(10:11:15)</span> <b>ian_cancercomputer:</b></span> s/blahp/the pilot jobs/<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:12:45)</span> <b>andrew.melo:</b></span> Brian would know much more than me, but IIRC, the job router on the condor side makes a new condor job with the blahp universe (?), and shoves it into blahp, who just submits the script directly to SLURM<br/>
<span style="color: #43761b"><span style="font-size: small">(10:15:09)</span> <b>blin:</b></span> yea but i think ian means the pilot payload on the worker nodes?<br/>
<span style="color: #e96699"><span style="font-size: small">(10:15:33)</span> <b>ian_cancercomputer:</b></span> Yes, that's what I was referring to.<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:15:56)</span> <b>andrew.melo:</b></span> Ah yeah, sorry<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:16:12)</span> <b>andrew.melo:</b></span> It's layers-on-layers<br/>
<span style="color: #e96699"><span style="font-size: small">(10:18:19)</span> <b>ian_cancercomputer:</b></span> All good. :slightly_smiling_face:  Anyways, I was mostly just wondering if the overhead of the 1:1 condor-CE &lt;-&gt; SLURM relationship (given it just allocates and schedules a local internal condor to actually fetch and crunch the jobs on each worker) is really significant enough to warrant implementing this meta-pilot-job idea?<br/>
<span style="color: #e96699"><span style="font-size: small">(10:19:42)</span> <b>ian_cancercomputer:</b></span> (I don't have 1000's of WNs (yet) to know from experience)<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:19:43)</span> <b>andrew.melo:</b></span> I'm trying to get some numbers. Basically, the other users of the cluster will submit 20k jobs in a single job array (in SLURM parlance), and we're the largest group, but don't use them and the scheduling time for the scheduler is dragging<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:21:36)</span> <b>andrew.melo:</b></span> We've only got 7000 or so cores, but I think the explosion is that the scheduling in SLURM is quite a bit more complicated, and our local groups have wildly different jobs, so the scheduler is struggling to backfill and schedule things<br/>
<span style="color: #e96699"><span style="font-size: small">(10:28:06)</span> <b>ian_cancercomputer:</b></span> Are the pilots you're recieving allocating individual cores or full WN's/NUMA-nodes?<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:28:41)</span> <b>andrew.melo:</b></span> Both... for pricing reasons<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:30:31)</span> <b>andrew.melo:</b></span> Basically, we're 1/3 of the cluster. Another 1/3 of the cluster is the structural biology group, who needs gobs of memory/core (like 16-20GB/core). They pay for very high memory nodes, and then we backfill the CPUs that would be otherwise starved<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:30:54)</span> <b>andrew.melo:</b></span> But then CMS also has some nodes for its exclusive use, and those are allocated node at a time<br/>
<span style="color: #de5f24"><span style="font-size: small">(10:30:59)</span> <b>tiradani:</b></span> @andrew.melo At FNAL, through HEPCloud, we are doing something like that now at NERSC using glideinWMS<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:31:14)</span> <b>andrew.melo:</b></span> Could you elaborate some?<br/>
<span style="color: #de5f24"><span style="font-size: small">(10:31:53)</span> <b>tiradani:</b></span> we are using the BOSCO ssh interface on the NERSC side.  We submit one pilot that ends up replicating across many nodes<br/>
<span style="color: #de5f24"><span style="font-size: small">(10:32:30)</span> <b>tiradani:</b></span> I donâ€™t have the technical details off the top of my head at the moment, but next week, I can dig them up<br/>
<span style="color: #de5f24"><span style="font-size: small">(10:33:28)</span> <b>tiradani:</b></span> We use the BOSCO submission scripts on the submit node to translate the requests<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:35:52)</span> <b>andrew.melo:</b></span> Hm. Interesting.<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:36:58)</span> <b>andrew.melo:</b></span> Well, like I said, it was a thought experiment. The SLURM guy has been mad at me for years, he can stay mad a bit longer. One thing that really hurt was when the factory started deleting pending jobs and then immediately resubmitting them<br/>
<span style="color: #de5f24"><span style="font-size: small">(10:41:46)</span> <b>tiradani:</b></span> the caveat is that there is support overhead for this kind of submission and we are running our own factory.<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:13:22)</span> <b>andrew.melo:</b></span> yeah, I figured. There's a pretty huge impedance mismatch between Condor+gWMS and SLURM in this case, where one will give you a job at a time, but the 2nd really wants all the jobs up front. It's (relatively) simple to clone 1 condor job into 100 SLURM jobs using some SLURM magic, which wouldn't have to dig up through all the different layers. But if that blows up gWMS, then that's a non-starter, and more things would have to change, which isn't worth it<br/>
<span style="color: #de5f24"><span style="font-size: small">(11:25:06)</span> <b>tiradani:</b></span> gwms can handle it.  I believe the features are all there in the latest release.  The support overhead I am referring to is that the site and/or factory admins must coordinate the BOSCO setup and the custom scripts.<br/>
<span style="color: #5b89d5"><span style="font-size: small">(11:25:33)</span> <b>sthapa:</b></span> Dave Lesny has this working on stampede2 and atlas jobs<br/>
<span style="color: #5b89d5"><span style="font-size: small">(11:26:29)</span> <b>sthapa:</b></span> the bahp  submit scripts were modified to claim a fixed number of nodes and a multilauncher command at TACC is used to start glideins with the same options on all nodes<br/>
<span style="color: #de5f24"><span style="font-size: small">(11:27:08)</span> <b>tiradani:</b></span> @sthapa is that through the CE that you are operating?<br/>
<span style="color: #5b89d5"><span style="font-size: small">(11:28:09)</span> <b>sthapa:</b></span> not really, but similar approach (RCCF factory).  a bosco factory submitting to stampede2 and other systems and a CE flocking jobs to the factory<br/>
<span style="color: #5b89d5"><span style="font-size: small">(11:28:29)</span> <b>sthapa:</b></span> Nothing would stop the CE that I run from doing something similar<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:29:50)</span> <b>andrew.melo:</b></span> @tiradani so if the factory sends me one job, and I run the exact same job twice, nothing breaks?<br/>
<span style="color: #de5f24"><span style="font-size: small">(11:31:18)</span> <b>tiradani:</b></span> In our configuration, the factory uses BOSCO to submit a glideinWMS pilot.  The BOSCO scripts (on the NESRC submit node) are configured to submit 1 job requesting many nodes.  The pilot is then executed on each node, pulling in work independently.<br/>
<span style="color: #de5f24"><span style="font-size: small">(11:32:43)</span> <b>tiradani:</b></span> sorry for being long winded, just want to make sure I am explaining it accurately<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:41:40)</span> <b>andrew.melo:</b></span> No, that's fine. I think we're talking about slightly different things, so it's important to make the distinction. If I understand, the factory is sending a single job (via bosco), then bosco in the background is tiurning it into multiple jobs?<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:02:33)</span> <b>tiradani:</b></span> Yup.  This is configured in the submission scripts you have to install at the site.<br/>
<span style="color: #a72f79"><span style="font-size: small">(12:03:43)</span> <b>andrew.melo:</b></span> It's a one-liner to change it for SLURM to run multiple of the same job<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:03:04)</span> <b>andrew.melo:</b></span> It doesn't hurt anything, but I also noticed that SLURM and condor have wildly different ideas of how many jobs are in the queue<br/><pre><br/>[root@ce4 ~]# condor_ce_router_q<br/>   JOBS ST Route                GridResource<br/>    215  I SLURM_Default        batch slurm<br/>     54  H SLURM_Default        batch slurm<br/>     24  I SLURM_WholeNode      batch slurm<br/>     38  R SLURM_WholeNode      batch slurm<br/>      5  H SLURM_WholeNode      batch slurm<br/>[root@ce3 ~]# condor_ce_router_q<br/>Not defined: JOB_ROUTER_SCHEDD2_NAME<br/>Not defined: JOB_ROUTER_SCHEDD2_POOL<br/>   JOBS ST Route                GridResource<br/>    102  I SLURM_Default        batch slurm<br/>      2  R SLURM_WholeNode      batch slurm<br/>     44  I SLURM_WholeNode      batch slurm<br/></pre><br/>is ~300 pending jobs on ce4, with another 150 on ce3, but slurm sees ~700<br/><pre><br/> qSummary ; qSummary -p nogpfs<br/>GROUP        USER       ACTIVE_JOBS  ACTIVE_CORES  PENDING_JOBS  PENDING_CORES<br/>------------------------------------------------------------------------------<br/>cms                         100          541           644          1445<br/>            cmspilot        100          541           628          1429<br/>            lcgadmin          0            0            16            16<br/>------------------------------------------------------------------------------<br/>Totals:                     100          541           644          1445<br/><br/>GROUP        USER       ACTIVE_JOBS  ACTIVE_CORES  PENDING_JOBS  PENDING_CORES<br/>------------------------------------------------------------------------------<br/>cms                          63          504            69            69<br/>            cmspilot         63          504            69            69<br/>------------------------------------------------------------------------------<br/>Totals:                      63          504            69            69<br/><br/></pre><br/>
<span style="color: #43761b"><span style="font-size: small">(16:43:08)</span> <b>blin:</b></span> is this after you made the one-line change to map 1 ce:N slurm jobs?<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:43:21)</span> <b>andrew.melo:</b></span> no, I didn't change any of that<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:43:29)</span> <b>andrew.melo:</b></span> I was just trying to think of a way to do it at some point<br/>
<span style="color: #43761b"><span style="font-size: small">(16:47:11)</span> <b>blin:</b></span> hrm, i would try and find out which slurm jobs don't have matching CE jobs<br/>
<span style="color: #43761b"><span style="font-size: small">(16:47:46)</span> <b>blin:</b></span> <tt>condor_ce_q -af gridjobid</tt> and find the set diff with the slurm job id's<br/>
<span style="color: #43761b"><span style="font-size: small">(16:48:14)</span> <b>blin:</b></span> then pick some slurm id's and poke through the gridmanager logs<br/>
<span style="color: #43761b"><span style="font-size: small">(16:48:26)</span> <b>blin:</b></span> that's certainly not supposed to be happening, though<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:50:46)</span> <b>andrew.melo:</b></span> well! might see a problem here:<br/><pre><br/>[root@ce3 ~]#  condor_ce_q -af gridjobid | grep undefined | wc -l<br/>208<br/></pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(16:50:59)</span> <b>andrew.melo:</b></span> <pre><br/>[root@ce3 ~]#  condor_ce_q -af gridjobid | grep undefined<br/>undefined<br/>undefined<br/>undefined<br/></pre><br/>
<span style="color: #43761b"><span style="font-size: small">(16:51:27)</span> <b>blin:</b></span> eh, that's normal, sorry<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:51:54)</span> <b>andrew.melo:</b></span> I will say -- when we update the SLURM configurations, it triggers an HA failover, and that takes 5-10 secs, so the scheduler was unavailble this afternoon for a bit while we added a new rack of machines<br/>
<span style="color: #43761b"><span style="font-size: small">(16:52:44)</span> <b>blin:</b></span> 1) the incoming job won't have a gridjobid but instead a routedtojobid attr that points to the CE job ID, which should have a gridjobid referencing the slurm job id<br/>
<span style="color: #43761b"><span style="font-size: small">(16:52:46)</span> <b>blin:</b></span> except when<br/>
<span style="color: #43761b"><span style="font-size: small">(16:53:16)</span> <b>blin:</b></span> 2) the slurm job has finished and the ce job with the gridjobid is cleaning itself up<br/>
<span style="color: #43761b"><span style="font-size: small">(16:53:53)</span> <b>blin:</b></span> the condor team added the feature that the previous gridjobid gets stored in <tt>LastGridJobId</tt> but i forget which version of condor that went in<br/>
<span style="color: #43761b"><span style="font-size: small">(16:54:15)</span> <b>blin:</b></span> i don't think a short failover like that would cause issues<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:55:39)</span> <b>andrew.melo:</b></span> oh yeah, it's a huge list<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:55:49)</span> <b>andrew.melo:</b></span> @andrew.melo uploaded a file: <a href="https://opensciencegrid.slack.com/files/UA01XMFP1/FBGMHB68J/-.txt">Untitled</a><br/>
<span style="color: #43761b"><span style="font-size: small">(16:55:55)</span> <b>blin:</b></span> i know lengthy failovers cause issues where the CE loses track of slurm jobs but i don't know if a 5-10s failover would cause that<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:56:16)</span> <b>andrew.melo:</b></span> It bounced a few times<br/>
<span style="color: #c386df"><span style="font-size: small">(16:59:43)</span> <b>matyas:</b></span> @tim.theisen I can get SOFTWARE-3268 ready for testing today. we still want to release a new version of rsv to get the fixes for SOFTWARE-3305<br/>
<span style="color: #73769d"><span style="font-size: small">(17:00:53)</span> <b>tim.theisen:</b></span> I think that we have run out of time. We release on Monday.<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:17:53)</span> <b>andrew.melo:</b></span> Well, even if it misses one update, the jobIDs are the same, so it should find them once the scheduler becomes responsive again<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:18:27)</span> <b>andrew.melo:</b></span> Maybe the state machine isn't sorted out all the way right<br/>
<span style="color: #43761b"><span style="font-size: small">(17:21:26)</span> <b>blin:</b></span> @andrew.melo if the backend scheduler goes down long enough, the gridmanager gives up on the jobs<br/>
<span style="color: #43761b"><span style="font-size: small">(17:21:38)</span> <b>blin:</b></span> i forget how long "long enough" is, though<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:21:55)</span> <b>andrew.melo:</b></span> It must be too short if the HA failover can trigger it<br/>
<span style="color: #43761b"><span style="font-size: small">(17:22:01)</span> <b>blin:</b></span> could you ping me on monday and i'll try and find out?<br/>
<span style="color: #43761b"><span style="font-size: small">(17:22:13)</span> <b>blin:</b></span> well at this point we don't know if the HA failover was the culprit<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:22:49)</span> <b>andrew.melo:</b></span> No dice, unfortunately. Going on a cruise with my family (:rage:) starting Monday, so I'll be away till Friday, then I'm flying straight to CHEP<br/>
<span style="color: #43761b"><span style="font-size: small">(17:23:12)</span> <b>blin:</b></span> ah, shoot<br/>
<span style="color: #43761b"><span style="font-size: small">(17:23:44)</span> <b>blin:</b></span> did you have the chance to poke at any of those particular job id's in the logs?<br/>
<span style="color: #43761b"><span style="font-size: small">(17:25:33)</span> <b>blin:</b></span> i really gotta split, though, sorry<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:26:29)</span> <b>andrew.melo:</b></span> Let's chat in either one or two weeks (are you going to CHEP?)<br/>
</body>
</html>

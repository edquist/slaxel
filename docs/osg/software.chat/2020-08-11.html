<!DOCTYPE html>
<html>
<head>
<title>Tue Aug 11, 2020 : #software (osg)</title>
</head>
<body>
<h3>Tue Aug 11, 2020 : #software (osg)</h3>
<span style="color: #a72f79"><span style="font-size: small">(11:12:30)</span> <b>andrew.melo:</b></span> What I really don't get is both CEs drop the jobs at the same time<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:02:24)</span> <b>bockjoo:</b></span> With blahp-1.18.46-1.osg35.el7.x86_64, I can seem to set the dynamic variable for OSG_WN_TMP=$TMPDIR ( <a href="https://opensciencegrid.atlassian.net/browse/SOFTWARE-3995?filter=12355">https://opensciencegrid.atlassian.net/browse/SOFTWARE-3995?filter=12355</a> ). I tested it and said OK to Tim, but I just found this when a CMS pilot operator complained about this issue.<br/>
<span style="color: #43761b"><span style="font-size: small">(13:03:48)</span> <b>blin:</b></span> do you mean you can't set the dynamic env var?<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:05:58)</span> <b>bockjoo:</b></span> Right!<br/>
<span style="color: #43761b"><span style="font-size: small">(13:06:29)</span> <b>blin:</b></span> how are you doing it?<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:07:11)</span> <b>bockjoo:</b></span> grep -v ^$ /etc/osg/config.d/40-localsettings.ini | grep -v ^\;<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:07:12)</span> <b>bockjoo:</b></span> [Storage]<br/>worker_node_temp = $TMPDIR<br/>[Local Settings]<br/>PATH=/cmsuf/bin:$PATH<br/>X509_CERT_DIR=/cvmfs/cms.cern.ch/grid/etc/grid-security/certificates<br/>SINGULARITY_BINDPATH=/cmsuf<br/>SINGULARITY_BIN=/usr/bin<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:07:44)</span> <b>bockjoo:</b></span> On the CEs with blahp 1.18.45, with this identical configuration, OSG_WN_TMP is correctly set to $TMPDIR.<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:08:39)</span> <b>bockjoo:</b></span> But not on the CE with blahp 1.18.46.<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:10:29)</span> <b>bockjoo:</b></span> So, I am now testing if it can be set after downgrading the blahp to 1.18.45.<br/>
<span style="color: #43761b"><span style="font-size: small">(13:13:01)</span> <b>blin:</b></span> could you open a GGUS ticket?<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:14:11)</span> <b>bockjoo:</b></span> Yes, I will.<br/>
<span style="color: #43761b"><span style="font-size: small">(13:17:09)</span> <b>blin:</b></span> @bockjoo please also include the contents of <tt>/var/lib/osg/osg-*job-environment.conf</tt><br/>
<span style="color: #7d414c"><span style="font-size: small">(13:25:30)</span> <b>bockjoo:</b></span> /var/lib/osg/osg-job-environment.conf<br/><pre>#!/bin/sh<br/>#---------- This file automatically generated by osg-configure<br/>#---------- This is periodically overwritten.  DO NOT HAND EDIT<br/>#---------- Instead, write any environment variable customizations into<br/>#---------- the config.ini [Local Settings] section, as documented here:<br/>#---------- <a href="https://opensciencegrid.github.io/docs/other/configuration-with-osg-configure/#local-settings">https://opensciencegrid.github.io/docs/other/configuration-with-osg-configure/#local-settings</a><br/>#---  variables -----<br/>OSG_APP="/cmsuf/data/app"<br/>OSG_DATA="/osg/data"<br/>OSG_DEFAULT_SE="<a href="http://cmsio.rc.ufl.edu">cmsio.rc.ufl.edu</a>"<br/>OSG_GRID="/cvmfs/cms.cern.ch/grid/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/current/el7-x86_64"<br/>OSG_HOSTNAME="<a href="http://cms.rc.ufl.edu">cms.rc.ufl.edu</a>"<br/>OSG_SITE_NAME="UFlorida-HPC"<br/>OSG_SITE_READ="/osg/data/tmp"<br/>OSG_SITE_WRITE="/osg/data/tmp"<br/>OSG_SQUID_LOCATION="squid1-data.ufhpc:3128"<br/>OSG_STORAGE_ELEMENT="True"<br/>OSG_WN_TMP="$TMPDIR"<br/><br/>#--- export variables -----<br/>export OSG_APP<br/>export OSG_DATA<br/>export OSG_DEFAULT_SE<br/>export OSG_GRID<br/>export OSG_HOSTNAME<br/>export OSG_SITE_NAME<br/>export OSG_SITE_READ<br/>export OSG_SITE_WRITE<br/>export OSG_SQUID_LOCATION<br/>export OSG_STORAGE_ELEMENT<br/>export OSG_WN_TMP</pre><br/>   /var/lib/osg/osg-local-job-environment.conf<br/><pre>#!/bin/sh<br/>#---------- This file automatically generated by osg-configure<br/>#---------- This is periodically overwritten.  DO NOT HAND EDIT<br/>#---------- Instead, write any environment variable customizations into<br/>#---------- the config.ini [Local Settings] section, as documented here:<br/>#---------- <a href="https://opensciencegrid.github.io/docs/other/configuration-with-osg-configure/#local-settings">https://opensciencegrid.github.io/docs/other/configuration-with-osg-configure/#local-settings</a><br/>#---  variables -----<br/>PATH="/cmsuf/bin:$PATH"<br/>SINGULARITY_BIN="/usr/bin"<br/>SINGULARITY_BINDPATH="/cmsuf"<br/>X509_CERT_DIR="/cvmfs/cms.cern.ch/grid/etc/grid-security/certificates"<br/><br/>#--- export variables -----<br/>export PATH<br/>export SINGULARITY_BIN<br/>export SINGULARITY_BINDPATH<br/>export X509_CERT_DIR</pre><br/><br/>
<span style="color: #43761b"><span style="font-size: small">(13:26:09)</span> <b>blin:</b></span> could you attach them to the GGUS ticket, too?<br/>
<span style="color: #43761b"><span style="font-size: small">(13:26:22)</span> <b>blin:</b></span> oh wait, is there an <tt>/etc/blah.config.rpmnew</tt>?<br/>
<span style="color: #43761b"><span style="font-size: small">(13:26:43)</span> <b>blin:</b></span> you may be missing this: <tt>blah_job_env_confs=/var/lib/osg/osg-job-environment.conf,/var/lib/osg/osg-local-job-environment.conf</tt><br/>
<span style="color: #7d414c"><span style="font-size: small">(13:28:27)</span> <b>bockjoo:</b></span> rpm -qf   /etc/blah.config<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:28:34)</span> <b>bockjoo:</b></span> blahp-1.18.46-1.osg35.el7.x86_64<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:28:43)</span> <b>bockjoo:</b></span> grep blah_job_env_confs /etc/blah.config<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:28:48)</span> <b>bockjoo:</b></span> &lt;nothing&gt;<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:29:07)</span> <b>bockjoo:</b></span> Is this what you mean?<br/>
<span style="color: #43761b"><span style="font-size: small">(13:29:39)</span> <b>blin:</b></span> add that line to <tt>/etc/blah.config</tt>. I think that'll fix your issue<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:33:58)</span> <b>bockjoo:</b></span> 1.18.45 does not have that line either, but works. I will test 1.18.46 with the env anyway.<br/>
<span style="color: #43761b"><span style="font-size: small">(13:44:32)</span> <b>blin:</b></span> yeah, that line and feature was added in 1.18.46<br/>
<span style="color: #43761b"><span style="font-size: small">(13:45:35)</span> <b>blin:</b></span> @horstseverini i know you also use dynamic WN env vars you may want to make sure that there aren't any <tt>/etc/blah.config.rpmnew</tt> files when updating to blahp 1.18.46<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:53:19)</span> <b>bockjoo:</b></span> So, the line is mandatory for dynamic WN env in 1.18.46, but in 1.18.45, the line is somehow built in?<br/>
<span style="color: #7d414c"><span style="font-size: small">(13:53:58)</span> <b>bockjoo:</b></span> So, I guess we don't need a ticket for this.<br/>
<span style="color: #43761b"><span style="font-size: small">(13:55:18)</span> <b>blin:</b></span> yeah, you should have seen an <tt>rpmnew</tt> file after the upgrade to 1.18.46<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:04:37)</span> <b>bockjoo:</b></span> Thanks @blin! I should look rpmnew carefully next time.<br/>
<span style="color: #43761b"><span style="font-size: small">(14:48:09)</span> <b>blin:</b></span> @andrew.melo when's the next set of jobs slated to get canceled?<br/>
<span style="color: #43761b"><span style="font-size: small">(14:48:56)</span> <b>blin:</b></span> i do see this in the latest logs<br/><pre>GridmanagerLog.cmslocal:08/10/20 17:13:53 [831207] (570320.0) gm state change: GM_SUBMITTED -&gt; GM_TRANSFER_PROXY<br/>GridmanagerLog.cmslocal:08/10/20 17:13:53 [831207] (570320.0) gm state change: GM_TRANSFER_PROXY -&gt; GM_REFRESH_PROXY<br/>GridmanagerLog.cmslocal:08/10/20 17:13:54 [831207] (570320.0) doEvaluateState called: gmState GM_REFRESH_PROXY, remoteState 2<br/>GridmanagerLog.cmslocal:08/10/20 17:13:54 [831207] (570320.0) blah_job_refresh_proxy() failed: Cannot locate old proxy: Permission denied<br/>GridmanagerLog.cmslocal:08/10/20 17:13:54 [831207] (570320.0) gm state change: GM_REFRESH_PROXY -&gt; GM_CANCEL</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(14:49:09)</span> <b>andrew.melo:</b></span> ~3-odd hours<br/>
<span style="color: #a72f79"><span style="font-size: small">(14:49:58)</span> <b>andrew.melo:</b></span> I have the following set:<br/><pre>#Set to yes if you wish to disable BLAH's machinery for transferring <br/>#or delegating proxies to the worker node where a job is running. (default = no)<br/>blah_disable_wn_proxy_renewal=yes<br/><br/>#Set to yes to enable delegation (instead of copy) of renewed proxies<br/>#to worker nodes. NOTE: limited *and* delegated proxes are not <br/>#accepted for default GSI authentication as of VDT 1.2, so this should<br/>#be enabled only if non-limited proxies are used for proxy renewal. (default = no)<br/>blah_delegate_renewed_proxies=no<br/><br/>#Set to yes to disable creation of a limited proxy. (default = no)<br/>blah_disable_limited_proxy=yes</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(14:51:02)</span> <b>blin:</b></span> that lgtm<br/>
<span style="color: #43761b"><span style="font-size: small">(14:51:30)</span> <b>blin:</b></span> strangely, i see the same thing 2 minutes later...<br/><pre>GridmanagerLog.cmslocal:08/10/20 17:15:58 [831207] (570320.0) doEvaluateState called: gmState GM_SUBMITTED, remoteState 2<br/>GridmanagerLog.cmslocal:08/10/20 17:15:58 [831207] (570320.0) gm state change: GM_SUBMITTED -&gt; GM_CANCEL<br/>GridmanagerLog.cmslocal:08/10/20 17:15:59 [831207] (570320.0) doEvaluateState called: gmState GM_CANCEL, remoteState 2<br/>GridmanagerLog.cmslocal:08/10/20 17:15:59 [831207] (570320.0) gm state change: GM_CANCEL -&gt; GM_DELETE_SANDBOX</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(14:56:27)</span> <b>andrew.melo:</b></span> So, that's adding to the noise. I don't know why it's trying to resubmit the job, but it fails w/a null "Attempt to submit failed: " error (no text back from slurm, not even the harmless informational text), and that gets set in the HoldReason<br/>
<span style="color: #43761b"><span style="font-size: small">(14:58:05)</span> <b>blin:</b></span> we probably don't get the info because the blahp is bad about returning lrms error messages =/<br/>
<span style="color: #a72f79"><span style="font-size: small">(14:58:28)</span> <b>andrew.melo:</b></span> I get them back if slurm is down or overloaded, it's just these resubmits that doesn't work<br/>
<span style="color: #43761b"><span style="font-size: small">(14:58:48)</span> <b>blin:</b></span> yeah, it depends on the code path iirc<br/>
<span style="color: #a72f79"><span style="font-size: small">(14:59:06)</span> <b>andrew.melo:</b></span> I've not looked very hard, but I've never seen it resubmit jobs before<br/>
<span style="color: #43761b"><span style="font-size: small">(14:59:39)</span> <b>blin:</b></span> anywho, meeting time!<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:33:48)</span> <b>andrew.melo:</b></span> Fun times. From those logs, can you think of more info I could grab now before everything dies again?<br/>
<span style="color: #43761b"><span style="font-size: small">(16:12:40)</span> <b>blin:</b></span> @andrew.melo i suspect that the factory is removing these jobs. from the source job:<br/><pre>RemoveReason = "by gridmanager (by user cmslocal)"</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(16:14:09)</span> <b>andrew.melo:</b></span> The auditlog is in one of the dumps and it also says something similar<br/>
<span style="color: #43761b"><span style="font-size: small">(16:15:15)</span> <b>blin:</b></span> i don't see that in the auditlog but in the auditlog and schedlog i see the source job getting its  proxy renewed then seeing the removal:<br/><pre>AuditLog:08/10/20 17:08:15 (cid:75167) Refresh GSI cred for job 570281.0 succeeded<br/>AuditLog:08/10/20 17:15:51 (cid:80482) Remove jobs 570281.0,571038.0,571096.0,570285.0,570286.0,571313.0,571757.0,570288.0,571845.0,571485.0</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(16:15:35)</span> <b>blin:</b></span> i'll comment in the ticket<br/>
<span style="color: #43761b"><span style="font-size: small">(16:16:12)</span> <b>blin:</b></span> oh this sucks, from edita:<br/>&gt;  There are no pilots logs for these aborted jobs.<br/>
<span style="color: #43761b"><span style="font-size: small">(16:16:29)</span> <b>blin:</b></span> can you find a pilot that's gonna die soon on a WN and tail its logs?<br/>
<span style="color: #43761b"><span style="font-size: small">(16:16:52)</span> <b>blin:</b></span> maybe we'll find a hint there<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:19:10)</span> <b>andrew.melo:</b></span> I posted it further up in the thread, SLURM gets an scancel, the condor wrapper gets the SIGTERM and it does the graceful shutdown<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:20:03)</span> <b>andrew.melo:</b></span> in the slurm database it just sees that the job was cancelled by the user<br/>
<span style="color: #43761b"><span style="font-size: small">(16:22:14)</span> <b>blin:</b></span> yeah, from what i can tell the cancel ultimately comes from the factory<br/>
<span style="color: #43761b"><span style="font-size: small">(16:22:24)</span> <b>blin:</b></span> i suspect maybe it's lost contact with the pilot and says whelp, kill it<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:23:42)</span> <b>bbockelm:</b></span> if it comes from the factory, the pilot job recorded in <tt>condor_ce_history</tt> will record that.<br/>
<span style="color: #43761b"><span style="font-size: small">(16:25:11)</span> <b>blin:</b></span> ya, we see <tt>RemoveReason = "by gridmanager (by user cmslocal)"</tt> for the source job<br/>
<span style="color: #43761b"><span style="font-size: small">(16:25:28)</span> <b>blin:</b></span> and for the routed job <tt>RemoveReason = "JobRouter aborted job (by user condor)"</tt><br/>
<span style="color: #9e3997"><span style="font-size: small">(16:25:42)</span> <b>bbockelm:</b></span> is <tt>cmslocal</tt> the Vandy username?<br/>
<span style="color: #43761b"><span style="font-size: small">(16:25:57)</span> <b>blin:</b></span> it is ._.<br/>
<span style="color: #43761b"><span style="font-size: small">(16:26:31)</span> <b>blin:</b></span> but the original job shouldn't be managed by the gridmanager, right?<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:27:24)</span> <b>bbockelm:</b></span> Looking at the code, that error message strongly points at the factory.<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:27:38)</span> <b>bbockelm:</b></span> Hence, yes, I'd talk to the gfactory folks and get their logs.<br/>
<span style="color: #43761b"><span style="font-size: small">(16:28:20)</span> <b>blin:</b></span> they aren't getting logs back from the pilot -- do you expect to see info in their condor/other gwms logs?<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:29:57)</span> <b>bbockelm:</b></span> This isn't in the pilot, this is in their schedd / gridmanager / cgahp_worker<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:30:45)</span> <b>bbockelm:</b></span> (or perhaps the factory process)<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:43:28)</span> <b>andrew.melo:</b></span> So, one wrinkle is that we just recently commissioned these new CEs in the last month or so, and I didn't notice they were failing this way because this weird failure doesn't show up in any usable metrics. Perhaps these new entries were configured differently?<br/>
<span style="color: #43761b"><span style="font-size: small">(17:04:12)</span> <b>blin:</b></span> hrm, maybe<br/>
<span style="color: #43761b"><span style="font-size: small">(17:04:26)</span> <b>blin:</b></span> it's not a failure mode i remember seeing before from a misconfigured factory but ¯\_(ツ)_/¯<br/>
<span style="color: #43761b"><span style="font-size: small">(17:04:51)</span> <b>blin:</b></span> @justas.balcas what version of ceph do you plan on using?<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:05:09)</span> <b>justas.balcas:</b></span> Nautilus now on cc7<br/>
<span style="color: #43761b"><span style="font-size: small">(17:07:20)</span> <b>blin:</b></span> hrm, the version i see available on centos7 is 10<br/>
<span style="color: #43761b"><span style="font-size: small">(17:07:33)</span> <b>blin:</b></span> and the xrootd packaging has a flag for building against ceph11<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:23:11)</span> <b>justas.balcas:</b></span> not sure which one you refer, ceph or xrootd?<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:23:25)</span> <b>justas.balcas:</b></span> • v14.2.10 - <a href="https://docs.ceph.com/docs/master/releases/nautilus/">https://docs.ceph.com/docs/master/releases/nautilus/</a><br/>
<span style="color: #43761b"><span style="font-size: small">(17:25:10)</span> <b>blin:</b></span> hrm, ok<br/>
<span style="color: #43761b"><span style="font-size: small">(17:25:29)</span> <b>blin:</b></span> my initial attempts at building didn't work so i'll have to tackle this tomorrow<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:28:30)</span> <b>andrew.melo:</b></span> @blin all the jobs are churning now, anything interesting to look at?<br/>
<span style="color: #43761b"><span style="font-size: small">(17:29:07)</span> <b>blin:</b></span> nothing i can really think of. maybe double check that the proxies have time left and that the factory -&gt; CE renewed them properly?<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:31:01)</span> <b>andrew.melo:</b></span> Hm, I guess they're gone now, since the new jobs are running<br/>
</body>
</html>

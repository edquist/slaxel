<!DOCTYPE html>
<html>
<head>
<title>Thu Aug 20, 2020 : #software (osg)</title>
</head>
<body>
<h3>Thu Aug 20, 2020 : #software (osg)</h3>
<span style="color: #53b759"><span style="font-size: small">(08:12:53)</span> <b>eric.appelt:</b></span> hi @blin - I didn't have my GGUS registration completed correctly yesterday, but I got it fixed overnight and attached the files to the ticket.<br/>
<span style="color: #43761b"><span style="font-size: small">(12:54:12)</span> <b>blin:</b></span> @eric.appelt @andrew.melo have you guys set <tt>GRIDMANAGER_DEBUG = D_FULLDEBUG</tt>?<br/>
<span style="color: #a72f79"><span style="font-size: small">(12:55:13)</span> <b>andrew.melo:</b></span> Yeah, that's been set for a while<br/>
<span style="color: #43761b"><span style="font-size: small">(12:57:38)</span> <b>blin:</b></span> hrm, it should be giving us the path to the proxy when we see <tt>blah_job_refresh_proxy</tt> command fail<br/>
<span style="color: #a72f79"><span style="font-size: small">(13:07:38)</span> <b>andrew.melo:</b></span> I just double-checked. Is there maybe another option it's gated behind?<br/><pre>[root@ce5 ~]# condor_ce_config_val -dump | grep GRIDMANAGER_DEBUG<br/>GRIDMANAGER_DEBUG = D_FULLDEBUG</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(13:08:49)</span> <b>blin:</b></span> i think maybe it's because the attached logs only include lines with the grid and routed job IDs<br/>
<span style="color: #43761b"><span style="font-size: small">(13:08:55)</span> <b>blin:</b></span> and that the proxy path may be on a different line<br/>
<span style="color: #a72f79"><span style="font-size: small">(13:24:40)</span> <b>andrew.melo:</b></span> Maybe it's better for me to pass this to you instead of guessing at what to grep :slightly_smiling_face:<br/>
<span style="color: #43761b"><span style="font-size: small">(13:29:41)</span> <b>blin:</b></span> perfect, thanks<br/>
<span style="color: #43761b"><span style="font-size: small">(13:40:26)</span> <b>blin:</b></span> can we pick a particular job and monitor the permissions on the proxy?<br/><pre>08/20/20 09:15:25 [1451603] (768937.0) blah_job_refresh_proxy() failed: Cannot locate old proxy: No such file or directory<br/>08/20/20 09:15:25 [1451603] (768937.0) gm state change: GM_REFRESH_PROXY -&gt; GM_CANCEL<br/>08/20/20 09:15:25 [1451603] GAHP[1451914] &lt;- 'RESULTS'<br/>08/20/20 09:15:25 [1451603] GAHP[1451914] -&gt; 'R'<br/>08/20/20 09:15:25 [1451603] GAHP[1451914] -&gt; 'S' '1'<br/>08/20/20 09:15:25 [1451603] GAHP[1451914] -&gt; '3202625' '1' 'Cannot locate old proxy: No such file or directory'<br/>08/20/20 09:15:25 [1451603] GAHP[1451914] &lt;- 'BLAH_JOB_REFRESH_PROXY 3202634 slurm/20200820/23124906 /mnt/ce-spool/ce5.vampire/9076/0/cluster769076.proc0.subproc0/credential_CMSG-v1_0.main_411868'<br/>08/20/20 09:15:25 [1451603] GAHP[1451914] -&gt; 'S'</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(13:43:42)</span> <b>blin:</b></span> i forgot if i mentioned it but we think we can improve the blahp to skip this check when it's configured to skip proxy delegation etc<br/>
<span style="color: #43761b"><span style="font-size: small">(13:44:03)</span> <b>blin:</b></span> but that'll probably take some time to implement<br/>
<span style="color: #53b759"><span style="font-size: small">(14:57:24)</span> <b>eric.appelt:</b></span> I'm trying to do that, in an idling job I see the credential file:<br/><pre>[root@ce5 cluster774411.proc0.subproc0]# ls -lh<br/>total 80K<br/>-rw------- 1 cmspilot cms 11K Aug 20 14:43 credential_CMSG-v1_0.main_411868<br/>-rwxr-xr-x 1 cmspilot cms 68K Aug 20 14:43 glidein_startup.sh</pre><br/>But on a running job I do not see it there:<br/><pre>[root@ce5 cluster43568.proc0.subproc0]# ls -lh<br/>total 172K<br/>-rw-r--r-- 1 uscmslocal cms 74K Jul 15 13:46 _condor_stderr<br/>-rw-r--r-- 1 uscmslocal cms 25K Jul 15 13:46 _condor_stdout<br/>-rwxr-xr-x 1 uscmslocal cms 68K Jul 15 10:09 glidein_startup.sh</pre><br/>Is that what is expected, and if so what file should I be looking at?<br/>
<span style="color: #43761b"><span style="font-size: small">(15:02:46)</span> <b>blin:</b></span> hrm, that's pretty curious 1. that it doesn't exist and 2. that if this is the issue, why we're getting permission denied instead of file doesn't exist<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:03:14)</span> <b>andrew.melo:</b></span> I might be missing something, but it appears it's complaining the file doesn't exist from your snippet above?<br/>
<blockquote>
<span style="color: #43761b"><span style="font-size: small">(15:05:41)</span> <b>blin:</b></span> oh you're right!<br/>
<span style="color: #43761b"><span style="font-size: small">(15:06:05)</span> <b>blin:</b></span> your old logs had permission denied errors<br/>
</blockquote>
<span style="color: #53b759"><span style="font-size: small">(15:03:36)</span> <b>eric.appelt:</b></span> lemme pick another job just to be sure about this<br/>
<span style="color: #53b759"><span style="font-size: small">(15:05:09)</span> <b>eric.appelt:</b></span> Ok, sorry, that was a stale directory, here is an actual running job:<br/><pre>[root@ce5 cluster770297.proc0.subproc0]# ls -lh<br/>total 156K<br/>-rw-r--r-- 1 cmspilot cms 55K Aug 20 09:45 _condor_stderr<br/>-rw-r--r-- 1 cmspilot cms 19K Aug 20 09:45 _condor_stdout<br/>-rw------- 1 cmspilot cms 11K Aug 20 09:33 credential_CMSG-v1_0.main_411868<br/>-rwxr-xr-x 1 cmspilot cms 68K Aug 20 09:33 glidein_startup.sh</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(15:05:29)</span> <b>andrew.melo:</b></span> The last lines of the job submission script have (eric, /tmp on ce5)<br/><pre># Remove the staged files, if any<br/>cd $old_home<br/>rm -f /mnt/ce-spool/ce5.vampire/3308/0/cluster773308.proc0.subproc0/credential_osg-flock-grid-iu-edu_OSG_gWMSFrontend.covid19_357657<br/><br/>exit $user_retcode</pre><br/><br/>
<span style="color: #43761b"><span style="font-size: small">(15:06:31)</span> <b>blin:</b></span> could you send me the full contents of a submission script?<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:06:48)</span> <b>andrew.melo:</b></span> <pre>[root@ce5 ~]# cat /tmp/bl_1fef4f4cf240.debug/submit.script <br/>#!/bin/bash<br/># SLURM job wrapper generated by slurm_submit.sh<br/># on Thu Aug 20 13:29:35 CDT 2020<br/>#<br/># stgcmd = no<br/># proxy_string = /mnt/ce-spool/ce5.vampire/3308/0/cluster773308.proc0.subproc0/credential_osg-flock-grid-iu-edu_OSG_gWMSFrontend.covid19_357657<br/># proxy_local_file = /mnt/ce-spool/ce5.vampire/3308/0/cluster773308.proc0.subproc0/credential_osg-flock-grid-iu-edu_OSG_gWMSFrontend.covid19_357657<br/>#<br/># SLURM directives:<br/>#SBATCH -o /dev/null<br/>#SBATCH -e /dev/null<br/>#SBATCH --nodes=1<br/>#SBATCH --ntasks=1<br/>#SBATCH --cpus-per-task=4<br/># Begin  /etc/blahp/slurm_local_submit_attributes.sh<br/>#SBATCH --mem=28000<br/>#SBATCH --time=48:00:00<br/>#SBATCH --priority=300<br/>#SBATCH --nice=300<br/>export PATH=$PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin<br/>export X509_CERT_DIR=/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/certificates<br/># End    /etc/blahp/slurm_local_submit_attributes.sh<br/><br/># Setting the environment:<br/>export "CONDORCE_COLLECTOR_HOST=<a href="http://ce5.accre.vanderbilt.edu:9619">ce5.accre.vanderbilt.edu:9619</a>"<br/>export "HOME=/home/c19pilot"<br/>export GLOBUS_LOCATION="/usr"<br/>export OSG_APP="/cvmfs/cms.cern.ch"<br/>export OSG_DATA="UNAVAILABLE"<br/>export OSG_GLEXEC_LOCATION="/usr/sbin/glexec"<br/>export OSG_GRID="/etc/osg/wn-client/"<br/>export OSG_HOSTNAME="<a href="http://ce5.accre.vanderbilt.edu">ce5.accre.vanderbilt.edu</a>"<br/>export OSG_SITE_NAME="Vanderbilt_CE5"<br/>export OSG_STORAGE_ELEMENT="False"<br/>export OSG_WN_TMP="/tmp"<br/>export LLGT_VOMS_DISABLE_CREDENTIAL_CHECK=1<br/>old_home=<tt>pwd</tt><br/>new_home=/tmp/home_bl_ce5.accre.vanderbilt.edu_9619_ce5.accre.vanderbilt.edu#773309.0#1597948158<br/>mkdir "$new_home"<br/>job_wait_cleanup () { wait "$job_pid"; cd "$old_home"; rm -rf "$new_home"; }<br/>on_signal () { kill -$1 "$job_pid"; job_wait_cleanup; exit 255; }<br/>trap_sigs () { for sig; do trap "on_signal $sig" $sig; done; }<br/>trap_sigs HUP INT QUIT TERM XCPU<br/>trap job_wait_cleanup EXIT<br/># Copy into new home any shared input sandbox file<br/># Move into new home any relative input sandbox file<br/>export HOME=$new_home<br/>cd $new_home<br/>export X509_USER_PROXY=/mnt/ce-spool/ce5.vampire/3308/0/cluster773308.proc0.subproc0/credential_osg-flock-grid-iu-edu_OSG_gWMSFrontend.covid19_357657<br/><br/># Command to execute:<br/>export NODE_COUNT=4<br/> /mnt/ce-spool/ce5.vampire/3308/0/cluster773308.proc0.subproc0/glidein_startup.sh "-v" "std" "-name" "gfactory_instance" "-entry" "COVID19_T2_US_Vanderbilt_ce5_8core" "-clientname" "osg-flock-grid-iu-edu_OSG_gWMSFrontend.covid19" "-schedd" "<a href="mailto:schedd_glideins7@gfactory-2.opensciencegrid.org">schedd_glideins7@gfactory-2.opensciencegrid.org</a>" "-proxy" "None" "-factory" "OSG" "-web" "<a href="http://gfactory-2.opensciencegrid.org/factory/stage">http://gfactory-2.opensciencegrid.org/factory/stage</a>" "-sign" "69c2c2e31cea24e534deb267e1ab0646f0898259" "-signentry" "909499bd83055ccc7750bc3cffd53ba8a1835402" "-signtype" "sha1" "-descript" "description.k8ka0X.cfg" "-descriptentry" "description.k8ka0X.cfg" "-dir" "OSG" "-param_GLIDEIN_Client" "osg-flock-grid-iu-edu_OSG_gWMSFrontend.covid19" "-submitcredid" "357657" "-slotslayout" "fixed" "-clientweb" "<a href="http://flock.opensciencegrid.org/vofrontend/stage">http://flock.opensciencegrid.org/vofrontend/stage</a>" "-clientsign" "f6b407bc524c75e6cf9cf6a6aa9b70539f6ad151" "-clientsigntype" "sha1" "-clientdescript" "description.k8k9q2.cfg" "-clientgroup" "covid19" "-clientwebgroup" "<a href="http://flock.opensciencegrid.org/vofrontend/stage/group_covid19">http://flock.opensciencegrid.org/vofrontend/stage/group_covid19</a>" "-clientsigngroup" "a4255b05a3a222cc4ab966971032d5530a671bc9" "-clientdescriptgroup" "description.k8k9q2.cfg" "-param_CONDOR_VERSION" "8.dot,9.dot,x" "-param_GLIDEIN_Glexec_Use" "NEVER" "-param_GLIDEIN_Job_Max_Time" "136800" "-param_GLIDECLIENT_ReqNode" "gfactory.minus,2.dot,opensciencegrid.dot,org" "-param_OSG_DEFAULT_CONTAINER_DISTRIBUTION" "95%__opensciencegrid/osgvo.minus,el7.colon,latest.nbsp,3%__opensciencegrid/osgvo.minus,el6.colon,latest.nbsp,2%__opensciencegrid/osgvo.minus,el8.colon,latest" "-param_GLIDEIN_CLAIM_WORKLIFE" "3600" "-param_MIN_DISK_GBS" "1" "-param_GLIDECLIENT_Rank" "1" "-param_OSG_DEFAULT_CONTAINER_DISTRIBUTION_GPU" "100%__opensciencegrid/osgvo.minus,el7.minus,cuda10.colon,latest" "-param_CONDOR_ARCH" "default" "-param_CONDOR_OS" "auto" "-param_UPDATE_COLLECTOR_WITH_TCP" "True" "-param_STARTD_JOB_ATTRS" "x509userproxysubject.comma,x509UserProxyFQAN.comma,x509UserProxyVOName.comma,x509UserProxyEmail.comma,x509UserProxyExpiration.comma,ProjectName" "-param_JAVA" "/usr/bin/java" "-param_USE_MATCH_AUTH" "True" "-param_GLIDEIN_Monitoring_Enabled" "False" "-param_OSG_DEFAULT_CVMFS_DATA" "/cvmfs/stash.dot,osgstorage.dot,org" "-param_GLIDEIN_Report_Failed" "NEVER" "-param_GLIDEIN_Collector" "flock.dot,opensciencegrid.dot,org.colon,9700.minus,9899" "-cluster" "3637175" "-subcluster" "0" &lt; "/dev/null" &gt; "/mnt/ce-spool/ce5.vampire/3308/0/cluster773308.proc0.subproc0/_condor_stdout" 2&gt; "/mnt/ce-spool/ce5.vampire/3308/0/cluster773308.proc0.subproc0/_condor_stderr" &amp;<br/>job_pid=$!<br/><br/># Wait for the user job to finish<br/>wait $job_pid<br/>user_retcode=$?<br/><br/># Move all relative outputsand paths out of temp home<br/>cd $new_home<br/># Move any remapped outputsand file to shared directories<br/><br/># Remove the staged files, if any<br/>cd $old_home<br/>rm -f /mnt/ce-spool/ce5.vampire/3308/0/cluster773308.proc0.subproc0/credential_osg-flock-grid-iu-edu_OSG_gWMSFrontend.covid19_357657<br/><br/>exit $user_retcode<br/>[root@ce5 ~]# </pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(15:10:22)</span> <b>andrew.melo:</b></span> It seems (?) ok?<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:10:41)</span> <b>andrew.melo:</b></span> I'm not sure why blah is considering the credential a staged file, though<br/>
<span style="color: #43761b"><span style="font-size: small">(15:10:47)</span> <b>blin:</b></span> yeah, agreed<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:11:04)</span> <b>andrew.melo:</b></span> We have this set: <tt>blah_shared_directories=/mnt/ce-spool/</tt><br/>
<span style="color: #43761b"><span style="font-size: small">(15:11:25)</span> <b>blin:</b></span> generally we leave that as <tt>/</tt><br/>
<span style="color: #43761b"><span style="font-size: small">(15:11:38)</span> <b>blin:</b></span> but i don't think your setting should have any negative effect<br/>
<span style="color: #43761b"><span style="font-size: small">(15:22:20)</span> <b>blin:</b></span> does this happen for all users or just the cmslocal and cmspilot users?<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:24:28)</span> <b>andrew.melo:</b></span> I don't have a good handle on that, the only other jobs that come through the CE is covid pilots<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:25:05)</span> <b>andrew.melo:</b></span> Out of 71 pilots, ~65 of them have been running &lt; 1 hour<br/>
<span style="color: #43761b"><span style="font-size: small">(15:29:41)</span> <b>blin:</b></span> could you check to see how many running jobs have proxies that exist? something like this should give you an idea:<br/><pre>condor_ce_q -const 'routedjob isnt undefined &amp;&amp; jobstatus == 2' -format '%s/' Iwd -format '%s\n' x509userproxy | xargs ls -l &gt; /dev/null</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(15:30:38)</span> <b>andrew.melo:</b></span> null<br/>
<span style="color: #43761b"><span style="font-size: small">(15:32:38)</span> <b>blin:</b></span> are your CEs sharing the same NFS?<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:33:50)</span> <b>andrew.melo:</b></span> Each CE exports it's local <tt>/mnt/ce-spool</tt> to NFS, then the worker nodes autofs-mount each CE's NFS mount<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:34:07)</span> <b>andrew.melo:</b></span> (e.g. /mnt/ce-spool/ce5.vampire, /mnt/ce-spool/ce6.vampire)<br/>
<span style="color: #43761b"><span style="font-size: small">(15:36:29)</span> <b>blin:</b></span> were the old CEs setup like this, too?<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:37:11)</span> <b>andrew.melo:</b></span> yeah<br/>
</body>
</html>

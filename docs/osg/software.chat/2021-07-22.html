<!DOCTYPE html>
<html>
<head>
<title>Thu Jul 22, 2021 : #software (osg)</title>
</head>
<body>
<h3>Thu Jul 22, 2021 : #software (osg)</h3>
<span style="color: #73769d"><span style="font-size: small">(08:21:43)</span> <b>tim.theisen:</b></span> I am putting together the release announcement for today's release. @justas.balcas Mat suggested the you might know specific fixes to XRootD 5.3.0 that I should mention in the announcement.<br/>
<span style="color: #de5f24"><span style="font-size: small">(08:25:27)</span> <b>justas.balcas:</b></span> Me? I think @didavila<br/>
<blockquote>
<span style="color: #a2a5dc"><span style="font-size: small">(11:32:22)</span> <b>didavila:</b></span> The only thing I requested was a fix on the process that concatenates all the CA certs in a single file since that was failing for old versions of CentOS<br/>
</blockquote>
<span style="color: #c386df"><span style="font-size: small">(08:45:53)</span> <b>matyas:</b></span> weren't there fixes you were interested in?<br/>
<span style="color: #c386df"><span style="font-size: small">(08:45:58)</span> <b>matyas:</b></span> or was that 5.2.0?<br/>
<span style="color: #de5f24"><span style="font-size: small">(08:53:00)</span> <b>justas.balcas:</b></span> I think all ended up in 5.2.0.rc1<br/>
<span style="color: #5870dd"><span style="font-size: small">(09:50:46)</span> <b>duncan.macleod:</b></span> sorry if this isn't the best place to post this, but I am seeing CVMFS user requests hanging for long periods (perhaps indefinitely) with the log message:<br/><pre>(<a href="http://ligo.osgstorage.org">ligo.osgstorage.org</a>) another process holds /var/lib/cvmfs/lock_cachedb, waiting.</pre><br/>(a) is that normal,<br/>(b) if it persists, can I just remove the (presumably stale) <tt>lock_cachedb</tt> file?<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:52:18)</span> <b>dweitzel:</b></span> Doesn't seem normal.  I haven't seen it before.  @dwd?<br/>
<span style="color: #5870dd"><span style="font-size: small">(10:10:40)</span> <b>duncan.macleod:</b></span> I just manually deleted the lock file and things seem to be recovering, fingers crossed that this isn't a terrible thing to have done...<br/>
<span style="color: #99a949"><span style="font-size: small">(10:30:06)</span> <b>dwd:</b></span> I don’t believe I have seen that before.  What kind of CVMFS user requests are you talking about @duncan.macleod?  Is it ordinary accesses to files under /cvmfs/ligo.osgstorage.org?<br/>
<span style="color: #99a949"><span style="font-size: small">(10:33:31)</span> <b>dwd:</b></span> In fact I am wondering why that path is <tt>/var/lib/cvmfs/lock_cachedb</tt> and not <tt>/var/lib/cvmfs/osgstorage/shared/lock_cachedb</tt>.  That apears to be a non-standard OSG configuration.<br/>
<span style="color: #99a949"><span style="font-size: small">(10:36:54)</span> <b>dwd:</b></span> In looking at cvmfs code it appears to me that lock is held by the cache manager process the entire time it is running.<br/>
<span style="color: #5870dd"><span style="font-size: small">(10:40:44)</span> <b>duncan.macleod:</b></span> this is just ordinary file requests for files in <tt>/cvmfs/ligo.osgstorage.org</tt> (via the symlink from <tt>/cvmfs/oasis.opensciencegrid.org/ligo/frames</tt>)<br/>
<span style="color: #99a949"><span style="font-size: small">(10:41:48)</span> <b>dwd:</b></span> I expect that lock was not stale, that there actually was a cvmfs2 process still running that was holding that lock.  On my machine I just tried doing a kill -9 on the osgstorage cache manager cvmfs2 processes which left the lock_cachedb file there and it recovered by itself when I remounted <a href="http://ligo.osgstorage.org">ligo.osgstorage.org</a>.<br/>
<span style="color: #5870dd"><span style="font-size: small">(10:42:14)</span> <b>duncan.macleod:</b></span> re path: the <tt>/etc/cvmfs/default.conf</tt> includes<br/><pre>CVMFS_CACHE_BASE=/var/lib/cvmfs</pre><br/>
<span style="color: #5870dd"><span style="font-size: small">(10:42:52)</span> <b>duncan.macleod:</b></span> but there _is_ a <tt>lock_cachedb</tt> file under <tt>/var/lib/cvmfs/osgstorage/shared/</tt> as well<br/>
<span style="color: #99a949"><span style="font-size: small">(10:42:54)</span> <b>dwd:</b></span> Sure but other configuration in the config repository adds the osgstorage and the shared<br/>
<span style="color: #5870dd"><span style="font-size: small">(10:43:42)</span> <b>duncan.macleod:</b></span> we have <tt>/etc/cvmfs/domain.d/osgstorage.org.local</tt> that includes:<br/><pre>CVMFS_WORKSPACE=/var/lib/cvmfs</pre><br/>which might explain that<br/>
<span style="color: #99a949"><span style="font-size: small">(10:43:48)</span> <b>dwd:</b></span> Ah, yes<br/>
<span style="color: #5870dd"><span style="font-size: small">(10:44:02)</span> <b>duncan.macleod:</b></span> that predates my administration, so I can't tell you why that was instituted<br/>
<span style="color: #99a949"><span style="font-size: small">(10:45:55)</span> <b>dwd:</b></span> I don’t think that should cause harm but I’m not entirely sure what the implications are and if I were you, I would remove that.<br/>
<span style="color: #5870dd"><span style="font-size: small">(10:46:28)</span> <b>duncan.macleod:</b></span> the local domain config defines a tiered cache between <tt>/var/lib/cvmfs</tt> and an alien cache somewhere else:<br/><pre>$ cat /etc/cvmfs/domain.d/osgstorage.org.local<br/># Set the local directory for storing special files (defaults to the cache directory).<br/>CVMFS_WORKSPACE=/var/lib/cvmfs<br/>CVMFS_QUOTA_LIMIT=10000<br/><br/># Tiered Cache The tiered cache manager combines two other cache manager instances as<br/># an upper layer and a lower layer into a single functional cache manager.<br/># Usually, a small and fast upper layer (SSD, memory) is combined with a larger<br/># and slower lower layer (HDD, network drive).<br/>CVMFS_CACHE_PRIMARY=hpc<br/>#<br/>CVMFS_CACHE_hpc_TYPE=tiered<br/>CVMFS_CACHE_hpc_UPPER=disk<br/>CVMFS_CACHE_hpc_LOWER=scratch<br/>CVMFS_CACHE_hpc_LOWER_READONLY=no<br/><br/>#posix Uses a cache directory with the standard cache implementation<br/>CVMFS_CACHE_disk_TYPE=posix<br/>CVMFS_CACHE_disk_BASE=/var/lib/cvmfs/<br/><br/># An "alien cache" provides the possibility to use a data cache outside the control of CernVM-FS.<br/># It is safe to have the alien directory shared by multiple CernVM-FS processes and it is safe to unlink files from the alien cache directory anytime.<br/># Since the alien cache is unmanaged, there is no automatic quota management provided by CernVM-FS;<br/># the alien cache directory is ever-growing. The CVMFS_ALIEN_CACHE requires CVMFS_QUOTA_LIMIT=-1 and CVMFS_SHARED_CACHE=no.<br/>CVMFS_CACHE_scratch_TYPE=posix<br/>CVMFS_CACHE_scratch_ALIEN=/scratch/LIGO/cvmfs<br/>CVMFS_CACHE_scratch_SHARED=no<br/>CVMFS_CACHE_scratch_QUOTA_LIMIT=-1</pre><br/><br/>
<span style="color: #5870dd"><span style="font-size: small">(10:48:36)</span> <b>duncan.macleod:</b></span> In no way do I consider myself an expert on CVMFS and/or caching with CVMFS, so if there is a better way to construct that sort of tiered cache, please educate me<br/>
<span style="color: #5870dd"><span style="font-size: small">(10:48:50)</span> <b>duncan.macleod:</b></span> and if there's a better venue to have this discussion, again, please educate me<br/>
<span style="color: #99a949"><span style="font-size: small">(10:55:08)</span> <b>dwd:</b></span> I’m also not much of an expert on tiered cache, we’d probably need Jakob Blomer for that.   Do you understand what the goal is for tiered cache for these machines?   I would guess that they’re attempting to provide a larger cache for the data files under /cvmfs/ligo.osgstorage.org.   I am guessing their might be a conflict if other <a href="http://osgstorage.org">osgstorage.org</a> repositories are mounted, and maybe this should only be a config.d/ligo.osgstorage.org.local config and not something for all <a href="http://osgstorage.org">osgstorage.org</a><br/>
<span style="color: #99a949"><span style="font-size: small">(10:55:51)</span> <b>dwd:</b></span> It’s probably also a bad idea to use /var/lib/cvmfs as the base as opposed to some subdirectory under there.<br/>
<blockquote>
<span style="color: #5870dd"><span style="font-size: small">(11:00:58)</span> <b>duncan.macleod:</b></span> would you recommend just updating the <tt>CVMFS_WORKSPACE</tt> to <tt>/var/lib/cvmfs/osgstorage</tt>? or something new altogether<br/>
</blockquote>
<span style="color: #5870dd"><span style="font-size: small">(10:57:55)</span> <b>duncan.macleod:</b></span> so, we mount <a href="http://ligo.osgstorage.org">ligo.osgstorage.org</a> and <a href="http://gwosc.osgstorage.org">gwosc.osgstorage.org</a>.<br/><br/>I _think_ the tiered cache is there because we already mount a bunch of LIGO data as a legacy of the pre-CVMFS era, and the tiered alien cache allows us to import that existing data to support <a href="http://ligo.osgstorage.org">ligo.osgstorage.org</a>?<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:00:58)</span> <b>duncan.macleod:</b></span> would you recommend just updating the <tt>CVMFS_WORKSPACE</tt> to <tt>/var/lib/cvmfs/osgstorage</tt>? or something new altogether<br/>
<blockquote>
<span style="color: #5870dd"><span style="font-size: small">(11:00:58)</span> <b>duncan.macleod:</b></span> would you recommend just updating the <tt>CVMFS_WORKSPACE</tt> to <tt>/var/lib/cvmfs/osgstorage</tt>? or something new altogether<br/>
</blockquote>
<span style="color: #99a949"><span style="font-size: small">(11:04:37)</span> <b>dwd:</b></span> You might be getting clashes between the two repositories.  I think CVMFS_WORKSPACE and CVMFS_CACHE_disk_BASE should be some new subdirectory, and that the configuration should only be applied in config.d/ligo.osgstorage.org.local (if at all).  I’m still unsure about whether or not this is doing something valuable.  What do the files under /scratch/LIGO/cvmfs look like?<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:06:00)</span> <b>duncan.macleod:</b></span> <pre>$ ls /scratch/LIGO/<br/>aLIGO  cvmfs  FILES_TO_CHECK.txt  frames  SUSPECT_FILES.txt<br/>$ ls /scratch/LIGO/cvmfs/<br/>00  12  24  36  48  5a  6c  7e  90  a2  b4  c6                                  d6  e8  fa<br/>01  13  25  37  49  5b  6d  7f  91  a3  b5  c7                                  d7  e9  fb<br/>02  14  26  38  4a  5c  6e  80  92  a4  b6  c8                                  d8  ea  fc<br/>03  15  27  39  4b  5d  6f  81  93  a5  b7  c9                                  d9  eb  fd<br/>04  16  28  3a  4c  5e  70  82  94  a6  b8  ca                                  da  ec  fe<br/>05  17  29  3b  4d  5f  71  83  95  a7  b9  cb                                  db  ed  ff<br/>06  18  2a  3c  4e  60  72  84  96  a8  ba  cc                                  dc  ee  quarantaine<br/>07  19  2b  3d  4f  61  73  85  97  a9  bb  cd                                  dd  ef  txn<br/>08  1a  2c  3e  50  62  74  86  98  aa  bc  ce                                  de  f0<br/>09  1b  2d  3f  51  63  75  87  99  ab  bd  cf                                  df  f1<br/>0a  1c  2e  40  52  64  76  88  9a  ac  be  <a href="http://cvmfschecksum.gwosc.osgstorage.org">cvmfschecksum.gwosc.osgstorage.org</a>  e0  f2<br/>0b  1d  2f  41  53  65  77  89  9b  ad  bf  <a href="http://cvmfschecksum.ligo.osgstorage.org">cvmfschecksum.ligo.osgstorage.org</a>   e1  f3<br/>0c  1e  30  42  54  66  78  8a  9c  ae  c0  d0                                  e2  f4<br/>0d  1f  31  43  55  67  79  8b  9d  af  c1  d1                                  e3  f5<br/>0e  20  32  44  56  68  7a  8c  9e  b0  c2  d2                                  e4  f6<br/>0f  21  33  45  57  69  7b  8d  9f  b1  c3  d3                                  e5  f7<br/>10  22  34  46  58  6a  7c  8e  a0  b2  c4  d4                                  e6  f8<br/>11  23  35  47  59  6b  7d  8f  a1  b3  c5  d5                                  e7  f9</pre><br/><br/>
<span style="color: #99a949"><span style="font-size: small">(11:06:32)</span> <b>dwd:</b></span> What’s under “frames”?<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:08:05)</span> <b>duncan.macleod:</b></span> it's basically the same scheme as /cvmfs/ligo.osgstorage.org<br/><pre>$ find /scratch/LIGO/frames -type f | head -n 10<br/>/scratch/LIGO/frames/O3/hoft_C01/L1/L-L1_HOFT_C01-12383/L-L1_HOFT_C01-1238360064-4096.gwf<br/>/scratch/LIGO/frames/O3/hoft_C01/L1/L-L1_HOFT_C01-12383/L-L1_HOFT_C01-1238372352-4096.gwf<br/>/scratch/LIGO/frames/O3/hoft_C01/L1/L-L1_HOFT_C01-12383/L-L1_HOFT_C01-1238315008-4096.gwf<br/>/scratch/LIGO/frames/O3/hoft_C01/L1/L-L1_HOFT_C01-12383/L-L1_HOFT_C01-1238368256-4096.gwf<br/>/scratch/LIGO/frames/O3/hoft_C01/L1/L-L1_HOFT_C01-12383/L-L1_HOFT_C01-1238384640-4096.gwf<br/>/scratch/LIGO/frames/O3/hoft_C01/L1/L-L1_HOFT_C01-12383/L-L1_HOFT_C01-1238323200-4096.gwf<br/>/scratch/LIGO/frames/O3/hoft_C01/L1/L-L1_HOFT_C01-12383/L-L1_HOFT_C01-1238327296-4096.gwf<br/>/scratch/LIGO/frames/O3/hoft_C01/L1/L-L1_HOFT_C01-12383/L-L1_HOFT_C01-1238396928-4096.gwf<br/>/scratch/LIGO/frames/O3/hoft_C01/L1/L-L1_HOFT_C01-12383/L-L1_HOFT_C01-1238388736-4096.gwf<br/>/scratch/LIGO/frames/O3/hoft_C01/L1/L-L1_HOFT_C01-12383/L-L1_HOFT_C01-1238355968-4096.gwf</pre><br/><br/>
<span style="color: #99a949"><span style="font-size: small">(11:08:41)</span> <b>dwd:</b></span> I wonder if those were created by hand or by cvmfs<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:08:54)</span> <b>duncan.macleod:</b></span> those were populated by hand<br/>
<span style="color: #99a949"><span style="font-size: small">(11:09:29)</span> <b>dwd:</b></span> I wonder if cvmfs actually uses them, do you know?<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:09:44)</span> <b>duncan.macleod:</b></span> how would I check that?<br/>
<span style="color: #99a949"><span style="font-size: small">(11:10:20)</span> <b>dwd:</b></span> I suppose I would set CVMFS_DEBUGLOG and look to see what it is doing when you access one of the files<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:11:58)</span> <b>duncan.macleod:</b></span> ok, I don't have time to try that right now (it's already past 5pm here), but I will try that in the morning and see what I can see.<br/><br/>I will also experiment with separating the caches for the two <a href="http://osgstorage.org">osgstorage.org</a> repositories and see what happens - we definitely don't have any local copies of GWOSC data here anyway, so that repo won't gain anything from the current config<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:12:06)</span> <b>duncan.macleod:</b></span> thanks @dwd for all of your help<br/>
<span style="color: #99a949"><span style="font-size: small">(11:12:40)</span> <b>dwd:</b></span> Actually, there’s nothing in the cvmfs config that refers to /scratch/LIGO/frames so cvmfs must be ignoring them.<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:16:48)</span> <b>duncan.macleod:</b></span> ah, ok, so the alien cache is doing what exactly? nothing? or is that just enabling use of a large disk for local caching on that host?<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:17:46)</span> <b>duncan.macleod:</b></span> <tt>/scratch</tt> is our big lustre file system<br/>
<span style="color: #99a949"><span style="font-size: small">(11:20:29)</span> <b>dwd:</b></span> I think it is only enabling the use of a large, unmanaged disk for caching.  Ah, it is on lustre, so it is shared between multiple machines, that might have some advantage.  On my test machine I read in one of those ligo frames files and it stored it in multiple 24MB chunks named by hash, not named by the original file name.<br/>
<span style="color: #99a949"><span style="font-size: small">(11:26:02)</span> <b>dwd:</b></span> So probably we should continue this conversation with Jakob.  Maybe the best way to do that would be to create a <a href="https://sft.its.cern.ch/jira/browse/CVM">https://sft.its.cern.ch/jira/browse/CVM</a> ticket with the message you’re seeing, the osgstorage.org.local config file, and a note that /scratch is on lustre.  Do you have a CERN login?<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:28:07)</span> <b>duncan.macleod:</b></span> I don't think so, can I just make one?<br/>
<span style="color: #99a949"><span style="font-size: small">(11:30:01)</span> <b>dwd:</b></span> Possibly. Please try.  They  have a concept of “light weight account” and “guest access” (depending on whether using the old or new authentication systems) but I don’t know if that extends to the jira.<br/>
<span style="color: #99a949"><span style="font-size: small">(11:33:25)</span> <b>dwd:</b></span> Nevermind, I tried using my LIGO account or my Google account and got nowhere.<br/>
<span style="color: #99a949"><span style="font-size: small">(11:34:06)</span> <b>dwd:</b></span> I’ll create a ticket and try to at least add your email as a watcher if I can<br/>
<span style="color: #99a949"><span style="font-size: small">(11:36:36)</span> <b>dwd:</b></span> What cvmfs client version is installed?<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:38:39)</span> <b>duncan.macleod:</b></span> sorry, I have to sign off, now, <tt>cvmfs2 --version</tt> reports 2.7.3<br/>
<span style="color: #99a949"><span style="font-size: small">(11:39:19)</span> <b>dwd:</b></span> Ok I think I’ll kick it off on a github issue, anybody can use that.<br/>
<span style="color: #5870dd"><span style="font-size: small">(11:58:16)</span> <b>duncan.macleod:</b></span> thanks again @dwd<br/>
<span style="color: #43761b"><span style="font-size: small">(12:08:47)</span> <b>blin:</b></span> hey @rynge why do we need to add the validation script to add the GPU job preference to the osgvo container? <a href="https://github.com/opensciencegrid/osgvo-docker-pilot/pull/20/files">https://github.com/opensciencegrid/osgvo-docker-pilot/pull/20/files</a><br/>
<span style="color: #674b1b"><span style="font-size: small">(12:40:16)</span> <b>rynge:</b></span> I think it was just a grouping of outstanding stuff<br/>
<span style="color: #43761b"><span style="font-size: small">(12:41:49)</span> <b>blin:</b></span> it doesn't work in the singularity case unless they bind mount something to <tt>/pilot</tt> and specify <tt>/pilot</tt> as the working dir because of the disk space check<br/>
<span style="color: #43761b"><span style="font-size: small">(12:42:00)</span> <b>blin:</b></span> i'm not sure what it does in the docker case<br/>
<span style="color: #674b1b"><span style="font-size: small">(17:19:47)</span> <b>rynge:</b></span> Hmm, ok, let's back it out then<br/>
<span style="color: #674b1b"><span style="font-size: small">(17:20:09)</span> <b>rynge:</b></span> You want a PR, or how does this work with your custom tags?<br/>
<span style="color: #674b1b"><span style="font-size: small">(17:20:56)</span> <b>rynge:</b></span> Actually, I think they have to bind mount to /pilot. Otherwise that will be a read-only dir<br/>
</body>
</html>

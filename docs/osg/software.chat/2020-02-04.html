<!DOCTYPE html>
<html>
<head>
<title>Tue Feb 4, 2020 : #software (osg)</title>
</head>
<body>
<h3>Tue Feb 4, 2020 : #software (osg)</h3>
<span style="color: #a2a5dc"><span style="font-size: small">(10:40:53)</span> <b>didavila:</b></span> <b>@here</b> does anybody know when is moria coming back?<br/>
<span style="color: #c386df"><span style="font-size: small">(10:51:04)</span> <b>matyas:</b></span> we're still not sure what the problem is. The machine itself is functioning but connections are flaky<br/>
<span style="color: #c386df"><span style="font-size: small">(10:51:51)</span> <b>matyas:</b></span> unfortunately CHTC doesn't control the firewall or the network so we have to wait for CS or campus IT to do something<br/>
<span style="color: #c386df"><span style="font-size: small">(10:52:30)</span> <b>matyas:</b></span> in the meantime, if you want to put some files in the upstream directory, one of the Madison folks can do that for you<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:07:34)</span> <b>u0928244:</b></span> Does anyone know why this keeps happening to me?<br/><br/><pre>[root@mitchell ~]# mount -t cvmfs <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a> /cvmfs/config-osg.opensciencegrid.org/<br/>CernVM-FS: running with credentials 997:994<br/>CernVM-FS: loading Fuse module... done<br/>CernVM-FS: mounted cvmfs on /cvmfs/config-osg.opensciencegrid.org<br/>[root@mitchell ~]# ls -la /cvmfs/<br/>ls: cannot access /cvmfs/config-osg.opensciencegrid.org: No such file or directory<br/>total 0<br/>drwxr-xr-x  12 root  root  287 Feb  4 09:44 .<br/>dr-xr-xr-x. 18 root  root  275 Feb  4 09:54 ..<br/>drwxr-xr-x   2 cvmfs cvmfs   6 Feb  4 09:43 <a href="http://atlas.cern.ch">atlas.cern.ch</a><br/>drwxr-xr-x   2 cvmfs cvmfs   6 Feb  4 09:43 <a href="http://cms.cern.ch">cms.cern.ch</a><br/>d??????????  ? ?     ?       ?            ? <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a></pre><br/>
<span style="color: #619a4f"><span style="font-size: small">(11:08:07)</span> <b>u0928244:</b></span> It works sometimes…<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:17:32)</span> <b>rynge:</b></span> @dwd ^^^<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:19:14)</span> <b>u0928244:</b></span> Here’s some more info<br/><br/><pre>cvmfs2 on /cvmfs/config-osg.opensciencegrid.org type fuse (ro,nosuid,nodev,relatime,user_id=0,group_id=0,default_permissions,allow_other)</pre><br/><pre>[root@mitchell ~]# rpm -qa |grep cvmfs<br/>cvmfs-2.7.0-1.osg35.el7.x86_64<br/>cvmfs-config-osg-2.4-1.osg35.el7.noarch</pre><br/><pre>[root@mitchell ~]# cat /etc/cvmfs/default.local<br/>CVMFS_SERVER_URL="<a href="http://cvmfs-s1bnl.opensciencegrid.org:8000/cvmfs/@fqrn@;http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/@fqrn@;http://cvmfs-s1goc.opensciencegrid.org:8000/cvmfs/@fqrn@">http://cvmfs-s1bnl.opensciencegrid.org:8000/cvmfs/@fqrn@;http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/@fqrn@;http://cvmfs-s1goc.opensciencegrid.org:8000/cvmfs/@fqrn@</a>"<br/>CVMFS_KEYS_DIR=/etc/cvmfs/keys/opensciencegrid.org/<br/>CVMFS_USE_GEOAPI=yes<br/>CVMFS_HTTP_PROXY="<a href="http://squid.chpc.utah.edu:3128">http://squid.chpc.utah.edu:3128</a>"<br/>CVMFS_QUOTA_LIMIT=20000<br/>CVMFS_REPOSITORIES=<a href="http://atlas.cern.ch">atlas.cern.ch</a>,<a href="http://sft.cern.ch">sft.cern.ch</a>,<a href="http://geant4.cern.ch">geant4.cern.ch</a>,<a href="http://grid.cern.ch">grid.cern.ch</a>,<a href="http://cms.cern.ch">cms.cern.ch</a>,<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>,<a href="http://connect.opensciencegrid.org">connect.opensciencegrid.org</a>,<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>,<a href="http://singularity.opensciencegird.org">singularity.opensciencegird.org</a>,<a href="http://icecube.opensciencegrid.org">icecube.opensciencegrid.org</a><br/>CVMFS_NFS_SOURCE=yes<br/>CVMFS_MEMCACHE_SIZE=256</pre><br/><br/>
<span style="color: #99a949"><span style="font-size: small">(11:25:06)</span> <b>dwd:</b></span> @u0928244 Well I have seen that type of ls listing in the past on older versions of cvmfs after attempting to access a cvmfs file that isn’t there yet; it is apparently what shows up from a negative-response from the fuse kernel cache.  The question is why is it happening in the first place.  The feature that caused it was removed in version 2.6.3 (I believe) but put back in 2.7.0 after revision, and it looks like it still has some problems.  Are you using the automounter?<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:26:08)</span> <b>u0928244:</b></span> I am mounting manually.<br/>
<span style="color: #99a949"><span style="font-size: small">(11:26:43)</span> <b>dwd:</b></span> Oh CVMFS_NFS_SOURCE means this is an nfs server, is that right?<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:26:56)</span> <b>u0928244:</b></span> It’s going to be<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:27:21)</span> <b>u0928244:</b></span> We had an NFS exporter to share CVMFS to the nodes, but it blew up on us last week. I’m trying to put it back together.<br/>
<span style="color: #99a949"><span style="font-size: small">(11:28:32)</span> <b>dwd:</b></span> It has no nfs clients yet though?  Maybe I can reproduce.<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:30:20)</span> <b>u0928244:</b></span> It doesn’t have clients and the machine is not sharing anything out yet. This is pretty much a fresh setup, and I’m just trying to get CVMFS working across reboots at this stage.<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:32:48)</span> <b>u0928244:</b></span> In fstab I have the repos mounting at boot<br/><pre># CVMFS REPOS<br/><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>          /cvmfs/config-osg.opensciencegrid.org   cvmfs   defaults,_netdev	0 0<br/><a href="http://atlas.cern.ch">atlas.cern.ch</a>				/cvmfs/atlas.cern.ch			cvmfs	defaults,_netdev	0 0<br/><a href="http://sft.cern.ch">sft.cern.ch</a>				/cvmfs/sft.cern.ch			cvmfs	defaults,_netdev	0 0<br/><a href="http://geant4.cern.ch">geant4.cern.ch</a>				/cvmfs/geant4.cern.ch			cvmfs	defaults,_netdev	0 0<br/><a href="http://grid.cern.ch">grid.cern.ch</a>				/cvmfs/grid.cern.ch			cvmfs	defaults,_netdev	0 0<br/><a href="http://cms.cern.ch">cms.cern.ch</a>				/cvmfs/cms.cern.ch			cvmfs	defaults,_netdev	0 0<br/><a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>		/cvmfs/oasis.opensciencegrid.org	cvmfs	defaults,_netdev	0 0<br/><a href="http://connect.opensciencegrid.org">connect.opensciencegrid.org</a>		/cvmfs/connect.opensciencegrid.org	cvmfs	defaults,_netdev	0 0<br/><a href="http://singularity.opensciencegrid.org">singularity.opensciencegrid.org</a>		/cvmfs/singularity.opensciencegird.org	cvmfs	defaults,_netdev	0 0<br/><a href="http://icecube.opensciencegrid.org">icecube.opensciencegrid.org</a>		/cvmfs/icecube.opensciencegrid.org	cvmfs	defaults,_netdev	0 0</pre><br/>There doesn’t seem to be a way around the fact that the boot process is going to fork each mount, which causes a dependency issue with the keys in <tt><a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a></tt> . So, I also run <tt>mount -a</tt> in rc.local<br/>
<span style="color: #99a949"><span style="font-size: small">(11:36:20)</span> <b>dwd:</b></span> The first 3 lines your /etc/cvmfs/default.local shouldn’t be there, although that shouldn’t cause this problem.  Those things are defined by cvmfs-config-osg.   Your boot sequence might be causing the problem.  I recommend putting “noauto” in all those entries, and use cvmfs_server mount -a instead of mount -a in rc.local.<br/>
<span style="color: #99a949"><span style="font-size: small">(11:36:47)</span> <b>dwd:</b></span> Actually come to think it, that might not do it…<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:36:56)</span> <b>andrew.melo:</b></span> you could noauto everything except <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a><br/>
<span style="color: #a72f79"><span style="font-size: small">(11:37:10)</span> <b>andrew.melo:</b></span> grrr, dave beat me to it :slightly_smiling_face:<br/>
<span style="color: #99a949"><span style="font-size: small">(11:37:27)</span> <b>dwd:</b></span> No, cvmfs_server won’t do it, that’s for stratum 0s &amp; 1s.<br/>
<span style="color: #99a949"><span style="font-size: small">(11:38:14)</span> <b>dwd:</b></span> But you could do your own loop looking in /etc/fstab, that would probably be better than trying to do the parallel mounts.<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:38:42)</span> <b>u0928244:</b></span> Ok I removed the first 3 lines in the default.local file.<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:41:08)</span> <b>u0928244:</b></span> I’ve had the boot sequence bring up all the repos correctly, and I’ve also had this fail when mounting manually (as shown above) without booting.<br/><br/>I’m open to suggestions on the best way to handle the mounts though.<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:41:43)</span> <b>u0928244:</b></span> In the past we used autofs to mount dynamically, then there was a cron which created sperate bind mounts from those and shared that via NFS.<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:42:04)</span> <b>u0928244:</b></span> I was hoping to cut the cron from this equation and use <tt>CVMFS_NFS_SOURCE=yes</tt><br/>
<span style="color: #99a949"><span style="font-size: small">(11:42:06)</span> <b>dwd:</b></span> <tt>awk '$3 == "cvmfs" {print $1}' /etc/fstab|while read REPO; do mount /cvmfs/$REPO; done</tt><br/>
<span style="color: #99a949"><span style="font-size: small">(11:46:19)</span> <b>dwd:</b></span> I will say that people have come to me with lots of problems with serving cvmfs over NFS, and I always try to steer people away from it if at all possible.  It requires cobbling together a lot of things.  This is the first I have heard of CVMFS_NFS_SOURCE that I recall.<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:48:29)</span> <b>u0928244:</b></span> Yeah I would like to avoid doing things this way, but I think at this time it’s the only way to get CVMFS on the nodes.<br/>
<span style="color: #99a949"><span style="font-size: small">(11:49:17)</span> <b>dwd:</b></span> What’s preventing installing cvmfs native on the nodes?<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:49:51)</span> <b>u0928244:</b></span> The admins do not want to run a fuse file system<br/>
<span style="color: #99a949"><span style="font-size: small">(11:50:04)</span> <b>dwd:</b></span> Is it an HPC system?<br/>
<span style="color: #619a4f"><span style="font-size: small">(11:50:09)</span> <b>u0928244:</b></span> Yes<br/>
<span style="color: #99a949"><span style="font-size: small">(11:50:59)</span> <b>dwd:</b></span> Part of the XSEDE collaboration?<br/>
<span style="color: #99a949"><span style="font-size: small">(11:51:58)</span> <b>dwd:</b></span> Have they installed singularity?<br/>
<span style="color: #99a949"><span style="font-size: small">(11:57:37)</span> <b>dwd:</b></span> The people who distribute software for XSEDE are planning on distributing it with cvmfs and asking all the HPCs to install it natively on the nodes.<br/>
<span style="color: #99a949"><span style="font-size: small">(11:58:16)</span> <b>dwd:</b></span> Is it a Cray?<br/>
<span style="color: #99a949"><span style="font-size: small">(11:59:05)</span> <b>dwd:</b></span> <a href="https://cvmfs.readthedocs.io/en/stable/cpt-hpc.html#using-singularity-pre-mounts">https://cvmfs.readthedocs.io/en/stable/cpt-hpc.html#using-singularity-pre-mounts</a><br/>
<span style="color: #99a949"><span style="font-size: small">(11:59:31)</span> <b>dwd:</b></span> I am going away for an hour for a lunch meeting …<br/>
<span style="color: #619a4f"><span style="font-size: small">(12:02:02)</span> <b>u0928244:</b></span> Sorry, I had to ask some specifics on this. We help provide access to XSEDE resources, and we have a campus champion on staff, but we are not a resource provider.<br/>
<span style="color: #619a4f"><span style="font-size: small">(12:02:11)</span> <b>u0928244:</b></span> We do have Singularity to answer your second question.<br/>
<span style="color: #619a4f"><span style="font-size: small">(12:06:32)</span> <b>u0928244:</b></span> I’ll look into the viability of using singularity pre mounts.<br/>
<span style="color: #99a949"><span style="font-size: small">(13:26:02)</span> <b>dwd:</b></span> @u0928244 It does unfortunately require some cooperation with the VO running the jobs, because they will need to invoke singularity with the right options.  This is a new feature and no VO has adapted to use it yet, but if they do I think the experience will be much better than using an NFS server.  I think we could get the OSG VO to adapt their pilots for it if there is an HPC site where it can be used.<br/>
<span style="color: #99a949"><span style="font-size: small">(13:31:40)</span> <b>dwd:</b></span> Meanwhile I am still interested in trying to figure out if we can find a way to fix your problem with mounting repositories at boot time.<br/>
<span style="color: #99a949"><span style="font-size: small">(13:33:09)</span> <b>dwd:</b></span> It looks like you also support CMS &amp; ATLAS; I think we could get CMS to adapt to, but ATLAS might take longer.<br/>
<span style="color: #619a4f"><span style="font-size: small">(14:35:43)</span> <b>u0928244:</b></span> Ok interesting, we will have to explore this further.<br/><br/>I used your loop in my rc.local file and that seemed to get things functioning across reboots, but I haven’t added the NFS bits yet.<br/>
</body>
</html>

<!DOCTYPE html>
<html>
<head>
<title>Fri Jan 10, 2020 : #software (osg)</title>
</head>
<body>
<h3>Fri Jan 10, 2020 : #software (osg)</h3>
<span style="color: #c386df"><span style="font-size: small">(09:34:07)</span> <b>matyas:</b></span> anyone know if there's a URL for downloading the source tarball of the "latest" github release of something, whatever latest is?<br/>
<span style="color: #e06b56"><span style="font-size: small">(09:35:41)</span> <b>jthiltges:</b></span> Maybe this format? <a href="https://github.com/bbockelm/htcondor_jobview/archive/master.tar.gz">https://github.com/bbockelm/htcondor_jobview/archive/master.tar.gz</a><br/>I suppose you have to know the branch name. =/<br/>
<span style="color: #c386df"><span style="font-size: small">(09:36:24)</span> <b>matyas:</b></span> no, not master, the latest _released_ version, i.e. the top one here: <a href="https://github.com/opensciencegrid/StashCache/releases">https://github.com/opensciencegrid/StashCache/releases</a><br/>
<span style="color: #e06b56"><span style="font-size: small">(09:36:46)</span> <b>jthiltges:</b></span> Ah, sorry.<br/>
<span style="color: #c386df"><span style="font-size: small">(09:39:00)</span> <b>matyas:</b></span> I'd like to make a recipe for getting a copy of stashcp in case neither it nor modules are available on the system<br/>
<span style="color: #e06b56"><span style="font-size: small">(09:39:57)</span> <b>jthiltges:</b></span> Hrm. I suppose the info is in JSON here: <a href="https://api.github.com/repos/opensciencegrid/StashCache/releases/latest">https://api.github.com/repos/opensciencegrid/StashCache/releases/latest</a><br/>
<span style="color: #235e5b"><span style="font-size: small">(10:01:15)</span> <b>dweitzel:</b></span> We hit this with Purdue, but just something to keep in mind.  If a WLCG reporting site (T1, T2...) change their Resource Group name in topology, then we need to coordinate with the WLCG on the new name.  There's a mapping within WLCG from WLCG name (Federation name) and "Site" (Resource Group in our topology).  Learn something every day.<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:36:55)</span> <b>andrew.melo:</b></span> Hi all (particularly @lincoln and @tslauson as recommended by Brian L) -- moving a thread from #uscms to here since it's not particularly CMS-specific <a href="https://opensciencegrid.slack.com/archives/CJ161RXEE/p1578672564082900">https://opensciencegrid.slack.com/archives/CJ161RXEE/p1578672564082900</a><br/>
<span style="color: #e96699"><span style="font-size: small">(10:40:08)</span> <b>lincoln:</b></span> @andrew.melo well as you probably know, IBM owns Ceph as well now :slightly_smiling_face: In fact, they own GPFS, Gluster, Ceph...<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:42:22)</span> <b>andrew.melo:</b></span> Hah, true. All the good guys get bought eventually (RIP redhat)<br/>
<span style="color: #c386df"><span style="font-size: small">(10:42:39)</span> <b>matyas:</b></span> IBM: Infernal Bloody Monopoly :slightly_smiling_face:<br/>
<span style="color: #a63024"><span style="font-size: small">(10:42:39)</span> <b>efajardo:</b></span> @justas.balcas also setup its own CEPH at Caltech for users and performs great<br/>
<span style="color: #e96699"><span style="font-size: small">(10:55:56)</span> <b>lincoln:</b></span> so anyhow, here are some random thoughts @andrew.melo<br/>The small file performance is okay but no distributed filesystem is going to do well there, IMO. I think the fixed overhead for file operations is probably just an order of magnitude or more higher for a distributed FS vs non-distributed FS. The best trick I ever learned was to disable colorful ls as a default alias on our machines where Ceph is mounted and we have a bunch of interactive users. just a simple readdir() vs .. stat'ing every file to see what they are and make things colorful.<br/><br/>we are on the latest version of Ceph at UChicago but we have a rather old cluster (since 2013 i think?) with a lot of bells and whistles turned on. We use a combination of EC + cache tiering with replication. There is a ton of write amplification because of that, and it makes things a lot slower than they should be. If i were to build the cluster again, I would benchmark pure EC with no cache tiering. At the time when we set up our cluster, that wasn't an option with CephFS, I believe it is now. You will get the best performance from 3x replication, though. Obviously at a large cost for capacity.<br/><br/>we're at a scale of about 3 PB raw / 500 OSDs. We use CephFS almost exclusively. We have 3 MDS servers, each with 48Gb RAM (recycled workers). More ram = more inodes cached = happier users. You will also probably want to have a separate pool of SSDs just for metadata operations.<br/><br/>We only have Ceph mounted on the interactive nodes, users need to use stashcp/http/condor file transfer for moving data around.<br/><br/>The RH buyout of Ceph was a good thing because started supporting the kernel driver in an official capacity, backporting it to the crazy kernel RH maintains for EL. however I find that it's easier to do the FUSE driver a lot of places. it's lower performance but you can just kill the mount and remount the filesystem when things go toes up, vs rebooting the whole server with a kind-of-dangerous <tt>echo b &gt; /proc/sysrq-trigger</tt> because the kernel driver is hung and the filesystem will otherwise hang during shutdown.<br/><br/>rolling upgrades between minor updates are almost totally painless. rolling upgrades between major versions need care, but we have never had a disaster. We've updated from the H release (Hammer) all the way up to Nautilus (6ish major versions?? trying to remember the alphabet and count on my fingers at the same time..).<br/><br/>the self-healing properties are the #1 reason to use Ceph IMHO. we have lost &lt;10 objects in our.. 800 million object pool over several years.<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:57:36)</span> <b>andrew.melo:</b></span> Wow, that's very excellent. So I guess the "hardware recommendations" in the docs are for a much smaller scale. Are your monitor servers similarly provisioned?<br/>
<span style="color: #e96699"><span style="font-size: small">(10:59:49)</span> <b>lincoln:</b></span> monitors are just simple VMs<br/>
<span style="color: #e96699"><span style="font-size: small">(11:00:15)</span> <b>lincoln:</b></span> @andrew.melo <a href="http://ganglia.mwt2.org/?c=Ceph&amp;m=load_one&amp;r=hour&amp;s=by%20name&amp;hc=4&amp;mc=2">http://ganglia.mwt2.org/?c=Ceph&amp;m=load_one&amp;r=hour&amp;s=by%20name&amp;hc=4&amp;mc=2</a> you can poke around here if you like<br/>
<span style="color: #e96699"><span style="font-size: small">(11:00:44)</span> <b>lincoln:</b></span> about half of our OSDs are bluestore.<br/>
<span style="color: #e96699"><span style="font-size: small">(11:02:18)</span> <b>lincoln:</b></span> i can talk/complain about Ceph all day :smile:<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:02:55)</span> <b>andrew.melo:</b></span> I appreciate it :slightly_smiling_face:<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:07:12)</span> <b>andrew.melo:</b></span> I've seen a bunch of recommendations around about plumbing a separate/private network for the OSDs to communicate/do their replication. HAve you found the hassle to be useful?<br/>
<span style="color: #e96699"><span style="font-size: small">(11:07:30)</span> <b>lincoln:</b></span> I've never bothered, but I've also never measured to see if it has an effect<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:08:24)</span> <b>andrew.melo:</b></span> The justification seems to be "keep client accesses from interfering with replication", but I'd imagine it's easier to just upgrade optics than to deal with the config<br/>
<span style="color: #e96699"><span style="font-size: small">(11:08:48)</span> <b>lincoln:</b></span> yeah thats kind of my belief too. if the network is saturated then maybe its time to get bigger pipes<br/>
<span style="color: #e96699"><span style="font-size: small">(11:09:31)</span> <b>lincoln:</b></span> I think there have been some improvements at the OSD level for controlling replication/backfill vs client priority<br/>
<span style="color: #e96699"><span style="font-size: small">(11:10:02)</span> <b>lincoln:</b></span> I imagine in the early days there was no kind of QOS but I am not sure<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:10:12)</span> <b>andrew.melo:</b></span> (As an aside, it's crazy how cheap 10g gear has gotten, I remember spending &gt;1k/optic not too terribly long ago)<br/>
<span style="color: #e96699"><span style="font-size: small">(11:10:30)</span> <b>lincoln:</b></span> yeah if you buy the off-brand optics they're pretty affordable. :thinking_face: i should do 10G at home..<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:13:18)</span> <b>andrew.melo:</b></span> As far as I know, on-brand optics don't exist?<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:13:23)</span> <b>andrew.melo:</b></span> I've certainly never seen any :slightly_smiling_face:<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:14:05)</span> <b>andrew.melo:</b></span> @lincoln we have a bunch of old copper 10g hardware collecting dust...<br/>
<span style="color: #e96699"><span style="font-size: small">(11:14:20)</span> <b>lincoln:</b></span> CX-4? :slightly_smiling_face:<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:19:33)</span> <b>andrew.melo:</b></span> Yeah, our very very first 10g stuff was copper. I assume because of the price (?)<br/>
<span style="color: #de5f24"><span style="font-size: small">(16:18:15)</span> <b>justas.balcas:</b></span> I am on phone, so not much details. We run just 3 storage nodes, 36 osds 8tb, plus 6x6tb nvme pci. Cephfs mounted on all tier2 r/w and the preformance compared to hdfs is like day and night. One mds with a lot memory and 6 monitors on pretty old hardware.  We have in use now 70tb and more than 60mln files. We moved away from many nfses to one ceph. Users keep same copy of miniaod on ceph and run via local queues reading/writing to ceph. I have not tried yet to mount it to gridftp/xrootd, but so9n. We dont use erqsure coding so far and in plans to make analyzis what fits us best. We might consider to move all tier2 from hdfs if we get some planned donated hardware. In any case, users are more happier than under nfs. Of problems, so far encountered only mtu issue and need to scale monitors. Can give more details after my flight and some benchmark numbers<br/>
<span style="color: #a72f79"><span style="font-size: small">(18:31:27)</span> <b>andrew.melo:</b></span> Thanks! Good flight!<br/>
</body>
</html>

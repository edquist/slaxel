<!DOCTYPE html>
<html>
<head>
<title>Wed Feb 12, 2020 : #software (osg)</title>
</head>
<body>
<h3>Wed Feb 12, 2020 : #software (osg)</h3>
<span style="color: #235e5b"><span style="font-size: small">(09:02:12)</span> <b>spiperov:</b></span> OK, here's what I need help with (and Brian Lin suggested that @matyas  maybe the person who has the answers):<br/>Looking at <tt>/etc/condor/config.d/00-osg_default_security.config</tt> I see a statement:<br/><pre>The pool password file located at /etc/condor/passwords.d/POOL must be copied to all hosts that are part of the pool.</pre><br/>but our /etc/condor/passwords.d/ directory is empty<br/>Further down it reads:<br/><pre># A pool password has already been created as part of the installation.<br/># In case you need to create a new pool password file, run<br/># /usr/libexec/condor/create_pool_password</pre><br/>and indeed a file /var/condor/log/pool_password exists already, so if I try to create a new password, as instructed above, I get the message that the file already exists.<br/><br/>Where do I start with this process?<br/>We did not have any security set up so far.<br/><br/>The upgrade document makes a very brief mention of those settings:<br/><a href="https://opensciencegrid.org/docs/release/release_series/#updating-to-htcondor-88x_1">https://opensciencegrid.org/docs/release/release_series/#updating-to-htcondor-88x_1</a><br/>and then points to the full manual, which is just too general:<br/><a href="https://htcondor.readthedocs.io/en/stable/admin-manual/security.html">https://htcondor.readthedocs.io/en/stable/admin-manual/security.html</a><br/><br/>Do you guys have an example for a typical USCMS site?<br/>
<span style="color: #c386df"><span style="font-size: small">(09:08:49)</span> <b>matyas:</b></span> why /var/condor/log/pool_password?<br/>
<span style="color: #c386df"><span style="font-size: small">(09:10:19)</span> <b>matyas:</b></span> the default pool password location is /etc/condor/passwords.d/POOL but if you change <tt>SEC_PASSWORD_FILE</tt> in the config, then that overrides the default<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:19:32)</span> <b>spiperov:</b></span> No, our <tt>SEC_PASSWORD_FILE</tt>  is still set to <tt>/etc/condor/passwords.d/POOL</tt>  - no changes from the default there.<br/>Yet the execution of <tt>create_pool_password</tt> tells me:<br/><pre>$ /usr/libexec/condor/create_pool_password<br/>create_pool_password: '/var/condor/log/pool_password' already exists!  Delete the file before creating a new one.</pre><br/><br/>
<span style="color: #43761b"><span style="font-size: small">(09:20:12)</span> <b>blin:</b></span> this is for OSG 3.5, right?<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:20:24)</span> <b>spiperov:</b></span> So, should I just use one of these <tt>/var/condor/log/pool_password</tt> files (and I see different ones on different nodes) as the <tt>/etc/condor/passwords.d/POOL</tt> ?<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:20:43)</span> <b>spiperov:</b></span> yes<br/>
<span style="color: #43761b"><span style="font-size: small">(09:20:48)</span> <b>blin:</b></span> could you run this command? <tt>condor_config_val SEC_PASSWORD_FILE 2&gt;/dev/null</tt><br/>
<span style="color: #235e5b"><span style="font-size: small">(09:21:05)</span> <b>spiperov:</b></span> on any node in particular?<br/>
<span style="color: #c386df"><span style="font-size: small">(09:21:08)</span> <b>matyas:</b></span> and <tt>bash -x /usr/libexec/condor/create_pool_password</tt><br/>
<span style="color: #43761b"><span style="font-size: small">(09:21:12)</span> <b>blin:</b></span> on the node where you're having issues :slightly_smiling_face:<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:21:37)</span> <b>spiperov:</b></span> <pre>$ condor_config_val SEC_PASSWORD_FILE 2&gt;/dev/null<br/>/var/condor/log/pool_password</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(09:21:52)</span> <b>blin:</b></span> whelp, there's your problem!<br/>
<span style="color: #43761b"><span style="font-size: small">(09:22:05)</span> <b>blin:</b></span> to find out where that's set in your config: <tt>condor_config_val -v SEC_PASSWORD_FILE</tt><br/>
<span style="color: #43761b"><span style="font-size: small">(09:22:41)</span> <b>blin:</b></span> i guess if that's where you want to keep your pool password, that's ok i guess. seems like kind of a weird place for it, though<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:22:43)</span> <b>spiperov:</b></span> <pre>SEC_PASSWORD_FILE = /var/condor/log/pool_password<br/> # at: /etc/condor/condor_config.local, line 576<br/> # raw: SEC_PASSWORD_FILE = $(LOCK)/pool_password</pre><br/>
<span style="color: #c386df"><span style="font-size: small">(09:23:46)</span> <b>matyas:</b></span> why is LOCK /var/condor/log?<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:24:16)</span> <b>spiperov:</b></span> No idea. Must be a left-over from olden days. These config files are pretty old<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:24:50)</span> <b>spiperov:</b></span> I probably need to clean up many old settings.<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:26:48)</span> <b>spiperov:</b></span> But back to my immediate question:<br/>Should I copy one (and the same) file in that <tt>SEC_PASSWORD_FILE</tt>location - even if it stays in the current place (<tt>/var/condor/log/pool_password</tt>) - to all nodes in the cluster?<br/>
<span style="color: #43761b"><span style="font-size: small">(09:27:55)</span> <b>blin:</b></span> yup, that's the shared password that hosts in your pool will be using for security<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:28:29)</span> <b>spiperov:</b></span> Right now I see each node having it's own version of that file (the schedd, collector, computes)<br/>
<span style="color: #43761b"><span style="font-size: small">(09:28:58)</span> <b>blin:</b></span> yeah, that makes sense if you didn't have any password before<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:29:08)</span> <b>spiperov:</b></span> correct<br/>
<span style="color: #43761b"><span style="font-size: small">(09:29:08)</span> <b>blin:</b></span> just choose one of them<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:29:15)</span> <b>spiperov:</b></span> OK, let me try.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:32:43)</span> <b>spiperov:</b></span> OK, I've put the same SEC_PASSWORD_FILE everywhere, but I keep seeing e.g. this:<br/><pre>02/12/20 11:29:11 enter Daemons::UpdateCollector<br/>02/12/20 11:29:11 Trying to update collector &lt;128.211.143.10:9618&gt;<br/>02/12/20 11:29:11 Attempting to send update via TCP to collector <a href="http://cms-condor.rcac.purdue.edu">cms-condor.rcac.purdue.edu</a> &lt;128.211.143.10:9618&gt;<br/>02/12/20 11:29:11 Daemon::startCommand(UPDATE_MASTER_AD,...) making connection to &lt;128.211.143.10:9618&gt;<br/>02/12/20 11:29:11 exit Daemons::UpdateCollector<br/>02/12/20 11:29:11 enter Daemons::CheckForNewExecutable<br/>02/12/20 11:29:11 Time stamp of running /usr/sbin/condor_master: 1578334694<br/>02/12/20 11:29:11 GetTimeStamp returned: 1578334694<br/>02/12/20 11:29:11 Calling Handler &lt;SecManStartCommand::WaitForSocketCallback UPDATE_MASTER_AD&gt; (4)<br/>02/12/20 11:29:11 Return from Handler &lt;SecManStartCommand::WaitForSocketCallback UPDATE_MASTER_AD&gt; 0.000318s<br/>02/12/20 11:29:11 Calling Handler &lt;SecManStartCommand::WaitForSocketCallback UPDATE_MASTER_AD&gt; (4)<br/>02/12/20 11:29:11 SECMAN: FAILED: Received "DENIED" from server for user unauthenticated@unmapped using no authentication method, which may imply host-based security.  Our address was '128.211.143.10', and server's address was '128.211.143.10'.  Check your ALLOW settings and IP protocols.<br/>02/12/20 11:29:11 ERROR: SECMAN:2010:Received "DENIED" from server for user unauthenticated@unmapped using no authentication method, which may imply host-based security.  Our address was '128.211.143.10', and server's address was '128.211.143.10'.  Check your ALLOW settings and IP protocols.<br/>02/12/20 11:29:11 Failed to start non-blocking update to &lt;128.211.143.10:9618&gt;.<br/>02/12/20 11:29:11 Return from Handler &lt;SecManStartCommand::WaitForSocketCallback UPDATE_MASTER_AD&gt; 0.000449s</pre><br/>in the central manager<br/>
<span style="color: #43761b"><span style="font-size: small">(10:35:16)</span> <b>blin:</b></span> do both hosts already have <tt>/etc/condor/config.d/00-osg_default_security.config</tt> ?<br/>
<span style="color: #43761b"><span style="font-size: small">(10:35:45)</span> <b>blin:</b></span> (if this doesn't have an obvious solution, we should turn this into a GGUS ticket)<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:04:16)</span> <b>spiperov:</b></span> Yes, they do - on all nodes I have what I believe is the default version of this file, with only the location of the <tt>SEC_PASSWORD_FILE</tt>  changed to <tt>/var/condor/log/pool_password</tt><br/><pre># This config file sets up basic authentication for HTCondor, consisting of<br/># a "pool password" for daemon-to-daemon configuration, and "filesystem auth"<br/># for user and admin commands.  The pool password file located at<br/># /etc/condor/passwords.d/POOL must be copied to all hosts that are part of<br/># the pool.  As your cluster grows, consider switching to a more advanced<br/># form of authentication, such as GSI or SSL.<br/><br/># See the HTCondor Manual on security at<br/># <a href="https://htcondor.readthedocs.io/en/v8_8_4/admin-manual/security.html">https://htcondor.readthedocs.io/en/v8_8_4/admin-manual/security.html</a><br/># for more information.<br/><br/># A pool password has already been created as part of the installation.<br/># In case you need to create a new pool password file, run<br/># /usr/libexec/condor/create_pool_password<br/><br/><br/># require authentication and integrity for everything...<br/>SEC_DEFAULT_AUTHENTICATION=REQUIRED<br/>SEC_DEFAULT_INTEGRITY=REQUIRED<br/><br/># ...except read access...<br/>SEC_READ_AUTHENTICATION=OPTIONAL<br/>SEC_READ_INTEGRITY=OPTIONAL<br/><br/># ...and the outgoing (client side) connection since the server side will enforce its policy<br/>SEC_CLIENT_AUTHENTICATION=OPTIONAL<br/>SEC_CLIENT_INTEGRITY=OPTIONAL<br/><br/># this will required PASSWORD authentications for daemon-to-daemon, and<br/># allow FS authentication for submitting jobs and running administrator commands<br/>SEC_DEFAULT_AUTHENTICATION_METHODS = FS, PASSWORD<br/>SEC_DAEMON_AUTHENTICATION_METHODS = PASSWORD<br/>SEC_NEGOTIATOR_AUTHENTICATION_METHODS = PASSWORD<br/>#SEC_PASSWORD_FILE = /etc/condor/passwords.d/POOL<br/>SEC_PASSWORD_FILE = /var/condor/log/pool_password<br/><br/># admin commands (e.g. condor_off) can be run by:<br/>#  1. root on the local host or the central manager<br/>#  2. condor user on the local host or the central manager<br/>ALLOW_ADMINISTRATOR = condor@*/$(FULL_HOSTNAME) condor@*/$(CONDOR_HOST) condor_pool@*/$(FULL_HOSTNAME) condor_pool@*/$(CONDOR_HOST)  root@$(UID_DOMAIN)/$(FULL_HOSTNAME)<br/># only the condor daemons on the central manager can negotiate<br/>ALLOW_NEGOTIATOR = condor@*/$(CONDOR_HOST) condor_pool@*/$(CONDOR_HOST)<br/># any authenticated daemons in the pool can read/write/advertise<br/>ALLOW_DAEMON = condor@* condor_pool@*</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(11:07:06)</span> <b>blin:</b></span> my guess is that you have some old config interfering<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:07:13)</span> <b>spiperov:</b></span> How much should i be worried about those <tt>unauthenticated@unmapped</tt> users that I see in the logs?<br/>E.g. in  <tt>/var/condor/log/CollectorLog</tt>  I see the following every time I restart the condor services on the submit host 128.211.143.130 :<br/><pre>02/12/20 12:05:15 PERMISSION DENIED to unauthenticated@unmapped from host 128.211.143.130 for command 15 (INVALIDATE_MASTER_ADS), access level ADVERTISE_MASTER: reason: cached result for ADVERTISE_MASTER; see first case for the full reason<br/>02/12/20 12:05:15 DC_AUTHENTICATE: Command not authorized, done!<br/>02/12/20 12:05:15 PERMISSION DENIED to unauthenticated@unmapped from host 128.211.143.130 for command 14 (INVALIDATE_SCHEDD_ADS), access level ADVERTISE_SCHEDD: reason: cached result for ADVERTISE_SCHEDD; see first case for the full reason<br/>02/12/20 12:05:15 DC_AUTHENTICATE: Command not authorized, done!<br/>02/12/20 12:05:15 PERMISSION DENIED to unauthenticated@unmapped from host 128.211.143.130 for command 18 (INVALIDATE_SUBMITTOR_ADS), access level ADVERTISE_SCHEDD: reason: cached result for ADVERTISE_SCHEDD; see first case for the full reason<br/>02/12/20 12:05:15 DC_AUTHENTICATE: Command not authorized, done!</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(11:08:11)</span> <b>blin:</b></span> they're pretty bad in this case because the commands that are failing <tt>ADVERTISE_MASTER</tt>, <tt>ADVERTISE_SCHEDD</tt>, etc are the condor daemons failing to authenticate with each other<br/>
<span style="color: #43761b"><span style="font-size: small">(11:08:37)</span> <b>blin:</b></span> could you open a ggus ticket with <tt>condor_config_val -summary</tt> output for that host?<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:09:41)</span> <b>spiperov:</b></span> Sure!<br/>You mean a generic GGUS ticket, correct?  Not a CMS one.<br/>
<span style="color: #43761b"><span style="font-size: small">(11:10:36)</span> <b>blin:</b></span> correct<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:13:48)</span> <b>spiperov:</b></span> so "Ticket Category" would be  "Service Request"<br/>What about "Type of Issue"? - Maybe "VO Specific Software"?<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:15:58)</span> <b>spiperov:</b></span> (sorry, I am not very much used to submitting generic GGUS tickets - we normally do CMS ones)<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:22:32)</span> <b>spiperov:</b></span> Should I assign it to specific support unit?<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:27:08)</span> <b>spiperov:</b></span> Or should I use "Workload Management" instead of "VO Specific Software" ?<br/>Which way will the ticket reach you fastest?<br/>
<span style="color: #43761b"><span style="font-size: small">(11:27:20)</span> <b>blin:</b></span> there should be an OSG support unit in there somewhere<br/>
<span style="color: #43761b"><span style="font-size: small">(11:27:28)</span> <b>blin:</b></span> workload management seems appropriate to me<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:29:05)</span> <b>spiperov:</b></span> There's a bunch of EGI units, but nothing OSG-specific<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:29:12)</span> <b>spiperov:</b></span> I'll leave that open<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:29:44)</span> <b>spiperov:</b></span> <a href="https://ggus.eu/index.php?mode=ticket_info&amp;ticket_id=145525">https://ggus.eu/index.php?mode=ticket_info&amp;ticket_id=145525</a><br/>
<span style="color: #43761b"><span style="font-size: small">(11:30:22)</span> <b>blin:</b></span> weird, i see an OSG Software Support support unit (<a href="https://ggus.eu/index.php?mode=ticket_info&amp;ticket_id=145486">https://ggus.eu/index.php?mode=ticket_info&amp;ticket_id=145486</a>)<br/>
<span style="color: #43761b"><span style="font-size: small">(11:30:48)</span> <b>blin:</b></span> but maybe that's only visible to GGUS folks. perhaps in the ticket description you can ask for them to assign it to our support unit<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:32:51)</span> <b>spiperov:</b></span> Brian, you didn't mean OSG ticket (instead of GGUS), did you?<br/><a href="https://support.opensciencegrid.org/support/tickets/new">https://support.opensciencegrid.org/support/tickets/new</a><br/>
<span style="color: #43761b"><span style="font-size: small">(11:33:41)</span> <b>blin:</b></span> nope! LHC sites should be opening GGUS tickets <a href="https://opensciencegrid.org/docs/common/help/#submitting-support-inquiries">https://opensciencegrid.org/docs/common/help/#submitting-support-inquiries</a><br/>
<span style="color: #235e5b"><span style="font-size: small">(11:33:53)</span> <b>spiperov:</b></span> OK - just checking ! :slightly_smiling_face:<br/>
<span style="color: #385a86"><span style="font-size: small">(14:13:51)</span> <b>goughes:</b></span> Turns out 20-slurm.ini was configured with the slurmdbd port (6819) instead of the mysql port (3306). :face_palm:<br/>
<blockquote>
<span style="color: #385a86"><span style="font-size: small">(2020-02-12 14:13:51)</span> <b>goughes:</b></span> Turns out 20-slurm.ini was configured with the slurmdbd port (6819) instead of the mysql port (3306). :face_palm:<br/>
</blockquote>
<span style="color: #385a86"><span style="font-size: small">(14:18:51)</span> <b>goughes:</b></span> Or maybe I should be using the slurmdbd port and need some extra configuration to allow external access? I'd be interested to know what other Slurm sites do @bockjoo @andrew.melo<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:23:42)</span> <b>bockjoo:</b></span> I use mysql port<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:24:11)</span> <b>bockjoo:</b></span> Our RC set up a readonly mysql db access.<br/>
<span style="color: #385a86"><span style="font-size: small">(14:24:29)</span> <b>goughes:</b></span> Ok, that's what I did<br/>
<span style="color: #385a86"><span style="font-size: small">(14:24:30)</span> <b>goughes:</b></span> Thanks!<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:29:56)</span> <b>bockjoo:</b></span> Did you make sure /etc/osg/config.d/20-slurm.ini and /etc/gratia/slurm/ProbeConfig have the same DB connection info?<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:31:01)</span> <b>bockjoo:</b></span> Did you also check mysql -u read_only__db_user -h mysql_slurmdb_host -P 3306 -p<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:31:03)</span> <b>bockjoo:</b></span> ?<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:31:22)</span> <b>bockjoo:</b></span> and see if your connection info works?<br/>
<span style="color: #385a86"><span style="font-size: small">(14:41:23)</span> <b>goughes:</b></span> Yeah it works since I changed the port in 20-slurm.ini to 3306<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:53:32)</span> <b>bockjoo:</b></span> Do you see any error when you run /usr/share/gratia/slurm/slurm_meter_running -s 1 -c<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:55:12)</span> <b>bockjoo:</b></span> At the end of the day, I usually run this<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:55:13)</span> <b>bockjoo:</b></span> grep "new records sent successfully" /var/log/gratia/$(date +%Y-%m-%d).log | wc -l<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:55:33)</span> <b>bockjoo:</b></span> to see how many records are successfully sent to osg as a cron.<br/>
<span style="color: #385a86"><span style="font-size: small">(15:19:21)</span> <b>goughes:</b></span> I don't see any errors since I changed the port. I saw a large amount of records that were sent once the port was updated.<br/>
<span style="color: #385a86"><span style="font-size: small">(15:19:39)</span> <b>goughes:</b></span> Things are good now<br/>
</body>
</html>

<!DOCTYPE html>
<html>
<head>
<title>Wed Oct 2, 2019 : #software (osg)</title>
</head>
<body>
<h3>Wed Oct 2, 2019 : #software (osg)</h3>
<span style="color: #dd8527"><span style="font-size: small">(08:15:14)</span> <b>marcom:</b></span> Thanks everyone. I will go the setup.py + rpm macros route then.<br/>
<span style="color: #c386df"><span style="font-size: small">(11:52:47)</span> <b>matyas:</b></span> @rynge <a href="https://github.com/opensciencegrid/osgvo-el8">https://github.com/opensciencegrid/osgvo-el8</a> you're quick aren't you :stuck_out_tongue:<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:53:52)</span> <b>rynge:</b></span> I have been waiting!<br/>
<span style="color: #c386df"><span style="font-size: small">(11:54:04)</span> <b>matyas:</b></span> I briefly tried adding EL8 support to our Koji build system but haven't succeeded so I need to debug<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:54:40)</span> <b>rynge:</b></span> Yeah no worries. Obviously we want the osg stack in there at some point, but for now we just want something so user can start testing<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:55:31)</span> <b>rynge:</b></span> For example, OSG VO still has to decide if we want set up modules on EL8<br/>
<span style="color: #c386df"><span style="font-size: small">(11:55:37)</span> <b>matyas:</b></span> ah, the centos:8 docker image is out now. Nice<br/>
<span style="color: #c386df"><span style="font-size: small">(11:56:05)</span> <b>matyas:</b></span> By modules do you mean modules out of CVMFS or do you mean Modularity?<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:56:18)</span> <b>rynge:</b></span> I mean modules out of CVMFS<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:56:33)</span> <b>rynge:</b></span> We have a large set of common packages in there we support<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:57:16)</span> <b>rynge:</b></span> <a href="https://support.opensciencegrid.org/support/solutions/articles/5000634397-software-modules-catalog">https://support.opensciencegrid.org/support/solutions/articles/5000634397-software-modules-catalog</a><br/>
<span style="color: #674b1b"><span style="font-size: small">(11:57:37)</span> <b>rynge:</b></span> On EL7, we used Spack to build it<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:57:58)</span> <b>rynge:</b></span> In theory, we should be able to rebuild on EL8, but I'm sure we will run into some issues<br/>
<span style="color: #43761b"><span style="font-size: small">(12:00:15)</span> <b>blin:</b></span> @dweitzel our ITB CE looks like it's having trouble communicating with the ITB central collector<br/><pre><br/>10/01/19 18:09:20 (D_ALWAYS) ERROR: SECMAN:2004:Failed to create security session to &lt;129.93.175.57:9619&gt; with TCP.|AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5008:We are trying to connect to a daemon with certificate DN (/DC=org/DC=incommon/C=US/postalCode=68588/ST=NE/L=Lincoln/street=14th And R St./O=University of Nebraska-Lincoln/OU=Holland Computing Center/CN=<a href="http://collector2.opensciencegrid.org">collector2.opensciencegrid.org</a>), but the host name in the certificate does not match any DNS name associated with the host to which we are connecting (host name is '<a href="http://collector-itb.opensciencegrid.org">collector-itb.opensciencegrid.org</a>', IP is '129.93.175.57', Condor connection address is '&lt;129.93.175.57:9619?alias=<a href="http://collector-itb.opensciencegrid.org">collector-itb.opensciencegrid.org</a>&gt;').  Check that DNS is correctly configured.  If the certificate is for a DNS alias, configure HOST_ALIAS in the daemon's configuration.  If you wish to use a daemon certificate that does not match the daemon's host name, make GSI_SKIP_HOST_CHECK_CERT_REGEX match the DN, or disable all host name checks by setting GSI_SKIP_HOST_CHECK=true or by defining GSI_DAEMON_NAME.<br/></pre><br/>
<span style="color: #235e5b"><span style="font-size: small">(12:01:46)</span> <b>dweitzel:</b></span> we have an ITB central collector?<br/>
<span style="color: #43761b"><span style="font-size: small">(12:02:42)</span> <b>blin:</b></span> looks like it?<br/><pre><br/>$ nslookup <a href="http://collector-itb.opensciencegrid.org">collector-itb.opensciencegrid.org</a><br/>Server:		128.104.254.254<br/>Address:	128.104.254.254#53<br/><br/>Non-authoritative answer:<br/><a href="http://collector-itb.opensciencegrid.org">collector-itb.opensciencegrid.org</a>	canonical name = <a href="http://hcc-anvil-175-57.unl.edu">hcc-anvil-175-57.unl.edu</a>.<br/><a href="http://hcc-anvil-175-57.unl.edu">hcc-anvil-175-57.unl.edu</a>	canonical name = <a href="http://hcc-osg-collector2.unl.edu">hcc-osg-collector2.unl.edu</a>.<br/>Name:	<a href="http://hcc-osg-collector2.unl.edu">hcc-osg-collector2.unl.edu</a><br/>Address: 129.93.175.57<br/></pre><br/>
<span style="color: #43761b"><span style="font-size: small">(12:02:48)</span> <b>blin:</b></span> at least DNS says so<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:06:46)</span> <b>dweitzel:</b></span> ok, let me take it out of DNS.  Looks like we just have the 2.  collector1 and collector2, which are both production.<br/>
<span style="color: #43761b"><span style="font-size: small">(12:07:59)</span> <b>blin:</b></span> hrm, well we do want an itb collector<br/>
<span style="color: #43761b"><span style="font-size: small">(12:08:19)</span> <b>blin:</b></span> ...maybe<br/>
<span style="color: #43761b"><span style="font-size: small">(12:08:55)</span> <b>blin:</b></span> osg-configure will point a CE at  collector or collector-itb depending on whether you configure your CE as an ITB site or not<br/>
<span style="color: #43761b"><span style="font-size: small">(12:09:19)</span> <b>blin:</b></span> personally i don't particularly care if we mix the two types of CEs in a single central collector<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:09:55)</span> <b>dweitzel:</b></span> I also don't care.<br/>
<span style="color: #c386df"><span style="font-size: small">(12:10:03)</span> <b>matyas:</b></span> do we really want <a href="http://fermicloud319.fnal.gov">fermicloud319.fnal.gov</a> in the production collector?<br/>
<span style="color: #43761b"><span style="font-size: small">(12:10:34)</span> <b>blin:</b></span> i mean, it already is because we don't have an ITB collector :stuck_out_tongue:<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:11:40)</span> <b>dweitzel:</b></span> it doesn't pollute things much, since the CE advertisements are not permanent.  They last ~15 minutes?<br/>
<span style="color: #c386df"><span style="font-size: small">(12:12:04)</span> <b>matyas:</b></span> OK<br/>
<span style="color: #43761b"><span style="font-size: small">(12:12:35)</span> <b>blin:</b></span> i think the central collector doesn't remove old ads for 7 days<br/>
<span style="color: #43761b"><span style="font-size: small">(12:13:03)</span> <b>blin:</b></span> and it also depends how long-lived the ITB CEs are themselves. if someone's running an ITB site, the CE ad will continue to be there<br/>
<span style="color: #43761b"><span style="font-size: small">(12:14:20)</span> <b>blin:</b></span> yeah, 7 days<br/><pre><br/># If a CE drops offline, we don't want to remove its ad from the collector.<br/>ABSENT_REQUIREMENTS = (MyType=?="Scheduler")<br/><br/># Even missing CEs expire after 7 days<br/>ABSENT_EXPIRE_ADS_AFTER = 7*86400<br/></pre><br/>
<span style="color: #c386df"><span style="font-size: small">(12:14:58)</span> <b>matyas:</b></span> Do we stick anything in the ad for ITB vs production?<br/>
<span style="color: #43761b"><span style="font-size: small">(12:16:07)</span> <b>blin:</b></span> not really<br/><pre><br/>OSG_Resource = "VDT-ITB-MADISON"<br/>OSG_ResourceGroup = "VDT-ITB"<br/>OSG_BatchSystems = "Condor"<br/>OSG_ResourceCatalog = { \<br/>  [ \<br/>    AllowedVOs = { "osg", "cms" }; \<br/>    CPUs = 32; \<br/>    MaxWallTime = 1440; \<br/>    Memory = 64375; \<br/>    Name = "Madison ITB"; \<br/>    Requirements = TARGET.RequestCPUs &lt;= CPUs &amp;&amp; TARGET.RequestMemory &lt;= Memory &amp;&amp; member(TARGET.VO, AllowedVOs); \<br/>    Transform = [ set_MaxMemory = RequestMemory; set_xcount = RequestCPUs; ]; \<br/>  ] \<br/>}<br/></pre><br/>
<span style="color: #235e5b"><span style="font-size: small">(12:25:31)</span> <b>dweitzel:</b></span> BatchProject translates to --account in SLURM.  That is used for "charging" in _some_ slurm installations.<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:25:59)</span> <b>dweitzel:</b></span> SLURM can have built in allocation management, or kinda.<br/>
<span style="color: #43761b"><span style="font-size: small">(12:26:45)</span> <b>blin:</b></span> is <tt>--account</tt> the only slurm-native way to do allocation management?<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:29:34)</span> <b>dweitzel:</b></span> the only one I know of.<br/>
</body>
</html>

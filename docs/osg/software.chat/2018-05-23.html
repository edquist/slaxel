<!DOCTYPE html>
<html>
<head>
<title>Wed May 23, 2018 : #software (osg)</title>
</head>
<body>
<h3>Wed May 23, 2018 : #software (osg)</h3>
<span style="color: #8d4b84"><span style="font-size: small">(09:18:51)</span> <b>kherner:</b></span> @blin We used to get the gpu discovery tool here: <a href="http://osg-flock.grid.iu.edu/gwms-extras/condor_gpu_discovery">http://osg-flock.grid.iu.edu/gwms-extras/condor_gpu_discovery</a><br/>
<span style="color: #8d4b84"><span style="font-size: small">(09:19:31)</span> <b>kherner:</b></span> I tried the obvious substitution of <a href="http://grid.iu.edu">grid.iu.edu</a> --&gt; <a href="http://opensciencegrid.org">opensciencegrid.org</a>, but no dice. I assume it lives on, but in some other place.<br/>
<span style="color: #43761b"><span style="font-size: small">(09:19:50)</span> <b>blin:</b></span> that's because it's <a href="http://flock.opensciencegrid.org/gwms-extras/condor_gpu_discovery">http://flock.opensciencegrid.org/gwms-extras/condor_gpu_discovery</a> :slightly_smiling_face:<br/>
<span style="color: #43761b"><span style="font-size: small">(09:20:10)</span> <b>blin:</b></span> @rynge @efajardo we really should be serving that over https...<br/>
<span style="color: #8d4b84"><span style="font-size: small">(09:20:28)</span> <b>kherner:</b></span> The right address does help, doesn't it... thanks!<br/>
<span style="color: #a63024"><span style="font-size: small">(09:35:17)</span> <b>efajardo:</b></span> Hi Ken<br/>
<span style="color: #a63024"><span style="font-size: small">(09:35:27)</span> <b>efajardo:</b></span> you can grab it from the condor tarball<br/>
<span style="color: #a63024"><span style="font-size: small">(09:36:10)</span> <b>efajardo:</b></span> @kherner here is how @rynge finds it <a href="https://github.com/opensciencegrid/osg-flock/blob/master/node-check/osgvo-node-advertise#L223">https://github.com/opensciencegrid/osg-flock/blob/master/node-check/osgvo-node-advertise#L223</a><br/>
<span style="color: #de5f24"><span style="font-size: small">(09:36:53)</span> <b>tiradani:</b></span> which version of HTCondor started to include the tool?<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:42:28)</span> <b>andrew.melo:</b></span> Might not be the right forum, but it has the right people... I just got this across my whole cluster, followed by <a href="http://cms.cern.ch">cms.cern.ch</a> unmounting<br/><pre><br/>May 23 09:41:03 vmp621.vampire cvmfs2[295880]: re-building cache database<br/>May 23 09:41:03 vmp621.vampire cvmfs2[295855]: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) geographic order of servers retrieved from <a href="http://cvmfs-s1fnal.opensciencegrid.org">cvmfs-s1fnal.opensciencegrid.org</a><br/>May 23 09:41:03 vmp621.vampire cvmfs2[295855]: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) CernVM-FS: linking /cvmfs/config-osg.opensciencegrid.org to repository <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a><br/>May 23 09:41:03 vmp621.vampire cvmfs2[295911]: (<a href="http://cms.cern.ch">cms.cern.ch</a>) geographic order of servers retrieved from <a href="http://cvmfs-s1fnal.opensciencegrid.org">cvmfs-s1fnal.opensciencegrid.org</a><br/>May 23 09:41:03 vmp621.vampire cvmfs2[295911]: (<a href="http://cms.cern.ch">cms.cern.ch</a>) CernVM-FS: linking /cvmfs/cms.cern.ch to repository <a href="http://cms.cern.ch">cms.cern.ch</a><br/>May 23 09:41:08 vmp621.vampire cvmfs2[295988]: (cms.cern.sh) geographic order of servers retrieved from <a href="http://cvmfs-s1fnal.opensciencegrid.org">cvmfs-s1fnal.opensciencegrid.org</a><br/>May 23 09:41:08 vmp621.vampire cvmfs2[295988]: (cms.cern.sh) switching host from <a href="http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.cern.sh">http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.cern.sh</a> to <a href="http://cvmfs-s1goc.opensciencegrid.org/cvmfs/cms.cern.sh">http://cvmfs-s1goc.opensciencegrid.org/cvmfs/cms.cern.sh</a><br/>May 23 09:41:08 vmp621.vampire cvmfs2[295988]: (cms.cern.sh) switching host from <a href="http://cvmfs-s1goc.opensciencegrid.org/cvmfs/cms.cern.sh">http://cvmfs-s1goc.opensciencegrid.org/cvmfs/cms.cern.sh</a> to <a href="http://cvmfs-s1bnl.opensciencegrid.org:8000/cvmfs/cms.cern.sh">http://cvmfs-s1bnl.opensciencegrid.org:8000/cvmfs/cms.cern.sh</a><br/>May 23 09:41:08 vmp621.vampire cvmfs2[295988]: (cms.cern.sh) failed to download repository manifest (9 - host returned HTTP error)<br/>May 23 09:41:08 vmp621.vampire cvmfs2[295988]: (cms.cern.sh) switching host from <a href="http://cvmfs-s1bnl.opensciencegrid.org:8000/cvmfs/cms.cern.sh">http://cvmfs-s1bnl.opensciencegrid.org:8000/cvmfs/cms.cern.sh</a> to <a href="http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.cern.sh">http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.cern.sh</a><br/>May 23 09:41:08 vmp621.vampire cvmfs2[295988]: (cms.cern.sh) switching host from <a href="http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.cern.sh">http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.cern.sh</a> to <a href="http://cvmfs-s1goc.opensciencegrid.org/cvmfs/cms.cern.sh">http://cvmfs-s1goc.opensciencegrid.org/cvmfs/cms.cern.sh</a><br/>May 23 09:41:08 vmp621.vampire cvmfs2[295988]: (cms.cern.sh) failed to fetch file catalog at cms.cern.sh:/ (hash: 0000000000000000000000000000000000000000, error 9 [host returned HTTP error])<br/>May 23 09:41:08 vmp621.vampire cvmfs2[295988]: (cms.cern.sh) Failed to initialize root file catalog (16 - file catalog failure)<br/></pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(09:43:21)</span> <b>andrew.melo:</b></span> Waiting a second and trying to do another <tt>ls</tt> gives the same error<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:44:09)</span> <b>dweitzel:</b></span> @dwd ^<br/>
<span style="color: #a63024"><span style="font-size: small">(09:46:00)</span> <b>efajardo:</b></span> @tiradani AFAIK at least since 8.4<br/>
<span style="color: #a63024"><span style="font-size: small">(09:46:29)</span> <b>efajardo:</b></span> but looking at the manual it looks like from 7 series however it could be the tarball that gwms creates was chopping it offf<br/>
<span style="color: #de5f24"><span style="font-size: small">(09:46:53)</span> <b>tiradani:</b></span> ah!  OK, thanks<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:48:49)</span> <b>bbockelm:</b></span> It's unrelated, but BNL worker nodes have really misconfigured things and are sending all the <a href="http://stash.osgstorage.org">stash.osgstorage.org</a> requests to the regular set of CVMFS Stratum-1s<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:49:27)</span> <b>bbockelm:</b></span> That's a suspicious one...<br/><pre><br/>May 23 09:41:08 vmp621.vampire cvmfs2[295988]: (cms.cern.sh) failed to fetch file catalog at cms.cern.sh:/ (hash: 0000000000000000000000000000000000000000, error 9 [host returned HTTP error])<br/></pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(09:50:01)</span> <b>bbockelm:</b></span> Is it possible your local squid is corrupt?<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:50:27)</span> <b>andrew.melo:</b></span> Wait. my diagnosis is idiotic<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:50:30)</span> <b>andrew.melo:</b></span> cms.cern.sh isn't a thing<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:50:37)</span> <b>bbockelm:</b></span> ha, I missed that<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:50:42)</span> <b>andrew.melo:</b></span> the mounts ARE down and not coming up though<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:50:46)</span> <b>dweitzel:</b></span> Is BNL not using oasis and the config repo?<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:50:53)</span> <b>andrew.melo:</b></span> <pre><br/>[root@amn0001 ~]# ls /cvmfs/config-osg.opensciencegrid.org/ /cvmfs/cms.cern.ch<br/>ls: cannot access /cvmfs/cms.cern.ch: No such file or directory<br/>/cvmfs/config-osg.opensciencegrid.org/:<br/>etc  libexec<br/></pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(09:51:02)</span> <b>bbockelm:</b></span> That's what I would suspect.<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:52:27)</span> <b>andrew.melo:</b></span> Nothing interesting from journalctl<br/><pre><br/>[root@amn0001 ~]# ls /cvmfs/cms.cern.ch /cvmfs/config-osg.opensciencegrid.org/<br/>ls: cannot access /cvmfs/cms.cern.ch: No such file or directory<br/>/cvmfs/config-osg.opensciencegrid.org/:<br/>etc  libexec<br/></pre><br/>
<span style="color: #73769d"><span style="font-size: small">(09:54:55)</span> <b>tim.theisen:</b></span> @tiradani @efajardo condor_gpu_discovery has been available since HTCondor version 8.1.4 released on February 27, 2014<br/>
<span style="color: #99a949"><span style="font-size: small">(10:06:57)</span> <b>dwd:</b></span> @andrew.melo what does /var/log/messages say on amn0001?<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:10:35)</span> <b>andrew.melo:</b></span> A message about loading the catalog for config-osg and nothing else<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:10:46)</span> <b>andrew.melo:</b></span> Walking between offices, I can copy paste the exact message<br/>
<span style="color: #99a949"><span style="font-size: small">(10:10:47)</span> <b>dwd:</b></span> Then restart autofs<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:10:59)</span> <b>bbockelm:</b></span> yeah, sounds a bit like an autofs issue<br/>
<span style="color: #99a949"><span style="font-size: small">(10:11:20)</span> <b>dwd:</b></span> If there’s no message, it means that autofs didn’t try again since the last failure.  Usually it waits for 5 minutes.<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:16:08)</span> <b>andrew.melo:</b></span> No dice<br/><pre><br/>[root@amn0001 ~]# systemctl restart autofs<br/>[root@amn0001 ~]# ls /cvmfs/cms.cern.ch<br/>ls: cannot access /cvmfs/cms.cern.ch: No such file or directory<br/></pre><br/>this is all from journalctl<br/><pre><br/>May 23 10:15:34 amn0001.vampire cvmfs2[518215]: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) geographic order of servers retrieved from <a href="http://cvmfs-s1goc.opensciencegrid.org">cvmfs-s1goc.opensciencegrid.org</a><br/>May 23 10:15:34 amn0001.vampire cvmfs2[518215]: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) CernVM-FS: linking /cvmfs/config-osg.opensciencegrid.org to repository <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a><br/></pre><br/>
<span style="color: #99a949"><span style="font-size: small">(10:17:12)</span> <b>dwd:</b></span> That is really bizarre.  Try ‘mount -t cvmfs <a href="http://cms.cern.ch">cms.cern.ch</a> /mnt’<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:18:02)</span> <b>andrew.melo:</b></span> (already have stuff in /mnt)<br/><pre><br/>[root@amn0001 ~]# mkdir /mnt/test<br/>[root@amn0001 ~]# mount -t cvmfs <a href="http://cms.cern.ch">cms.cern.ch</a> /mnt/test<br/>Repository <a href="http://cms.cern.ch">cms.cern.ch</a> is already mounted on /mnt/test<br/>[root@amn0001 ~]# ls /mnt/test<br/>[root@amn0001 ~]# ls /cvmfs/cms.cern.ch<br/>ls: cannot access /cvmfs/cms.cern.ch: No such file or directory<br/></pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(10:18:11)</span> <b>andrew.melo:</b></span> nada on journalctl<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:18:19)</span> <b>andrew.melo:</b></span> let me move to a host without active users<br/>
<span style="color: #99a949"><span style="font-size: small">(10:18:36)</span> <b>dwd:</b></span> mount anywhere else then, make a new directory<br/>
<span style="color: #99a949"><span style="font-size: small">(10:18:48)</span> <b>dwd:</b></span> Oh wait<br/>
<span style="color: #99a949"><span style="font-size: small">(10:19:07)</span> <b>dwd:</b></span> I see you did that.  Ok that says that there’s already <a href="http://cms.cern.ch">cms.cern.ch</a> processes running<br/>
<span style="color: #99a949"><span style="font-size: small">(10:19:21)</span> <b>dwd:</b></span> Look for cvmfs2 processes mentioning <a href="http://cms.cern.ch">cms.cern.ch</a><br/>
<span style="color: #a72f79"><span style="font-size: small">(10:19:34)</span> <b>andrew.melo:</b></span> Same symptoms, new host<br/><pre><br/>[root@amn0004 ~]# ls /cvmfs/cms.cern.ch<br/>ls: cannot access /cvmfs/cms.cern.ch: No such file or directory<br/>[root@amn0004 ~]# journalctl -f<br/>-- Logs begin at Mon 2018-05-21 18:49:06 CDT. --<br/>May 23 10:19:00 amn0004.vampire sshd[506777]: error: Could not load host key: /etc/ssh/ssh_host_dsa_key<br/>May 23 10:19:00 amn0004.vampire sshd[506777]: Accepted publickey for root from 10.0.32.126 port 45132 ssh2: RSA SHA256:HE2SM9Hre82TH25wHR7B6vpJwczG9xYOjx3xP9zbIrA<br/>May 23 10:19:00 amn0004.vampire systemd[1]: Created slice User Slice of root.<br/>May 23 10:19:00 amn0004.vampire systemd[1]: Starting User Slice of root.<br/>May 23 10:19:00 amn0004.vampire systemd-logind[1634]: New session 207 of user root.<br/>May 23 10:19:00 amn0004.vampire systemd[1]: Started Session 207 of user root.<br/>May 23 10:19:00 amn0004.vampire systemd[1]: Starting Session 207 of user root.<br/>May 23 10:19:00 amn0004.vampire sshd[506777]: pam_unix(sshd:session): session opened for user root by (uid=0)<br/>May 23 10:19:04 amn0004.vampire cvmfs2[506850]: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) geographic order of servers retrieved from <a href="http://cvmfs-s1goc.opensciencegrid.org">cvmfs-s1goc.opensciencegrid.org</a><br/>May 23 10:19:04 amn0004.vampire cvmfs2[506850]: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) CernVM-FS: linking /cvmfs/config-osg.opensciencegrid.org to repository <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a><br/></pre><br/>
<span style="color: #99a949"><span style="font-size: small">(10:19:46)</span> <b>dwd:</b></span> Do you have singularity containers running on this machine?<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:19:55)</span> <b>andrew.melo:</b></span> docker<br/>
<span style="color: #99a949"><span style="font-size: small">(10:20:14)</span> <b>dwd:</b></span> This is outside of docker or inside?<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:20:18)</span> <b>andrew.melo:</b></span> outside<br/>
<span style="color: #99a949"><span style="font-size: small">(10:20:27)</span> <b>dwd:</b></span> do you have <a href="http://cms.cern.ch">cms.cern.ch</a> cvmfs2 processes?<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:20:29)</span> <b>andrew.melo:</b></span> the containers bind-mount /cvmfs/cms.cern.ch<br/>
<span style="color: #e96699"><span style="font-size: small">(10:20:39)</span> <b>ian_cancercomputer:</b></span> @andrew.melo I've found /var/log/messages tends to provide more helpful info than journalctl does, don't know if that helps you diagnose.<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:20:55)</span> <b>andrew.melo:</b></span> I do have some:<br/><pre><br/>[root@amn0004 ~]# ps aux | grep <a href="http://cms.cern.ch">cms.cern.ch</a><br/>cvmfs     214773  0.0  0.0 763896 28828 ?        Sl   May22   0:00 /usr/bin/cvmfs2 -o rw,fsname=cvmfs2,allow_other,grab_mountpoint,uid=994,gid=990 <a href="http://cms.cern.ch">cms.cern.ch</a> /cvmfs/cms.cern.ch<br/>cvmfs     214777  0.0  0.0  89788 25016 ?        S    May22   0:00 /usr/bin/cvmfs2 -o rw,fsname=cvmfs2,allow_other,grab_mountpoint,uid=994,gid=990 <a href="http://cms.cern.ch">cms.cern.ch</a> /cvmfs/cms.cern.ch<br/></pre><br/>
<span style="color: #99a949"><span style="font-size: small">(10:23:03)</span> <b>dwd:</b></span> Yes so it is probably being held open inside a docker container, but it got unmounted outside.  I have seen that with singularity because singularity does not hold a file descriptor open outside of the container.  I haven’t observed it with docker but it may have the same problem.  The CMS glidein is aware of this and keeps a reference to the the repository outside of singularity.  In your case maybe what you need is a different docker mount option …<br/>
<span style="color: #99a949"><span style="font-size: small">(10:23:25)</span> <b>dwd:</b></span> --mount type=bind,source=/cvmfs,target=/cvmfs,bind-propagation=rshared<br/>
<span style="color: #99a949"><span style="font-size: small">(10:23:36)</span> <b>dwd:</b></span> Instead of just -v /cvmfs<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:24:25)</span> <b>andrew.melo:</b></span> Ughhhhhhh. it's propagated through mesos, which doesn't share the propagation. What's the default idle time for autofs? I can just have it <tt>ls</tt> the path periodically<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:24:44)</span> <b>andrew.melo:</b></span> (it's esp. bad because this is the cluster I'm trying to use for a tutorial @ 1 :disappointed: )<br/>
<span style="color: #99a949"><span style="font-size: small">(10:24:47)</span> <b>dwd:</b></span> 5 minutes, which you could change.  A  lot of people set timeout=0<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:25:49)</span> <b>andrew.melo:</b></span> I'll just hack salt real quick instead of making a config management change ... is there a way to gracefully recover these nodes, or do I need to slay the <a href="http://cms.cern.ch">cms.cern.ch</a> mounts to get them to come up?<br/>
<span style="color: #99a949"><span style="font-size: small">(10:26:02)</span> <b>dwd:</b></span> They need to be slain<br/>
<span style="color: #99a949"><span style="font-size: small">(10:26:16)</span> <b>dwd:</b></span> Or just unmounted inside the docker container(s)<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:26:43)</span> <b>andrew.melo:</b></span> I looked SO hard and couldn't figure out how to get docker to just use shared bind propagation by default<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:27:03)</span> <b>andrew.melo:</b></span> I found some people suggesting a systemd dropin, which also didn't work<br/>
<span style="color: #de5f24"><span style="font-size: small">(10:42:29)</span> <b>tiradani:</b></span> At FNAL we set the autofs timeout=0<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:44:20)</span> <b>bbockelm:</b></span> Is it a CMS-specific Docker image or a generic worker node?<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:44:48)</span> <b>bbockelm:</b></span> For a CMS-specific Docker image, you might break down and just manually mount things.<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:50:24)</span> <b>andrew.melo:</b></span> CMS-specific<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:51:13)</span> <b>andrew.melo:</b></span> I tried mounting /cvmfs, but the mount propagation wasn't working right if you started up your container before <a href="http://cms.cern.ch">cms.cern.ch</a> was mounted, so I tried mounting <a href="http://cms.cern.ch">cms.cern.ch</a> directly. Now i'm going to force <a href="http://cms.cern.ch">cms.cern.ch</a> to stay alive and go back to mounting /cvmfs<br/>
</body>
</html>

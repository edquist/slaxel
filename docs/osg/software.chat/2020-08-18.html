<!DOCTYPE html>
<html>
<head>
<title>Tue Aug 18, 2020 : #software (osg)</title>
</head>
<body>
<h3>Tue Aug 18, 2020 : #software (osg)</h3>
<span style="color: #a72f79"><span style="font-size: small">(09:20:00)</span> <b>andrew.melo:</b></span> Brian -- I'm gonna ask to re-provision CE3 to get some jobs sent there. The timing is suspicious that when we start seeing cancellations in the SLURM accounting DB is when the new CEs are added to the global pool<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:20:53)</span> <b>andrew.melo:</b></span> Should I maybe roll back one of the new CEs to an older version (maybe the osg 35 equivalent of the 34 packages on the old ones)<br/>
<span style="color: #43761b"><span style="font-size: small">(09:21:01)</span> <b>blin:</b></span> @marcom brought up that he's seen similar issues related to aggressive frontend/factory policies so we should reach out to antonio<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:21:21)</span> <b>andrew.melo:</b></span> well, at the least, I can get jobs that don't die so users get happier :wink:<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:21:37)</span> <b>andrew.melo:</b></span> I see that antonio CCd himeself to the ticket<br/>
<span style="color: #43761b"><span style="font-size: small">(09:21:39)</span> <b>blin:</b></span> i would just spin up the old one and see how it acts then if it exhibits the same problem maybe we downgrade<br/>
<span style="color: #43761b"><span style="font-size: small">(09:21:48)</span> <b>blin:</b></span> ah ok, i'll follow up on the ticket then<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:27:06)</span> <b>andrew.melo:</b></span> OK, I'll ask on that ticket to re-enable CE3. Are the right people on there?<br/>
<span style="color: #43761b"><span style="font-size: small">(09:29:58)</span> <b>blin:</b></span> we can re-enable it in topology, we just merged an @eric.appelt topology PR to deactivate it<br/>
<span style="color: #43761b"><span style="font-size: small">(09:30:10)</span> <b>blin:</b></span> idk what else is needed on the factory/frontend but if edita/antonio are on the ticket i think that's enough<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:49:39)</span> <b>andrew.melo:</b></span> I don't think the topology matters, the new CEs weren't even in there<br/>
<span style="color: #43761b"><span style="font-size: small">(10:07:54)</span> <b>blin:</b></span> eh, may as well keep topology up to date since that'll update gocdb<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:17:26)</span> <b>andrew.melo:</b></span> Hmmm, since you reassigned to software support, will the factory people see the mail?<br/>
<span style="color: #43761b"><span style="font-size: small">(10:18:29)</span> <b>blin:</b></span> ah, i don't think so. i don't see them on the cc list<br/>
<span style="color: #7d414c"><span style="font-size: small">(10:30:52)</span> <b>bockjoo:</b></span> Was there any error in the __condor__std{err,out}?<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:33:48)</span> <b>andrew.melo:</b></span> On the pilot? No. Just it receives a signal (SIGTERM), does a graceful shutdown, and everything exits. I even tailed the payload job's stdout/err and nothing<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:40:55)</span> <b>andrew.melo:</b></span> SLURM reports that the job was cancelled by the (pilot) user, so I suspect (?) it's somewhere above the SLURM layer<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:42:53)</span> <b>andrew.melo:</b></span> Brian -- should I mail someone about the entries?<br/>
<span style="color: #43761b"><span style="font-size: small">(10:46:30)</span> <b>blin:</b></span> i'm not sure, tbh. idk who's on the various lists that are cc'ed. it wouldn't hurt to send another email to factory ops and point them to this ticket<br/>
<span style="color: #7d414c"><span style="font-size: small">(10:46:37)</span> <b>bockjoo:</b></span> I would take a look at the stderr files under the spool directory.<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:46:48)</span> <b>andrew.melo:</b></span> what's that mail again?<br/>
<span style="color: #43761b"><span style="font-size: small">(10:47:20)</span> <b>blin:</b></span> <a href="mailto:osg-gfactory-support@physics.ucsd.edu">osg-gfactory-support@physics.ucsd.edu</a><br/>
<span style="color: #a72f79"><span style="font-size: small">(11:11:42)</span> <b>andrew.melo:</b></span> Unrelated to this current issue, I'm following up with another issue about CVMFS. Is there a way to get the stacktrace for this not fail?<br/><pre>Aug 15 17:56:19 ng1004 cvmfs2: (<a href="http://singularity.opensciencegrid.org">singularity.opensciencegrid.org</a>) stack trace generation failed<br/>Aug 15 17:56:19 ng1004 cvmfs2: (<a href="http://singularity.opensciencegrid.org">singularity.opensciencegrid.org</a>) Signal 6, errno 12<br/>Aug 15 17:56:19 ng1004 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) Backtrace (17 symbols):#012/usr/lib64/libcvmfs_fuse.so(+0x9c3d3) [0x7fc13a64c3d3]#012/lib64/libpthread.so.0(+0xf5d0) [0x7fc13d8575d0]#012/lib64/libc.so.6(gsignal+0x37) [0x7fc13cc92207]#012/lib64/libc.so.6(abort+0x148) [0x7fc13cc938f8]#012/lib64/libc.so.6(+0x2f026) [0x7fc13cc8b026]#012/lib64/libc.so.6(+0x2f0d2) [0x7fc13cc8b0d2]#012/usr/lib64/libcvmfs_fuse.so(+0x82bc6) [0x7fc13a632bc6]#012/usr/lib64/libcvmfs_fuse.so(+0x8443d) [0x7fc13a63443d]#012/usr/lib64/libcvmfs_fuse.so(+0x845dc) [0x7fc13a6345dc]#012/usr/lib64/libcvmfs_fuse.so(+0xd8bd2) [0x7fc13a688bd2]#012/usr/lib64/libcvmfs_fuse.so(+0xd8c98) [0x7fc13a688c98]#012/usr/lib64/libcvmfs_fuse.so(+0xd02d9) [0x7fc13a6802d9]#012/usr/lib64/libcvmfs_fuse_stub.so(+0x8ab7) [0x7fc13ca1fab7]#012/lib64/libfuse.so.2(+0x16b6b) [0x7fc13c11cb6b]#012/lib64/libfuse.so.2(+0x13401) [0x7fc13c119401]#012/lib64/libpthread.so.0(+0x7dd5) [0x7fc13d84fdd5]#012/lib64/libc.so.6(clone+0x6d) [0x7fc13cd59ead]</pre><br/><br/>
<span style="color: #43761b"><span style="font-size: small">(11:12:30)</span> <b>blin:</b></span> @dwd ^^<br/>
<span style="color: #99a949"><span style="font-size: small">(11:13:27)</span> <b>dwd:</b></span> by “not fail” you mean you want it to have more detailed symbols?<br/>
<span style="color: #99a949"><span style="font-size: small">(11:14:46)</span> <b>dwd:</b></span> @andrew.melo<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:15:19)</span> <b>andrew.melo:</b></span> Or perhaps make CVMFS dump core? Someone mentioned it looked like an OOM, but I strongly think it's not, considering there's no OOM messages in dmesg, and SLURM is configured to use cgroups which boxes off a few gigs for not-SLURM<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:15:42)</span> <b>andrew.melo:</b></span> Yeah, we're still seeing these errors with the latest (at the time I updated it) CVMFS client<br/>
<span style="color: #99a949"><span style="font-size: small">(11:20:24)</span> <b>dwd:</b></span> There is another library /usr/lib64/libcvmfs_fuse_debug.so that has the more detailed symbols already installed.  I’m not sure though how to get it to be used by the stacktrace in /var/log/messages.  Is this a rare issue?  It isn’t practical to turn on client debugging everywhere because the logs get too big<br/>
<span style="color: #99a949"><span style="font-size: small">(11:22:05)</span> <b>dwd:</b></span> According to <a href="https://cvmfs.readthedocs.io/en/stable/cpt-configure.html#debug-logs">https://cvmfs.readthedocs.io/en/stable/cpt-configure.html#debug-logs</a> there should also be a file “stacktrace” in the cache directory, I wonder if that has more detail<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:29:00)</span> <b>andrew.melo:</b></span> What we're seeing @ Vanderbilt is that we have two partitions -- production and "nogpfs". Nogpfs doesn't mount /home, so "regular" jobs can't work there w/o modifications. I plumbed CMS jobs so they never touch /home, so they can use that partition and effectively get 100% of it (GPFS is licensed per-socket, so we basically have a one-in-one-out policy for production, and the extras get shoved into nogpfs). Because only CMS jobs run there, pilots land, touch CVMFS, see it doesn't work and then exits. But the next job lands so quickly, autofs never times out, and the mounts stay broken.<br/><br/>Anyway, in a given week, out of ~230 of these nodes, we'll get 10-30 of them in this state that need to have a <tt>killall -9 -u cvmfs ; umount /cvmfs/*</tt> to clean them out<br/>
<span style="color: #99a949"><span style="font-size: small">(11:33:43)</span> <b>dwd:</b></span> So you’re saying that non-CMS pilots land, and for some reason related to no /home, CVMFS mounts are broken?<br/>
<span style="color: #a72f79"><span style="font-size: small">(11:49:38)</span> <b>andrew.melo:</b></span> No, it's ~only CMS pilots in that partition. It's way too expensive to have GPFS on every single node, so we have a partition w/o home. Since CMS doesn't need /home, half the jobs get sent to that partition. CVMFS is failing for an unrelated reason to that, but the nodes aren't recovering because autofs never cycles it<br/>
<span style="color: #99a949"><span style="font-size: small">(11:58:08)</span> <b>dwd:</b></span> Since you need to do <tt>killall -9 -u cvmfs</tt> I take it that means some processes are not exiting and letting go of the mountpoint, which would explain why autofs couldn’t unmount it.  What processes are hanging on?<br/>
<span style="color: #a72f79"><span style="font-size: small">(12:00:16)</span> <b>andrew.melo:</b></span> one sec<br/>
<span style="color: #a72f79"><span style="font-size: small">(12:07:50)</span> <b>andrew.melo:</b></span> Here's one<br/><pre>[meloam@ng1013 ~]$ ps aux | grep cvmfs<br/>cvmfs     2967  0.0  0.0 725252  1628 ?        Sl   Aug07   0:15 /usr/bin/cvmfs2 -o rw,system_mount,fsname=cvmfs2,allow_other,grab_mountpoint,uid=995,gid=992 <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a> /cvmfs/config-osg.opensciencegrid.org<br/>cvmfs     2972  0.0  0.0  91936   104 ?        S    Aug07   0:00 /usr/bin/cvmfs2 -o rw,system_mount,fsname=cvmfs2,allow_other,grab_mountpoint,uid=995,gid=992 <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a> /cvmfs/config-osg.opensciencegrid.org<br/>cvmfs     5932  0.0  0.1 947120 25532 ?        Sl   09:26   0:02 /usr/bin/cvmfs2 -o rw,system_mount,fsname=cvmfs2,allow_other,grab_mountpoint,uid=995,gid=992 <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a> /cvmfs/oasis.opensciencegrid.org<br/>cvmfs     5936  0.0  0.0  91936 23024 ?        S    09:26   0:00 /usr/bin/cvmfs2 -o rw,system_mount,fsname=cvmfs2,allow_other,grab_mountpoint,uid=995,gid=992 <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a> /cvmfs/oasis.opensciencegrid.org<br/>meloam   16377  0.0  0.0 112712   980 pts/0    S+   12:05   0:00 grep --color=auto cvmfs<br/>cvmfs    17587  0.5  0.1 1929184 30316 ?       Sl   08:46   1:09 /usr/bin/cvmfs2 -o rw,system_mount,fsname=cvmfs2,allow_other,grab_mountpoint,uid=995,gid=992 <a href="http://singularity.opensciencegrid.org">singularity.opensciencegrid.org</a> /cvmfs/singularity.opensciencegrid.org<br/>cvmfs    17592  0.0  0.0  91936 21020 ?        S    08:46   0:00 /usr/bin/cvmfs2 -o rw,system_mount,fsname=cvmfs2,allow_other,grab_mountpoint,uid=995,gid=992 <a href="http://singularity.opensciencegrid.org">singularity.opensciencegrid.org</a> /cvmfs/singularity.opensciencegrid.org<br/>cvmfs    25658  0.0  0.0  60704    32 ?        S    Aug01   0:00 /usr/bin/cvmfs2 __cachemgr__ . 7 8 21474836480 10737418240 0 3 -1 :<br/>cvmfs    25661  0.0  0.0  73708  2752 ?        S    Aug01  23:31 /usr/bin/cvmfs2 __cachemgr__ . 7 8 21474836480 10737418240 0 3 -1 :<br/>[meloam@ng1013 ~]$ ls /cvmfs/cms.cern.ch<br/>ls: cannot access /cvmfs/cms.cern.ch: Transport endpoint is not connected</pre><br/>
<span style="color: #99a949"><span style="font-size: small">(12:52:10)</span> <b>dwd:</b></span> Yes but the question is what user processes are still holding /cvmfs/oasis.opensciencegrid.org and /cvmfs/singularity.opensciencegrid.org open.  Probably the best way to see that is fuser -m as root.<br/>
<span style="color: #99a949"><span style="font-size: small">(12:54:29)</span> <b>dwd:</b></span> Oh maybe in this case the problem is /cvmfs/cms.cern.ch, since that’s the one with transport not connected and there are no <a href="http://cms.cern.ch">cms.cern.ch</a> cvmfs processes running anymore<br/>
<span style="color: #99a949"><span style="font-size: small">(12:56:16)</span> <b>dwd:</b></span> The reason there are two processes for each mountpoint is that one of them is a “watchdog” process that watches for the other to exit and stores the stacktrace and tries to unmount the mountpoint.  The watchdog has gone away, but it apparently failed in its attempt to unmount /cvmfs/cms.cern.ch<br/>
<span style="color: #a72f79"><span style="font-size: small">(12:56:58)</span> <b>andrew.melo:</b></span> Oh, I see<br/>
<span style="color: #a72f79"><span style="font-size: small">(12:57:48)</span> <b>andrew.melo:</b></span> <pre>[root@ng1013 ~]# fuser -m /cvmfs/cms.cern.ch<br/>Cannot stat /cvmfs/cms.cern.ch: Transport endpoint is not connected<br/>[root@ng1013 ~]# fuser -m /cvmfs/<br/><a href="http://cms.cern.ch">cms.cern.ch</a>                      <a href="http://config-osg.opensciencegrid.org/">config-osg.opensciencegrid.org/</a>  <a href="http://oasis.opensciencegrid.org/">oasis.opensciencegrid.org/</a>       <a href="http://singularity.opensciencegrid.org/">singularity.opensciencegrid.org/</a> <br/>[root@ng1013 ~]# fuser -m /cvmfs/config-osg.opensciencegrid.org/<br/>[root@ng1013 ~]# fuser -m /cvmfs/oasis.opensciencegrid.org/<br/>[root@ng1013 ~]# fuser -m /cvmfs/singularity.opensciencegrid.org/<br/>/cvmfs/singularity.opensciencegrid.org:  8211  8566  8592e  8707e</pre><br/>
<span style="color: #99a949"><span style="font-size: small">(12:58:58)</span> <b>dwd:</b></span> Maybe an lsof on all processes and look for a file descriptor that’s open to /cvmfs/cms.cern.ch<br/>
<span style="color: #a72f79"><span style="font-size: small">(13:00:26)</span> <b>andrew.melo:</b></span> bupkis<br/><pre>[root@ng1013 ~]# lsof| grep <a href="http://cms.cern.ch">cms.cern.ch</a><br/>lsof: WARNING: can't stat() fuse file system /cvmfs/cms.cern.ch<br/>      Output information may be incomplete.<br/>[root@ng1013 ~]# </pre><br/>
<span style="color: #99a949"><span style="font-size: small">(13:01:34)</span> <b>dwd:</b></span> Try <tt>lsof 2&gt;&amp;1 | less</tt> and find out which process complained about the stat<br/>
<span style="color: #99a949"><span style="font-size: small">(13:04:12)</span> <b>dwd:</b></span> If you can figure out what process it is try to get a stack trace to see where it is stuck<br/>
<span style="color: #a72f79"><span style="font-size: small">(13:05:26)</span> <b>andrew.melo:</b></span> argh, sorry to do this, but I've gotta loop back around to this later in the afternoon. I'll see what I can find<br/>
<span style="color: #99a949"><span style="font-size: small">(13:05:41)</span> <b>dwd:</b></span> ok<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:57:14)</span> <b>andrew.melo:</b></span> Well @blin -- CE5 didn't kill its jobs today, bunch of jobs have been running for &gt;15 hours at this point. I didn't touch it, so I'm very confused<br/>
<span style="color: #43761b"><span style="font-size: small">(16:57:57)</span> <b>blin:</b></span> FIXED!<br/>
<span style="color: #43761b"><span style="font-size: small">(16:58:11)</span> <b>blin:</b></span> i wonder if frontend or factory ops changed something?<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:00:08)</span> <b>andrew.melo:</b></span> I have no idea, but CE6 killed its jobs just a bit ago....<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:01:15)</span> <b>andrew.melo:</b></span> I guess I'll leave it be for a couple cycles and see if it stays?<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:02:18)</span> <b>andrew.melo:</b></span> Bummed I asked Jeff and Eric to turn back on the old CE to troubleshoot and the new one fixed itself<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:03:47)</span> <b>andrew.melo:</b></span> Is there much of a change between these versions?<br/><pre>---&gt; Package htcondor-ce-slurm.noarch 0:4.2.1-1.osg35.el7 will be updated<br/>---&gt; Package htcondor-ce-slurm.noarch 0:4.4.1-1.osg35.el7 will be an update</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(17:04:52)</span> <b>andrew.melo:</b></span> (I didn't hit it, but that's basically the difference between ce5 &amp; ce6)<br/>
<span style="color: #43761b"><span style="font-size: small">(17:08:06)</span> <b>blin:</b></span> not that i can see from the changelog<br/>
<span style="color: #43761b"><span style="font-size: small">(17:08:19)</span> <b>blin:</b></span> nothing that should affect blahp + proxies at least<br/>
<span style="color: #43761b"><span style="font-size: small">(17:15:21)</span> <b>blin:</b></span> :confused:<br/>
<span style="color: #43761b"><span style="font-size: small">(17:15:53)</span> <b>blin:</b></span> are you maybe having shared FS issues? are you still running the custom blahp with http servers?<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:32:45)</span> <b>andrew.melo:</b></span> The person @ Vanderbilt who was vehemantly opposed to NFS mounts on cluster nodes left, so the CEs export their spool dir via NFS and the cluster mounts it now, so I don't gotta deal with the HTTP thing anymore<br/>
<span style="color: #a72f79"><span style="font-size: small">(18:12:27)</span> <b>andrew.melo:</b></span> J/K, the other machine killed its jobs, but strangely, it left a lot of the local jobs?<br/>
</body>
</html>

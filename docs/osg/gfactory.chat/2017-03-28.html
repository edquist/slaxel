<!DOCTYPE html>
<html>
<head>
<title>Tue Mar 28, 2017 : #gfactory (osg)</title>
</head>
<body>
<h3>Tue Mar 28, 2017 : #gfactory (osg)</h3>
<span style="color: #43761b"><span style="font-size: small">(11:29:34)</span> <b>blin:</b></span> hey guys, question from christina koch at UW: we're seeing a user with 1 cpu/1gb mem jobs get 4x the amount of slots out in the OSG as a user with 1 cpu/2gb mem jobs. Is that disparity expected i.e. are most opportunistic pilots limited to 1 core/1 gb mem?<br/>
<span style="color: #bd9336"><span style="font-size: small">(11:32:55)</span> <b>mkandes:</b></span> I don't think there's much of a difference between opportunistic and non-opportunistic resources out there. Can you clarify what's happening? What is the user requesting in terms of CPU and MEM? And how are you confirming they are actually being reserved more than that?<br/>
<span style="color: #8d4b84"><span style="font-size: small">(11:33:04)</span> <b>kherner:</b></span> @blin It's probably coming from being able to use "leftover" slices of partitionable slots. Suppose you have a user that asks for 1 core and 3 GB ram, or 2.5 GB of RAM; a job requesting only 1 GB could fit into a "leftover" slice that a job requesting 2 GB couldn't.<br/>
<span style="color: #bd9336"><span style="font-size: small">(11:34:10)</span> <b>mkandes:</b></span> Ah, okay. You're saying one user is getting more resources than a separate user.<br/>
<span style="color: #43761b"><span style="font-size: small">(11:34:30)</span> <b>blin:</b></span> @mkandes yea, sorry if that wasn't clear<br/>
<span style="color: #bd9336"><span style="font-size: small">(11:34:33)</span> <b>mkandes:</b></span> Yes, the one with the lower resource request will likely match more slots.<br/>
<span style="color: #8d4b84"><span style="font-size: small">(11:34:50)</span> <b>kherner:</b></span> Whether that should buy you a factor of 4 I don't know, but it doesn't surprise me that it is a big boost<br/>
<span style="color: #bd9336"><span style="font-size: small">(11:34:53)</span> <b>mkandes:</b></span> In general, the average amount of resources out there is 2 GB per 1 CPU core.<br/>
<span style="color: #43761b"><span style="font-size: small">(11:34:57)</span> <b>blin:</b></span> thanks @kherner , that's along the lines of what i figured to be happening<br/>
<span style="color: #bd9336"><span style="font-size: small">(11:36:35)</span> <b>mkandes:</b></span> We saw same problem when CMS transitioned to multicore glideins. At sites that kept running single-core glideins alongside multicore glideins saw single-core glideins starving out the multicore glideins.<br/>
<span style="color: #8d4b84"><span style="font-size: small">(11:37:04)</span> <b>kherner:</b></span> @mkandes We were also seeing that on FNAL GPGrid<br/>
<span style="color: #8d4b84"><span style="font-size: small">(11:37:24)</span> <b>kherner:</b></span> at least somewhat. We ended up asking ATLAS to submit multicore pilots only<br/>
<span style="color: #bd9336"><span style="font-size: small">(11:37:27)</span> <b>mkandes:</b></span> Yeah, it's a general problem with pretty much all batch systems.<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:43:33)</span> <b>rynge:</b></span> That is why we prefer whole node glideins, and then use pslots on them. Less pressure on the gwms, and the most flexibility for the VO.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:44:45)</span> <b>bbockelm:</b></span> Who's doing factory ops today?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:46:40)</span> <b>bbockelm:</b></span> ( @rynge - John is here today and we're going to take another whack at whole nodes.  Haven't forgotten about you!)<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:46:54)</span> <b>rynge:</b></span> Cool!<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:47:26)</span> <b>bbockelm:</b></span> We have a ton of 16 core / 32GB RAM hosts that can only land a single 8 core / 20GB RAM CMS pilot.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:47:47)</span> <b>bbockelm:</b></span> (plus all the other inefficiencies from the fact a lot of our generations of hardware have &gt;2GB RAM / core)<br/>
<span style="color: #bd9336"><span style="font-size: small">(12:03:36)</span> <b>mkandes:</b></span> I'm covering factory ops today if you need something ...<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:14:11)</span> <b>bbockelm:</b></span> @mkandes - what are the lifetimes for CMS pilots at Nebraska?  Are they 48 hours?<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:14:35)</span> <b>bbockelm:</b></span> I was surprised by how short the <tt>GLIDEIN_ToDie</tt> was set to for a few pilots I was looking at last night.<br/>
<span style="color: #bd9336"><span style="font-size: small">(12:15:09)</span> <b>mkandes:</b></span> Looks like 24 hours.<br/>
<span style="color: #bd9336"><span style="font-size: small">(12:15:33)</span> <b>mkandes:</b></span> But we're also not setting the wall time submit attribute for some reason.<br/>
<span style="color: #bd9336"><span style="font-size: small">(12:15:48)</span> <b>mkandes:</b></span> So maybe you are assigning a default at the CE?<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:18:32)</span> <b>rynge:</b></span> Or you have a really long GLIDEIN_Job_Max_Time<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:19:37)</span> <b>bbockelm:</b></span> Yeah - the analysis jobs claim to all run for 24 hours, but they actually run for something like 2 hours.<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:20:01)</span> <b>bbockelm:</b></span> So the 48 hour pilot quickly becomes a 24 hour pilot + draining.<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:26:35)</span> <b>rynge:</b></span> Well if it is 24 hours - 24 hours, that would mean it would only accept jobs a little bit in the beginning. There is a warning in the stdout/stderr of the glidein if that happens<br/>
<span style="color: #bd9336"><span style="font-size: small">(12:32:56)</span> <b>mkandes:</b></span> @bbockelm: Sorry, I misspoke earlier ... for CMS the glidein lifetime ranges from 48 hours to 72 hours ... I thought we were talking about the whole nodes for OSG as an opportunistic user. Looks like all opportunistic users at Nebraska only get 24 hours.<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:33:28)</span> <b>bbockelm:</b></span> That part sounds correct.  Can you bump the 48 hour CMS pilots to 72 hours?<br/>
<span style="color: #bd9336"><span style="font-size: small">(12:33:52)</span> <b>mkandes:</b></span> Yes. So you want all CMS entries up to 72 hours at Nebraska?<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:34:46)</span> <b>bbockelm:</b></span> yes please.<br/>
<span style="color: #bd9336"><span style="font-size: small">(12:35:37)</span> <b>mkandes:</b></span> Okay, will do as soon as GOC systems comes back up .... looks like they are rebooting systems for maintenance at the moment.<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:48:14)</span> <b>bbockelm:</b></span> ah yes, happy Tuesday!<br/>
<span style="color: #bd9336"><span style="font-size: small">(13:38:35)</span> <b>mkandes:</b></span> @bbockelm: Okay, I think I've got everything sorted out. All CMS glideins should be requesting 72 hours, while all non-CMS glideins should be requesting 24 hours ...<br/>
<span style="color: #bd9336"><span style="font-size: small">(13:38:38)</span> <b>mkandes:</b></span> CMS_T2_US_Nebraska_Red : GFACTOPS-1125<br/>    CMS_T2_US_Nebraska_Red-hotpotato<br/>    CMS_T2_US_Nebraska_Red_gw1<br/>    CMS_T2_US_Nebraska_Red_gw1_whole<br/>    CMS_T2_US_Nebraska_Red_gw2<br/>    CMS_T2_US_Nebraska_Red_gw2_whole<br/>    CMS_T2_US_Nebraska_Red_whole<br/>    CMS_T2_US_Nebraska_sandhills_ce1<br/>    <br/>        set +maxWallTime = 4320; GLIDEIN_Max_Walltime = 257400<br/>    <br/>    CMS_T2_US_Nebraska_Red_gw1_op<br/>    CMS_T2_US_Nebraska_Red_gw2_op<br/>    CMS_T2_US_Nebraska_Red_op<br/>    CMS_T2_US_Nebraska_sandhills_ce1_op<br/>    <br/>        set +maxWallTime = 1440; GLIDEIN_Max_Walltime = 84600; also<br/>        created CMS_T2_US_Nebraska_sandhills_ce1_op<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:39:10)</span> <b>bbockelm:</b></span> Ok.  I'll follow up in a few days (maybe next week?) with factory ops if we need to increase things again.<br/>
<span style="color: #bd9336"><span style="font-size: small">(13:39:57)</span> <b>mkandes:</b></span> Sounds good. I'll leave our internal Jira ticket open then.<br/>
<span style="color: #385a86"><span style="font-size: small">(16:30:45)</span> <b>jdost321:</b></span> @mkandes i can add the condor 8.6 pilots to goc-itb if you haven't already for the cms ipv6 testing<br/>
<span style="color: #bd9336"><span style="font-size: small">(16:32:47)</span> <b>mkandes:</b></span> Yeah, go for it.<br/>
<span style="color: #385a86"><span style="font-size: small">(16:41:28)</span> <b>jdost321:</b></span> interesting i see they were already made, just not added to the xml<br/>
<span style="color: #385a86"><span style="font-size: small">(16:41:29)</span> <b>jdost321:</b></span> -rw-r--r-- 1 root     root     5331291 Mar  7 21:46 gfactory-3.2.18-1-condor-8.6.1-x86_64_RedHat6-stripped.tar.gz<br/>
<span style="color: #385a86"><span style="font-size: small">(16:41:39)</span> <b>jdost321:</b></span> in  /var/lib/gwms-factory/condor/<br/>
<span style="color: #bd9336"><span style="font-size: small">(16:45:25)</span> <b>mkandes:</b></span> Ah, I think I made them when I updated 3.2.18 ... but did not add since no one had asked for them.<br/>
<span style="color: #bd9336"><span style="font-size: small">(16:45:55)</span> <b>mkandes:</b></span> I think I used 8.6.1. as example while showing Vaibahv how factory update worked.<br/>
<span style="color: #385a86"><span style="font-size: small">(16:46:17)</span> <b>jdost321:</b></span> it's still the current 8.6 so i'll just put it in the xml<br/>
<span style="color: #bd9336"><span style="font-size: small">(16:46:27)</span> <b>mkandes:</b></span> Sounds good.<br/>
<span style="color: #bd9336"><span style="font-size: small">(19:48:48)</span> <b>mkandes:</b></span> [root@osg-jetstream-worker condor]# condor_status<br/>Name               OpSys      Arch   State     Activity LoadAv Mem   ActvtyTime<br/><br/>slot1@osg-jetstrea LINUX      X86_64 Claimed   Busy      0.090 1985  0+00:00:53<br/>slot2@osg-jetstrea LINUX      X86_64 Unclaimed Idle      0.000 1985  0+00:30:05<br/>slot3@osg-jetstrea LINUX      X86_64 Unclaimed Idle      0.000 1985  0+00:30:06<br/>slot4@osg-jetstrea LINUX      X86_64 Unclaimed Idle      0.000 1985  0+00:30:07<br/>slot5@osg-jetstrea LINUX      X86_64 Unclaimed Idle      0.000 1985  0+00:30:08<br/>slot6@osg-jetstrea LINUX      X86_64 Unclaimed Idle      0.000 1985  0+00:30:09<br/>slot7@osg-jetstrea LINUX      X86_64 Unclaimed Idle      0.000 1985  0+00:30:10<br/>slot8@osg-jetstrea LINUX      X86_64 Unclaimed Idle      0.000 1985  0+00:30:03<br/>                     Machines Owner Claimed Unclaimed Matched Preempting<br/><br/>        X86_64/LINUX        8     0       1         7       0          0<br/><br/>               Total        8     0       1         7       0          0<br/>[root@osg-jetstream-worker condor]# exit<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:51:10)</span> <b>rynge:</b></span> Nice! So are we getting close to production?<br/>
<span style="color: #bd9336"><span style="font-size: small">(19:53:28)</span> <b>mkandes:</b></span> Yeah, I will send email shortly to the group. I've got about as much configured as I can probably do. I see at least two tweaks that will need some adjusting: (1) sizing image to instance flavor disk size --- doesn't happen automagically; and (2) here above all the cores are auto-assigned to slot1 for some reason by default ... not sure why.<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:54:30)</span> <b>rynge:</b></span> Hmm do they have unique hostnames?<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:55:15)</span> <b>rynge:</b></span> The easiest is probably to do pslots and ask for whole node glideins in the factory<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:55:38)</span> <b>rynge:</b></span> So, one glidein per vm<br/>
<span style="color: #bd9336"><span style="font-size: small">(19:55:44)</span> <b>mkandes:</b></span> Is there a simple condor knob for that. Never set it up myself.<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:55:46)</span> <b>rynge:</b></span> and then let the VO partition it up<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:55:51)</span> <b>rynge:</b></span> pslots?<br/>
<span style="color: #bd9336"><span style="font-size: small">(19:55:54)</span> <b>mkandes:</b></span> Yes.<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:56:39)</span> <b>rynge:</b></span> # dynamic slots<br/>SLOT_TYPE_1 = cpus=100%,disk=100%,swap=100%<br/>SLOT_TYPE_1_PARTITIONABLE = TRUE<br/>NUM_SLOTS = 1<br/>NUM_SLOTS_TYPE_1 = 1<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:56:58)</span> <b>rynge:</b></span> Put that in the condor config on the workers<br/>
<span style="color: #bd9336"><span style="font-size: small">(19:57:13)</span> <b>mkandes:</b></span> Okay, I'll add that in.<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:58:53)</span> <b>rynge:</b></span> Not sure you need it to be partitional<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:59:04)</span> <b>rynge:</b></span> I mean, all we want is 1 glidein per vm<br/>
<span style="color: #674b1b"><span style="font-size: small">(19:59:12)</span> <b>rynge:</b></span> So that could be one big static slot as well<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:29:22)</span> <b>mkandes:</b></span> Yeah, there is an 8-core glidein running in that one slot for some reason by default, which was not the case on SDSC Openstack cluster .... startdlog on jetstream assigned all the cores to slot1 out-of-the-box for some reason.<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:29:24)</span> <b>mkandes:</b></span> 03/29/17 00:10:06 Allocating auto shares for slot type 0: Cpus: auto, Memory: auto, Swap: auto, Disk: auto<br/>slot type 0: Cpus: 1.000000, Memory: 1985, Swap: 12.50%, Disk: 12.50%<br/>slot type 0: Cpus: 1.000000, Memory: 1985, Swap: 12.50%, Disk: 12.50%<br/>slot type 0: Cpus: 1.000000, Memory: 1985, Swap: 12.50%, Disk: 12.50%<br/>slot type 0: Cpus: 1.000000, Memory: 1985, Swap: 12.50%, Disk: 12.50%<br/>slot type 0: Cpus: 1.000000, Memory: 1985, Swap: 12.50%, Disk: 12.50%<br/>slot type 0: Cpus: 1.000000, Memory: 1985, Swap: 12.50%, Disk: 12.50%<br/>slot type 0: Cpus: 1.000000, Memory: 1985, Swap: 12.50%, Disk: 12.50%<br/>slot type 0: Cpus: 1.000000, Memory: 1985, Swap: 12.50%, Disk: 12.50%<br/>03/29/17 00:10:06 slot1: New machine resource allocated<br/>03/29/17 00:10:06 Setting up slot pairings<br/>03/29/17 00:10:06 slot2: New machine resource allocated<br/>03/29/17 00:10:06 Setting up slot pairings<br/>03/29/17 00:10:06 slot3: New machine resource allocated<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:29:43)</span> <b>mkandes:</b></span> I think ...<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:31:58)</span> <b>mkandes:</b></span> Well, but looks like glidein RequestCpu = 1 .... need to double check.<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:32:00)</span> <b>mkandes:</b></span> RemoteUserCpu = 8.0<br/>RequestCpus = 1<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:32:52)</span> <b>mkandes:</b></span> Ah, I probably need to config the Job Router still ...<br/>
</body>
</html>

<!DOCTYPE html>
<html>
<head>
<title>Wed Jan 27, 2021 : #uscms (osg)</title>
</head>
<body>
<h3>Wed Jan 27, 2021 : #uscms (osg)</h3>
<span style="color: #7d414c"><span style="font-size: small">(00:00:56)</span> <b>bockjoo:</b></span> <pre>universe = grid<br/>grid_resource = condor <a href="http://cms.rc.ufl.edu">cms.rc.ufl.edu</a> <a href="http://cms.rc.ufl.edu:9619">cms.rc.ufl.edu:9619</a><br/>executable = check_site_slurm_gpu.sh<br/>arguments = "1 2 3 4 5"<br/>output = check_site_slurm_gpu.out<br/>error = check_site_slurm_gpu.err<br/>log = check_site_slurm_gpu.log<br/>TransferInputFiles = check_site_slurm_gpu.sub<br/>ShouldTransferFiles = YES<br/>WhenToTransferOutput = ON_EXIT<br/>use_x509userproxy = true<br/>+RequestGPUs = 1 <br/>queue</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(00:01:13)</span> <b>bockjoo:</b></span> Executable:<br/>
<span style="color: #7d414c"><span style="font-size: small">(00:01:58)</span> <b>bockjoo:</b></span> <pre>#!/bin/bash<br/>echo INITIAL DIRECTORY<br/>pwd<br/>echo Hostname<br/>uname -a<br/>source  /home/bockjoo/setup_gpu.sh <br/>echo INFO checking which python<br/>which python<br/>export LD_LIBRARY_PATH=/lib64:$LD_LIBRARY_PATH:/cmsuf/ana/bockjoo/cuda-11.2/lib64<br/>echo INFO checking GPU<br/>python -c "import tensorflow as tf ; import time ; print('GPU list physical devices ',tf.config.list_physical_devices('GPU'))"<br/>status=$?<br/>echo INFO Job Done<br/>exit $status</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(00:02:59)</span> <b>bockjoo:</b></span> Verify CE Router once the job is submitted:<br/>
<span style="color: #7d414c"><span style="font-size: small">(00:03:20)</span> <b>bockjoo:</b></span> <pre>condor_ce_q -const 'RouteName == "Slurm_GPU"' -af:jh RequestGPUs</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(00:04:09)</span> <b>bockjoo:</b></span> Debug:...<br/>
<span style="color: #7d414c"><span style="font-size: small">(00:05:41)</span> <b>bockjoo:</b></span> This procedure worked for me with my local account mapping. I don't have GPUs for pilots so I have not tested it with CRAB, but that should work.<br/>
<span style="color: #7d414c"><span style="font-size: small">(00:11:41)</span> <b>bockjoo:</b></span> My local account belongs to one qos where we have 5 GPUs, but the qos to which the pilot account belongs does not have any GPU.<br/>
<span style="color: #7d414c"><span style="font-size: small">(00:11:56)</span> <b>bockjoo:</b></span> How many GPUs do other sites have?<br/>
<span style="color: #de5f24"><span style="font-size: small">(00:17:46)</span> <b>justas.balcas:</b></span> 8x Titan Xp; 18 x 1080 - that is on Tier2. On HPC - we have access to another 200 GPUs (P100); We simply wait for 30XX series to become available<br/>
<span style="color: #7d414c"><span style="font-size: small">(00:34:21)</span> <b>bockjoo:</b></span> 26 GPUs from the T2 fund?<br/>
<blockquote>
<span style="color: #de5f24"><span style="font-size: small">(08:43:22)</span> <b>justas.balcas:</b></span> It is on Tier2 infrastructure, but not from Tier2 funds. We have not bought any GPUs with Tier2 funds yet.<br/>
</blockquote>
<span style="color: #73769d"><span style="font-size: small">(08:11:43)</span> <b>smithnp:</b></span> we put a T4 in each of our new hammer compute nodes, 16 total. Thanks for the info guys!  Gives me plenty of stuff to work through today...<br/>
<span style="color: #73769d"><span style="font-size: small">(08:13:30)</span> <b>smithnp:</b></span> We are also adding some V100's to the Purdue GPU community cluster, but that is next months task...  I believe the end goal is to have these accessible by CMS users in some fashion as well, but that cluster does not have a CE associated with it...<br/>
<span style="color: #7d414c"><span style="font-size: small">(16:09:38)</span> <b>bockjoo:</b></span> @smithnp requested the config files here. So I am putting the contents here:<br/>
<span style="color: #7d414c"><span style="font-size: small">(16:09:44)</span> <b>bockjoo:</b></span> <pre>/etc/condor-ce/config.d/02-ce-slurm.conf</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(16:10:04)</span> <b>bockjoo:</b></span> <pre>JOB_ROUTER_ENTRIES @=jre<br/>   [<br/>     GridResource = "batch slurm";<br/>     name = "SAM_tests";<br/>     Requirements = (Owner == "lcg");<br/>     set_default_maxMemory = 2000;<br/>     set_default_xcount = 1;<br/>     set_default_maxWallTime = 10;<br/>   ]<br/>   [<br/>     GridResource = "batch slurm";<br/>     name = "Local_Slurm";<br/>     Requirements = ( (Owner != "lcg") &amp;&amp; ( isUndefined(TARGET.RequestGPUs) || (TARGET.RequestGPUs =?= 0) ) ) ;<br/>     MaxIdleJobs = 120;<br/>     # 48 hrs max walltime<br/>     set_default_maxWallTime = 2880;<br/>   ]<br/>   [<br/>     GridResource = "batch slurm";<br/>     name = "Slurm_GPU";<br/>     Requirements = TARGET.RequestGPUs &gt;= 1;<br/>     MaxIdleJobs = 120;<br/>     # 48 hrs max walltime<br/>     set_default_maxWallTime = 2880;<br/>     set_default_CERequirements = "RequestGPUs";<br/>   ]<br/>@jre</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(16:10:26)</span> <b>bockjoo:</b></span> <pre>/etc/condor-ce/config.d/99-local.conf</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(16:10:52)</span> <b>bockjoo:</b></span> <pre>ALL_DEBUG=D_ALWAYS:2 D_CAT<br/>GRIDMANAGER_DEBUG = D_FULLDEBUG<br/>MASTER_DEBUG = D_FULLDEBUG<br/>SCHEDD_DEBUG = D_FULLDEBUG<br/>MAX_GRIDMANAGER_LOG=10Mb<br/>MAX_NUM_GRIDMANAGER_LOG = 8<br/>GRIDMANAGER_MAX_SUBMITTED_JOBS_PER_RESOURCE = 20000<br/>SYSTEM_PERIODIC_HOLD = (x509userproxysubject =?= UNDEFINED) || (x509UserProxyExpiration =?= UNDEFINED) || (time() &gt; x509UserProxyExpiration) || (RoutedBy is null &amp;&amp; JobUniverse =!= 1 &amp;&amp; JobUniverse =!= 5 &amp;&amp; JobUniverse =!= 7 &amp;&amp; JobUniverse =!= 12)<br/>SYSTEM_PERIODIC_HOLD_REASON = \<br/>   strcat("CE job in status ", JobStatus, \<br/>     " put on hold by SYSTEM_PERIODIC_HOLD due to ", \<br/>     ifThenElse(isUndefined(x509userproxysubject) || isUndefined(x509UserProxyExpiration),  "missing user proxy.", \<br/>       ifThenElse(time() &gt; x509UserProxyExpiration, "expired user proxy.", \<br/>         ifThenElse(RoutedBy is null &amp;&amp; JobUniverse =!= 1 &amp;&amp; JobUniverse =!= 5 &amp;&amp; JobUniverse =!= 7 &amp;&amp; JobUniverse =!= 12, \<br/>           "invalid job universe.", \<br/>           "non-existent route or entry in JOB_ROUTER_ENTRIES." \<br/>         ) \<br/>       ) \<br/>     ) \<br/>   )<br/>SPOOL=/var/lib/condor-ce/cms/spool <br/>CONDOR_VIEW_HOST = <a href="http://collector1.opensciencegrid.org:9619?alias=collector1.opensciencegrid.org,collector2.opensciencegrid.org:9619?alias=collector2.opensciencegrid.org">collector1.opensciencegrid.org:9619?alias=collector1.opensciencegrid.org,collector2.opensciencegrid.org:9619?alias=collector2.opensciencegrid.org</a><br/>ALLOW_ADMINISTRATOR = <a href="mailto:cms.rc.ufl.edu@daemon.opensciencegrid.org">cms.rc.ufl.edu@daemon.opensciencegrid.org</a>/cms.rc.ufl.edu<br/>JOB_ROUTER_ROUTE_NAMES = SAM_tests Local_Slurm Slurm_GPU<br/>CONDOR_Q_DASH_BATCH_IS_DEFAULT=False<br/>CONDOR_Q_ONLY_MY_JOBS = False<br/>SCHEDD_INTERVAL = 150</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(16:11:13)</span> <b>bockjoo:</b></span> <pre>/etc/blahp/slurm_local_submit_attributes.sh</pre><br/>
<span style="color: #73769d"><span style="font-size: small">(16:11:22)</span> <b>smithnp:</b></span> ahh, the ce-slurm part was what I wasn't sure about, but looks like I am on the right track<br/>
<span style="color: #7d414c"><span style="font-size: small">(16:15:32)</span> <b>bockjoo:</b></span> <pre>#!/bin/sh<br/>if [[ "${RequestGPUs:-0}" -gt 0 ]]; then<br/>echo "#SBATCH --account=avery"<br/>echo "#SBATCH --qos=avery"<br/>echo "#SBATCH --partition=gpu"<br/>echo "#SBATCH --gres=gpu:1"<br/>echo "#SBATCH --time=01:00:00"<br/>echo "#SBATCH --ntasks=1"                       #Processors per node<br/>else # if [[ "${RequestGPUs:-0}" -gt 0 ]]; then<br/>echo "#SBATCH --account=cmsdata"<br/>echo "#SBATCH --qos=cmsdata"<br/>if [ $(/bin/id -un) == lcg ] ; then<br/>echo "#SBATCH --partition=hpg2-compute"<br/>else<br/>echo "#SBATCH --partition=hpg2-compute"<br/>fi<br/>echo "#### Walltime=$Walltime CE=cms"<br/>if [ "x$(/usr/bin/whoami)" == "xlcg" ] ; then<br/>       echo "#SBATCH --mem=2000"<br/>       echo "#SBATCH --cpus-per-task=1"<br/>       echo "#SBATCH --time=00:10:00"<br/>else<br/>       echo "#SBATCH --time=48:00:00"<br/>fi<br/>fi # if [[ "${RequestGPUs:-0}" -gt 0 ]]; then<br/>echo "[ -f /etc/profile.d/modules.sh ] &amp;&amp; source /etc/profile.d/modules.sh"</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(16:15:58)</span> <b>bockjoo:</b></span> Glad to hear that!<br/>
</body>
</html>

<!DOCTYPE html>
<html>
<head>
<title>Tue Jan 7, 2020 : #uscms (osg)</title>
</head>
<body>
<h3>Tue Jan 7, 2020 : #uscms (osg)</h3>
<span style="color: #de5f24"><span style="font-size: small">(14:54:03)</span> <b>justas.balcas:</b></span> @bockjoo which one RAID Controller?<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:06:01)</span> <b>andrew.melo:</b></span> @bockjoo we have the version w/o the external interface<br/>
<span style="color: #de5f24"><span style="font-size: small">(15:11:14)</span> <b>justas.balcas:</b></span> @andrew.melo <a href="https://cms-gwmsmon.cern.ch/totalview/json/T2_US_Caltech/summary">https://cms-gwmsmon.cern.ch/totalview/json/T2_US_Caltech/summary</a><br/>
<span style="color: #de5f24"><span style="font-size: small">(15:11:36)</span> <b>justas.balcas:</b></span> dict -&gt; PilotUsage -&gt; PilotStatistics<br/>
<span style="color: #de5f24"><span style="font-size: small">(15:11:54)</span> <b>justas.balcas:</b></span> and you can map this with your local mon stats<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:12:01)</span> <b>andrew.melo:</b></span> Ahhhhhh very nice<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:13:56)</span> <b>andrew.melo:</b></span> I guess (?) I could also find pilot jobs that lasted &lt;5 mins or something?<br/>
<span style="color: #de5f24"><span style="font-size: small">(15:15:53)</span> <b>justas.balcas:</b></span> And that is what we have:<br/>
<span style="color: #de5f24"><span style="font-size: small">(15:15:55)</span> <b>justas.balcas:</b></span> also who is running and how much<br/>
<span style="color: #de5f24"><span style="font-size: small">(15:16:09)</span> <b>justas.balcas:</b></span> also I can see pilot splitting in num of cores,mem<br/>
<span style="color: #de5f24"><span style="font-size: small">(15:19:46)</span> <b>justas.balcas:</b></span> and yes, you can plot and find out pilots which ran too low in terms of time or is just restarting all the time<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:35:04)</span> <b>andrew.melo:</b></span> @bockjoo It looks like you could get it friday from amazon? <a href="https://www.amazon.com/Best-MEGARAID-SAS-9280-24I4E-SINGLE/dp/B00P8OFIXW">https://www.amazon.com/Best-MEGARAID-SAS-9280-24I4E-SINGLE/dp/B00P8OFIXW</a><br/>
<span style="color: #a72f79"><span style="font-size: small">(15:35:41)</span> <b>andrew.melo:</b></span> @justas.balcas I really appreciate the pointers .. I didn't realize there was a json output to gwmsmon<br/>
<span style="color: #de5f24"><span style="font-size: small">(15:36:52)</span> <b>justas.balcas:</b></span> :slightly_smiling_face: good luck playing around :wink:<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:45:06)</span> <b>andrew.melo:</b></span> interesting ... from a node with the pilot wrapper running but not showing up in gwms<br/><br/>
<span style="color: #a72f79"><span style="font-size: small">(15:46:28)</span> <b>andrew.melo:</b></span> <a href="https://gist.github.com/PerilousApricot/b2f987ce54dea78a6f80dbb0f326e211">https://gist.github.com/PerilousApricot/b2f987ce54dea78a6f80dbb0f326e211</a><br/>
<span style="color: #de5f24"><span style="font-size: small">(15:47:08)</span> <b>justas.balcas:</b></span> INFO   VALIDATION FAILED, No singularity but required (GLIDEIN_REQUIRED_OS == any)<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:47:25)</span> <b>andrew.melo:</b></span> but, I can run singularity and it appears to work<br/><pre>[root@ng686 cluster6903058.proc0.subproc0]# /usr/bin/singularity exec --home /tmp/glide_TzKwiC:/srv --bind /cvmfs --pwd /srv --contain --ipc --pid /cvmfs/singularity.opensciencegrid.org/cmssw/cms:rhel6 echo Hello World<br/>WARNING: container does not have /.singularity.d/actions/exec, calling echo directly<br/>Hello World</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(15:52:14)</span> <b>blin:</b></span> what version of singularity? this seem familiar<br/>
<span style="color: #43761b"><span style="font-size: small">(15:52:46)</span> <b>blin:</b></span> &gt;  WARNING: container does not have /.singularity.d/actions/exec, calling echo directly<br/>so is it calling singularity?<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:55:03)</span> <b>andrew.melo:</b></span> Hmm, I've not seen this one before<br/><pre>[root@ng686 glide_DLkY9t]# ls /cvmfs/cms.cern.ch/SITECONF/local/<br/>ls: cannot access /cvmfs/cms.cern.ch/SITECONF/local/: Transport endpoint is not connected</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(15:55:14)</span> <b>andrew.melo:</b></span> <pre>[root@ng686 glide_DLkY9t]# singularity --version<br/>singularity version 3.4.1-1.2.osg34.el7</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(15:59:25)</span> <b>blin:</b></span> @dwd does the <tt>.singularity</tt> warning above look familiar to you? i feel like this came up and was maybe addressed in singularity 3.5+?<br/>
<span style="color: #99a949"><span style="font-size: small">(16:01:02)</span> <b>dwd:</b></span> Thatâ€™s a cvmfs-only message, not singularity<br/>
<span style="color: #99a949"><span style="font-size: small">(16:02:04)</span> <b>dwd:</b></span> It means that the cvmfs2 watchdog process of the cvmfs2 fuse client process died without unmounting the repo like it is supposed to<br/>
<span style="color: #99a949"><span style="font-size: small">(16:02:52)</span> <b>dwd:</b></span> Although that particular path is a symlink, so it might actually point outside of cvmfs .. what is CMS_LOCAL_SITE set to?<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:03:44)</span> <b>andrew.melo:</b></span> In this case, FUSE actually died ..<br/><pre>[root@ng686 glide_DLkY9t]# ls /cvmfs/<br/>ls: cannot access /cvmfs/cms.cern.ch: Transport endpoint is not connected<br/><a href="http://cms.cern.ch">cms.cern.ch</a>  <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>  <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>  <a href="http://singularity.opensciencegrid.org">singularity.opensciencegrid.org</a><br/>[root@ng686 glide_DLkY9t]# umount /cvmfs/cms.cern.ch <br/>[root@ng686 glide_DLkY9t]# ls /cvmfs/<br/><a href="http://cms.cern.ch">cms.cern.ch</a>  <a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>  <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>  <a href="http://singularity.opensciencegrid.org">singularity.opensciencegrid.org</a><br/>[root@ng686 glide_DLkY9t]# umount -f /cvmfs/cms.cern.ch <br/>umount: /cvmfs/cms.cern.ch: not mounted<br/>[root@ng686 glide_DLkY9t]# ls /cvmfs/^C<br/>[root@ng686 glide_DLkY9t]# ls /cvmfs/cms.cern.ch/<br/>(correct values)</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(16:04:02)</span> <b>andrew.melo:</b></span> We point SITECONF/local to within CVMFS<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:04:23)</span> <b>andrew.melo:</b></span> We wrote a nagios check for this, but apparently I did it wrong :neutral_face:<br/>
<span style="color: #99a949"><span style="font-size: small">(16:11:55)</span> <b>dwd:</b></span> If you could find any messages in /var/log/messages about it dying that could be helpful.<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:16:05)</span> <b>andrew.melo:</b></span> let me see. if I can get anything useful<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:24:20)</span> <b>andrew.melo:</b></span> jackpot<br/><pre>[root@ng1008 ~]# ls /cvmfs<br/>ls: cannot access /cvmfs/singularity.opensciencegrid.org: Transport endpoint is not connected<br/>ls: cannot access /cvmfs/connect.opensciencegrid.org: Transport endpoint is not connected<br/>ls: cannot access /cvmfs/oasis.opensciencegrid.org: Transport endpoint is not connected<br/>ls: cannot access /cvmfs/cms.cern.ch: Transport endpoint is not connected<br/><a href="http://cms.cern.ch">cms.cern.ch</a>  <a href="http://connect.opensciencegrid.org">connect.opensciencegrid.org</a>  <a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>  <a href="http://singularity.opensciencegrid.org">singularity.opensciencegrid.org</a></pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(16:24:48)</span> <b>andrew.melo:</b></span> or not :disappointed:<br/><pre>[root@ng1008 ~]# grep cvmfs /var/log/messages<br/>Jan  5 03:12:32 ng1008 snmpd[5076]: Cannot statfs /cvmfs/cms.cern.ch#012: Transport endpoint is not connected<br/>Jan  5 03:12:32 ng1008 snmpd[5076]: Cannot statfs /cvmfs/oasis.opensciencegrid.org#012: Transport endpoint is not connected<br/>Jan  5 03:12:32 ng1008 snmpd[5076]: Cannot statfs /cvmfs/connect.opensciencegrid.org#012: Transport endpoint is not connected<br/>Jan  5 03:12:32 ng1008 snmpd[5076]: Cannot statfs /cvmfs/singularity.opensciencegrid.org#012: Transport endpoint is not connected</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(16:25:26)</span> <b>andrew.melo:</b></span> all the rotated logs just say that forever<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:26:37)</span> <b>andrew.melo:</b></span> journalctl -u cvmfs has a bit more. couple instances of<br/><pre>Sep 07 21:26:21 ng1008.vampire cvmfs2[12510]: (cms.cern.sh) geographic order of servers retrieved from <a href="http://cvmfs-s1goc.opensciencegrid.org">cvmfs-s1goc.opensciencegrid.org</a><br/>Sep 07 21:26:21 ng1008.vampire cvmfs2[12510]: (cms.cern.sh) switching host from <a href="http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.cern.sh">http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.cern.sh</a> to <a href="http://cvmfs-s1goc.opensciencegrid.org:8000/cvmfs/cms.c">http://cvmfs-s1goc.opensciencegrid.org:8000/cvmfs/cms.c</a><br/>Sep 07 21:26:21 ng1008.vampire cvmfs2[12510]: (cms.cern.sh) switching host from <a href="http://cvmfs-s1goc.opensciencegrid.org:8000/cvmfs/cms.cern.sh">http://cvmfs-s1goc.opensciencegrid.org:8000/cvmfs/cms.cern.sh</a> to <a href="http://cvmfs-s1bnl.opensciencegrid.org:8000/cvmfs/cms.ce">http://cvmfs-s1bnl.opensciencegrid.org:8000/cvmfs/cms.ce</a><br/>Sep 07 21:26:21 ng1008.vampire cvmfs2[12510]: (cms.cern.sh) failed to download repository manifest (9 - host returned HTTP error)<br/>Sep 07 21:26:21 ng1008.vampire cvmfs2[12510]: (cms.cern.sh) switching host from <a href="http://cvmfs-s1bnl.opensciencegrid.org:8000/cvmfs/cms.cern.sh">http://cvmfs-s1bnl.opensciencegrid.org:8000/cvmfs/cms.cern.sh</a> to <a href="http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.c">http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.c</a><br/>Sep 07 21:26:21 ng1008.vampire cvmfs2[12510]: (cms.cern.sh) switching host from <a href="http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.cern.sh">http://cvmfs-s1fnal.opensciencegrid.org:8000/cvmfs/cms.cern.sh</a> to <a href="http://cvmfs-s1goc.opensciencegrid.org:8000/cvmfs/cms.c">http://cvmfs-s1goc.opensciencegrid.org:8000/cvmfs/cms.c</a><br/>Sep 07 21:26:21 ng1008.vampire cvmfs2[12510]: (cms.cern.sh) failed to fetch file catalog at cms.cern.sh:/ (hash: 0000000000000000000000000000000000000000, error 9 [host returned HTTP error])</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(16:26:47)</span> <b>andrew.melo:</b></span> bit it looks like it recovers<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:27:41)</span> <b>andrew.melo:</b></span> (sorry, autofs is the service)<br/>
<span style="color: #99a949"><span style="font-size: small">(16:54:51)</span> <b>dwd:</b></span> Well maybe if you fix your nagios check you can catch one sooner when thereâ€™s still logs.  Is the cvmfs client a recent version?<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:56:36)</span> <b>andrew.melo:</b></span> Well, fortunately I have more than a handful of nodes doing this, so<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:43:47)</span> <b>justas.balcas:</b></span> @andrew.melo I have seen these errors. We have cronjob on all compute nodes, to check this every 15mins and report errors<br/><pre># First try to list it and try to umount.<br/>for d in <tt>ls /cvmfs/</tt>; do<br/>  OUT="$(ls -l /cvmfs/$d 2&gt;&amp;1 &gt; /dev/null)"<br/>  if echo "$OUT" | grep -q "Transport endpoint is not connected"; then<br/>    timestamp=$( date +%T )<br/>    umount /cvmfs/$d<br/>    publisherror $d<br/>  fi<br/>done</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(17:52:09)</span> <b>andrew.melo:</b></span> do you see it often?<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:52:20)</span> <b>justas.balcas:</b></span> I see two nodes are failing right now<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:52:23)</span> <b>justas.balcas:</b></span> <pre>Jan  7 15:47:31 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 281, CAS key e8ec3d88b62ebf526e4e5a4ff6162a3aa48a6b78, error code 2<br/>Jan  7 15:47:33 compute-11-19 cvmfs2: (<a href="http://oasis.opensciencegrid.org">oasis.opensciencegrid.org</a>) failed to load blacklist from config repository (15 - signature verification failure)<br/>Jan  7 15:47:33 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:33 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:34 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:36 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:38 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:39 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:41 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:43 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:44 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:45 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:47 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:47 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 289, CAS key ba6ff2e51cd01548412e78bc989547dadedeb539, error code 2<br/>Jan  7 15:47:49 compute-11-19 cvmfs2: re-building cache database<br/>Jan  7 15:47:49 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) failed to open inode: 281, CAS key e8ec3d88b62ebf526e4e5a4ff6162a3aa48a6b78, error code 2<br/>Jan  7 15:47:51 compute-11-19 cvmfs2: (<a href="http://singularity.opensciencegrid.org">singularity.opensciencegrid.org</a>) failed to load blacklist from config repository (15 - signature verification failure)<br/>Jan  7 15:48:09 compute-11-19 cvmfs2: (<a href="http://config-osg.opensciencegrid.org">config-osg.opensciencegrid.org</a>) recovered from offline mode</pre><br/>
<span style="color: #de5f24"><span style="font-size: small">(17:52:48)</span> <b>justas.balcas:</b></span> but usually it is once a week for 1 to 2 nodes<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:58:20)</span> <b>justas.balcas:</b></span> Once it fails badly and script does not fix, this does:<br/>  sudo cvmfs_config reload -c<br/>  sudo service autofs restart<br/>
<span style="color: #a72f79"><span style="font-size: small">(18:10:35)</span> <b>andrew.melo:</b></span> @dwd ^ looks like I"m not the only one?<br/>
</body>
</html>

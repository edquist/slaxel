<!DOCTYPE html>
<html>
<head>
<title>Mon Jul 12, 2021 : #uscms (osg)</title>
</head>
<body>
<h3>Mon Jul 12, 2021 : #uscms (osg)</h3>
<span style="color: #de5f24"><span style="font-size: small">(09:11:44)</span> <b>justas.balcas:</b></span> Did unmerged blow up at other sites too?<br/>
<span style="color: #d1707d"><span style="font-size: small">(09:34:55)</span> <b>gattebury:</b></span> I checked ours last night after the weekend emails and we still seem to be hovering ~187TB which is _within reason_ of what it’s been for months from my recollection. Higher than a year ago for sure, where ~100TB was the norm, but not “blowing up” like others it seems<br/>
<span style="color: #de5f24"><span style="font-size: small">(09:44:50)</span> <b>justas.balcas:</b></span> Somehow few sites are targeted for this only<br/>
<span style="color: #bc3663"><span style="font-size: small">(16:57:38)</span> <b>dmason:</b></span> Is anybody seeing evidence of cleanup jobs cleaning up anything other than log archives?  The way this was supposed to work is once files are merged, followup jobs would be launched that remove the unmerged files and keep the unmerged spaces clean.  Any centralized fiddling should have been a backup plan for things that were left untracked.<br/>
<span style="color: #bc3663"><span style="font-size: small">(16:58:45)</span> <b>dmason:</b></span> But for all I know they've changed this to some hokey centralized thing that either can't scale or only cleans up after a workflow is done (which uses more space)<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:02:44)</span> <b>hufnagel:</b></span> AFAIK cleanup jobs are still part of the agent workflows. Of course, cleanup jobs use their own code path in the stageout plugin code and cleanup failures are ignored, so 100% cleanup failure at a site is possible without anyone noticing for a while.<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:03:06)</span> <b>hufnagel:</b></span> In which case the only fallback would be that hokey centralized thing :slightly_smiling_face:<br/>
<span style="color: #bc3663"><span style="font-size: small">(17:05:13)</span> <b>dmason:</b></span> *Which depending on how implemented can cause unmerged to expand when they do things like shift around priorities and let unfinished workflows get buried by new ones?  Dunno can see a lot of ways to get here, especially if we're not tracking if the cleanups are cleaning up...*  <br/>
<span style="color: #235e5b"><span style="font-size: small">(17:07:10)</span> <b>hufnagel:</b></span> Cleanup jobs (like merge and other utility jobs) get a priority boost, it should be impossible for them to be held up by (processing) jobs from other high priority work.<br/>
<span style="color: #bc3663"><span style="font-size: small">(17:08:42)</span> <b>dmason:</b></span> Yes though not sure to what extent they follow priority anymore, especially since they let PPD set things like that.  If somebody put in things with a priority factor of a gazillion you'd never see merges or for that matter cleanups.<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:11:20)</span> <b>andrew.melo:</b></span> I guess the Q is: Are the cleanups not happening because of an error (i.e. not being scheduled) or because there are too many open workflows and the files are being legitimtely held on disk<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:14:59)</span> <b>justas.balcas:</b></span> Is there a way to get log files of cleanup jobs? (or they are just dumped)<br/>
<span style="color: #bc3663"><span style="font-size: small">(17:15:25)</span> <b>dmason:</b></span> might be hard if they're "successful"<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:19:19)</span> <b>hufnagel:</b></span> Cleanup jobs don't produce logArchive (no CMSSW), but they do produce the normal stdout/stderr like any other job. So one can look in the JobAccount or JobArchiver directories on the agent and find out what they were doing.<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:21:00)</span> <b>hufnagel:</b></span> But yes, I doubt these would show up anywhere in our monitoring for successful jobs. I think WMStats only include such stdout/stderr snippets for failed jobs.<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:22:01)</span> <b>hufnagel:</b></span> Which leaves digging through agent logs directly on the agent machine. Doable, but you would have to get a hold of one of the WM folks and preferably be able to point at a specific job.<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:23:24)</span> <b>hufnagel:</b></span> Another option would be to look in ElasticSearch, although of the top of my hat I don't know how you would query for cleanup jobs. I am sure they are in there though.<br/>
<span style="color: #bc3663"><span style="font-size: small">(17:24:34)</span> <b>dmason:</b></span> there is a jobtype -- am seeing there's a few dozen of those at caltech every day, but for all I can tell those are cleanups of log archives.  Poking around wmstats atm...<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:24:53)</span> <b>hufnagel:</b></span> CMS_JobType = Cleanup works<br/>
<span style="color: #bc3663"><span style="font-size: small">(17:25:02)</span> <b>dmason:</b></span> yeah<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:27:52)</span> <b>hufnagel:</b></span> Yeah, plenty of them at Caltech<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:28:22)</span> <b>hufnagel:</b></span> If you look at WMAgent_SubTaskName, you get an idea what type of files it's deleting (or trying to delete)<br/>For instance /pdmvserv_task_HIG-RunIISummer20UL18GEN-00001__v1_T_210624_141400_8650/HIG-RunIISummer20UL18GEN-00001_0/HIG-RunIISummer20UL18SIM-00144_0/HIG-RunIISummer20UL18SIM-00144_0CleanupUnmergedRAWSIMoutput<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:28:49)</span> <b>hufnagel:</b></span> So there do seem to be cleanup jobs running for MC output produced at Caltech<br/>
<span style="color: #bc3663"><span style="font-size: small">(17:28:58)</span> <b>dmason:</b></span> Or logfiles :wink:<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:29:25)</span> <b>hufnagel:</b></span> ExitCode 0 for that job, so it's (supposedly) working as well<br/>
<span style="color: #bc3663"><span style="font-size: small">(17:29:45)</span> <b>dmason:</b></span> yeah they've been 100% exit code zero<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:29:46)</span> <b>hufnagel:</b></span> No, not logfiles, that would be a different SubTaskName<br/>
<span style="color: #bc3663"><span style="font-size: small">(17:30:03)</span> <b>dmason:</b></span> ah OK.  yes RAWSIMoutput<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:30:15)</span> <b>hufnagel:</b></span> Actually, we don't delete logArchives in cleanup jobs.<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:30:25)</span> <b>hufnagel:</b></span> They get deleted directly from the LogCollect jobs<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:30:31)</span> <b>hufnagel:</b></span> As far as I remember...<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:32:56)</span> <b>hufnagel:</b></span> None of this completely rules out the cleanup internally not working for some reason, but for that you would have to look at an actual cleanup job stdout/stderr<br/>
<span style="color: #235e5b"><span style="font-size: small">(17:33:44)</span> <b>hufnagel:</b></span> i.e. whatever is in the removeFile part of the stageout plugin could return exit code 0 without actually deleting anything...<br/>
<span style="color: #bc3663"><span style="font-size: small">(17:54:02)</span> <b>dmason:</b></span> Um.  I'm not convinced the gfal2 plugin deletes anything actually.  Could it be this has been accumulating since switching to ceph?  And prior to that you were feeding it a pfn for a locally mounted hdfs filesystem?<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:55:05)</span> <b>justas.balcas:</b></span> <pre>oot<br/>INFO:root:===&gt; Attempting To Delete.<br/>INFO:root:LFN to PFN match made:<br/>LFN: /store/unmerged/RunIISummer20UL18wmLHEGEN/ST_t-channel_top_4f_InclusiveDecays_TuneCP5down_13TeV-powheg-madspin-pythia8/LHE/106X_upgrade2018_realistic_v4-v3/130005/FA5281DB-8F6D-4845-BCA6-9D356015C12F.root<br/>PFN: gsi<a href="ftp://transfer-lb.ultralight.org:2811/storage/cms//store/unmerged/RunIISummer20UL18wmLHEGEN/ST_t-channel_top_4f_InclusiveDecays_TuneCP5down_13TeV-powheg-madspin-pythia8/LHE/106X_upgrade2018_realistic_v4-v3/130005/FA5281DB-8F6D-4845-BCA6-9D356015C12F.root">ftp://transfer-lb.ultralight.org:2811/storage/cms//store/unmerged/RunIISummer20UL18wmLHEGEN/ST_t-channel_top_4f_InclusiveDecays_TuneCP5down_13TeV-powheg-madspin-pythia8/LHE/106X_upgrade2018_realistic_v4-v3/130005/FA5281DB-8F6D-4845-BCA6-9D356015C12F.root</a><br/><br/>INFO:root:Command exited with status: 0<br/>Output message: stdout: Mon Jul 12 22:19:23 UTC 2021<br/>gsi<a href="ftp://transfer-lb.ultralight.org:2811/storage/cms//store/unmerged/RunIISummer20UL18wmLHEGEN/ST_t-channel_top_4f_InclusiveDecays_TuneCP5down_13TeV-powheg-madspin-pythia8/LHE/106X_upgrade2018_realistic_v4-v3/130005/FA5281DB-8F6D-4845-BCA6-9D356015C12F.root">ftp://transfer-lb.ultralight.org:2811/storage/cms//store/unmerged/RunIISummer20UL18wmLHEGEN/ST_t-channel_top_4f_InclusiveDecays_TuneCP5down_13TeV-powheg-madspin-pythia8/LHE/106X_upgrade2018_realistic_v4-v3/130005/FA5281DB-8F6D-4845-BCA6-9D356015C12F.root</a>       DELETED<br/><br/>stderr: /srv/startup_environment.sh: line 2: BASHOPTS: readonly variable<br/>/srv/startup_environment.sh: line 9: BASH_VERSINFO: readonly variable<br/>/srv/startup_environment.sh: line 20: EUID: readonly variable<br/>/srv/startup_environment.sh: line 126: PPID: readonly variable<br/>/srv/startup_environment.sh: line 133: SHELLOPTS: readonly variable<br/>/srv/startup_environment.sh: line 147: UID: readonly variable<br/><br/>INFO:root:===&gt; Delete Successful:<br/>====&gt; LFN: /store/unmerged/RunIISummer20UL18wmLHEGEN/ST_t-channel_top_4f_InclusiveDecays_TuneCP5down_13TeV-powheg-madspin-pythia8/LHE/106X_upgrade2018_realistic_v4-v3/130005/FA5281DB-8F6D-4845-BCA6-9D356015C12F.root<br/>====&gt; PFN: gsi<a href="ftp://transfer-lb.ultralight.org:2811/storage/cms//store/unmerged/RunIISummer20UL18wmLHEGEN/ST_t-channel_top_4f_InclusiveDecays_TuneCP5down_13TeV-powheg-madspin-pythia8/LHE/106X_upgrade2018_realistic_v4-v3/130005/FA5281DB-8F6D-4845-BCA6-9D356015C12F.root">ftp://transfer-lb.ultralight.org:2811/storage/cms//store/unmerged/RunIISummer20UL18wmLHEGEN/ST_t-channel_top_4f_InclusiveDecays_TuneCP5down_13TeV-powheg-madspin-pythia8/LHE/106X_upgrade2018_realistic_v4-v3/130005/FA5281DB-8F6D-4845-BCA6-9D356015C12F.root</a><br/>====&gt; PNN:  T2_US_Caltech_Ceph</pre><br/>
<span style="color: #de5f24"><span style="font-size: small">(17:55:39)</span> <b>justas.balcas:</b></span> So wmagent keeps deleting (we can rule this out)<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:56:53)</span> <b>justas.balcas:</b></span> I just got 1 log from prod Cleanup - and it goes to correct PFN to delete file and also files are deleted<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:56:57)</span> <b>justas.balcas:</b></span> from storage itself<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:58:09)</span> <b>justas.balcas:</b></span> <br/>
<span style="color: #bc3663"><span style="font-size: small">(18:00:41)</span> <b>dmason:</b></span> You checked that the file was actually deleted?<br/>
<span style="color: #de5f24"><span style="font-size: small">(18:00:45)</span> <b>justas.balcas:</b></span> yep<br/>
<span style="color: #bc3663"><span style="font-size: small">(18:02:50)</span> <b>dmason:</b></span> when was the directory made?<br/>
<span style="color: #de5f24"><span style="font-size: small">(18:03:57)</span> <b>justas.balcas:</b></span> This job ran:<br/><pre>[jbalcas@vocms0283 job_2635847]$ condor_history 23064.62<br/> ID     OWNER          SUBMITTED   RUN_TIME     ST COMPLETED   CMD<br/>23064.62  cmst1           7/13 00:17   0+00:00:43 C   7/13 00:19 /data/srv/wmagent/v1.4.7.patch2/sw/slc<br/>^C</pre><br/>
<span style="color: #de5f24"><span style="font-size: small">(18:04:05)</span> <b>justas.balcas:</b></span> 1hr ago?<br/>
<span style="color: #de5f24"><span style="font-size: small">(18:04:41)</span> <b>justas.balcas:</b></span> Dir created: Jul 12 15:34<br/>
<span style="color: #de5f24"><span style="font-size: small">(18:04:51)</span> <b>justas.balcas:</b></span> sorry - accessed, not created<br/>
<span style="color: #bc3663"><span style="font-size: small">(18:05:26)</span> <b>dmason:</b></span> • *pdmvserv_task_TOP-RunIISummer20UL18wmLHEGEN-00117__v1_T_210628_184621_1156*<br/><br/>
<span style="color: #bc3663"><span style="font-size: small">(18:05:44)</span> <b>dmason:</b></span> workflow from June 28...<br/>
<span style="color: #de5f24"><span style="font-size: small">(18:05:49)</span> <b>justas.balcas:</b></span> <pre>stat /storage/cms//store/unmerged/RunIISummer20UL18wmLHEGEN/ST_t-channel_top_4f_InclusiveDecays_TuneCP5down_13TeV-powheg-madspin-pythia8/LHE/106X_upgrade2018_realistic_v4-v3/130005/<br/>  File: '/storage/cms//store/unmerged/RunIISummer20UL18wmLHEGEN/ST_t-channel_top_4f_InclusiveDecays_TuneCP5down_13TeV-powheg-madspin-pythia8/LHE/106X_upgrade2018_realistic_v4-v3/130005/'<br/>  Size: 5         	Blocks: 0          IO Block: 65536  directory<br/>Device: 0h/0d	Inode: 1099521600409  Links: 1<br/>Access: (0755/drwxr-xr-x)  Uid: ( 2007/ cmsprod)   Gid: ( 2007/ cmsprod)<br/>Access: 2021-07-12 02:24:31.560306066 -0700<br/>Modify: 2021-07-12 15:34:05.215774088 -0700<br/>Change: 2021-07-12 15:34:05.215774088 -0700<br/> Birth: -</pre><br/>
<span style="color: #de5f24"><span style="font-size: small">(18:06:55)</span> <b>justas.balcas:</b></span> I could grep and find out whenever unmerged was written (we know when it was deleted)<br/>
<span style="color: #de5f24"><span style="font-size: small">(18:07:08)</span> <b>justas.balcas:</b></span> But - I dont think there is an issue here<br/>
<span style="color: #bc3663"><span style="font-size: small">(18:08:43)</span> <b>dmason:</b></span> So in principle this workflow has been running for the last 2 weeks.<br/>
<span style="color: #bc3663"><span style="font-size: small">(18:08:58)</span> <b>dmason:</b></span> switched to running-open in 6/29<br/>
<span style="color: #de5f24"><span style="font-size: small">(18:15:00)</span> <b>justas.balcas:</b></span> Woohoo - we will go down sooner :smile:<br/><pre># ceph -s<br/>  cluster:<br/>    id:     12d9d70a-e993-464c-a6f8-4f674db35136<br/>    health: HEALTH_WARN<br/>            1 backfillfull osd(s)<br/>            2 pool(s) backfillfull</pre><br/>
</body>
</html>

<!DOCTYPE html>
<html>
<head>
<title>Tue Feb 23, 2021 : #uscms (osg)</title>
</head>
<body>
<h3>Tue Feb 23, 2021 : #uscms (osg)</h3>
<span style="color: #43761b"><span style="font-size: small">(09:06:12)</span> <b>blin:</b></span> hey @bockjoo @justas.balcas IIRC, you guys are using custom config for WholeNode GPU jobs, right?<br/>
<span style="color: #de5f24"><span style="font-size: small">(09:06:27)</span> <b>justas.balcas:</b></span> I do<br/>
<span style="color: #43761b"><span style="font-size: small">(09:12:32)</span> <b>blin:</b></span> would you be able to help test htcondor-ce-4.5.0? it adds that + a few other things<br/>
<span style="color: #43761b"><span style="font-size: small">(09:12:55)</span> <b>blin:</b></span> <a href="https://github.com/htcondor/htcondor-ce/releases/tag/v4.5.0">https://github.com/htcondor/htcondor-ce/releases/tag/v4.5.0</a><br/>
<span style="color: #de5f24"><span style="font-size: small">(09:13:04)</span> <b>justas.balcas:</b></span> can I see the change? I can do this on one of CEs<br/>
<span style="color: #de5f24"><span style="font-size: small">(09:13:30)</span> <b>justas.balcas:</b></span> is it on repo?<br/>
<span style="color: #43761b"><span style="font-size: small">(09:13:33)</span> <b>blin:</b></span> the RPM is available in <tt>osg-testing</tt><br/>
<span style="color: #43761b"><span style="font-size: small">(09:13:54)</span> <b>blin:</b></span> beware: if you are getting condor from upcoming, we have htcondor-ce 5.0.0 in <tt>osg-upcoming-testing</tt><br/>
<span style="color: #de5f24"><span style="font-size: small">(09:15:21)</span> <b>justas.balcas:</b></span> <pre>Dependencies Resolved<br/><br/>=================================================================================================================================<br/> Package                            Arch                   Version                             Repository                   Size<br/>=================================================================================================================================<br/>Updating:<br/> htcondor-ce                        noarch                 4.5.0-1.osg35.el7                   osg-testing                  49 k<br/>Installing for dependencies:<br/> python3                            x86_64                 3.6.8-18.el7                        updates                      70 k<br/> python3-libs                       x86_64                 3.6.8-18.el7                        updates                     6.9 M<br/> python3-pip                        noarch                 9.0.3-8.el7                         base                        1.6 M<br/> python3-setuptools                 noarch                 39.2.0-10.el7                       base                        629 k<br/>Updating for dependencies:<br/> htcondor-ce-client                 noarch                 4.5.0-1.osg35.el7                   osg-testing                  52 k<br/> htcondor-ce-condor                 noarch                 4.5.0-1.osg35.el7                   osg-testing                  25 k<br/><br/>Transaction Summary<br/>=================================================================================================================================<br/>Install             ( 4 Dependent packages)<br/>Upgrade  1 Package  (+2 Dependent packages)</pre><br/>
<span style="color: #de5f24"><span style="font-size: small">(09:22:14)</span> <b>justas.balcas:</b></span> Updated, submitted GPU job to CE with WantWholeNode=true, gpus=1; Seems all ok.  Script: <a href="https://login-1.hep.caltech.edu/~jbalcas/ce-gpu.sh">https://login-1.hep.caltech.edu/~jbalcas/ce-gpu.sh</a> and log: <a href="https://login-1.hep.caltech.edu/~jbalcas/ce-gpu-4.5.0.out">https://login-1.hep.caltech.edu/~jbalcas/ce-gpu-4.5.0.out</a><br/>
<span style="color: #de5f24"><span style="font-size: small">(09:28:13)</span> <b>justas.balcas:</b></span> and job route defaults dump:<br/><br/>
<span style="color: #de5f24"><span style="font-size: small">(09:28:13)</span> <b>justas.balcas:</b></span> <pre># condor_ce_config_val JOB_ROUTER_DEFAULTS | sed 's/;/;\n/g'<br/>[ MaxIdleJobs = 2000;<br/><br/>MaxJobs = 10000;<br/><br/>/* by default, accept all jobs */<br/>Requirements = True;<br/><br/>/* now modify routed job attributes */<br/>/* remove routed job if the client disappears for 48 hours or it is idle for 6 */<br/>/*set_PeriodicRemove = (LastClientContact - time() &gt; 48*60*60) ||<br/>(JobStatus == 1 &amp;&amp; (time() - QDate) &gt; 6*60);<br/>*/<br/>delete_PeriodicRemove = true;<br/><br/>delete_CondorCE = true;<br/><br/>delete_TotalSubmitProcs = true;<br/><br/>set_RoutedJob = true;<br/><br/>/* Set the environment */<br/>copy_environment = "orig_environment";<br/><br/>set_osg_environment = "OSG_GRID='/etc/osg/wn-client/' OSG_SQUID_LOCATION='10.3.11.14:3128' OSG_SITE_READ='/mnt/hadoop/osg' OSG_HOSTNAME='<a href="http://cit-gatekeeper.ultralight.org">cit-gatekeeper.ultralight.org</a>' OSG_DATA='UNAVAILABLE' GLOBUS_LOCATION='/usr' OSG_STORAGE_ELEMENT='True' OSG_SITE_NAME='CIT_CMS_T2' OSG_WN_TMP='/wntmp' PATH='/bin:/usr/bin:/sbin:/usr/sbin' OSG_DEFAULT_SE='<a href="http://transfer.ultralight.org">transfer.ultralight.org</a>' OSG_SITE_WRITE='/mnt/hadoop/osg'";<br/><br/>eval_set_environment = mergeEnvironment(join(" ",<br/>True =?= True ?<br/>strcat("HOME=", userHome(Owner, "/")) :<br/>"",<br/>ifThenElse(False =?= True, "", strcat("CONDORCE_COLLECTOR_HOST=", "<a href="http://cit-gatekeeper.ultralight.org:9619">cit-gatekeeper.ultralight.org:9619</a>"))),<br/>osg_environment,<br/>orig_environment,<br/>"",<br/>default_pilot_job_env);<br/><br/>/* Set new requirements */<br/>/* set_requirements = LastClientContact - time() &lt; 30*60;<br/> */<br/>set_requirements = True;<br/><br/>/* Note default memory request of 2GB */<br/>/* Note yet another nested condition allow pass attributes (maxMemory,xcount,jobtype,queue)<br/>via gWMS Factory described within ClassAd */<br/>eval_set_OriginalMemory = ifThenElse(maxMemory isnt undefined,<br/>maxMemory,<br/>ifThenElse(default_maxMemory isnt undefined,<br/>default_maxMemory,<br/>2000));<br/><br/>/* Duplicate OriginalMemory expression and add remote_ prefix.<br/>This passes the attribute from gridmanager to BLAHP. */<br/>eval_set_remote_OriginalMemory = ifThenElse(maxMemory isnt undefined,<br/>maxMemory,<br/>ifThenElse(default_maxMemory isnt undefined,<br/>default_maxMemory,<br/>2000));<br/><br/>set_JOB_GLIDEIN_Memory = "$$(TotalMemory:0)";<br/><br/>set_JobMemory = JobIsRunning ? int(MATCH_EXP_JOB_GLIDEIN_Memory)*95/100 : OriginalMemory;<br/><br/>set_RequestMemory = ifThenElse(WantWholeNode is true,<br/>!isUndefined(TotalMemory) ? TotalMemory*95/100 : JobMemory,<br/>OriginalMemory);<br/><br/>eval_set_remote_queue = ifThenElse(batch_queue isnt undefined,<br/>batch_queue,<br/>ifThenElse(queue isnt undefined,<br/>queue,<br/>ifThenElse(default_queue isnt undefined,<br/>default_queue,<br/>"")));<br/><br/>/* Request GPUs for whole node jobs (HTCONDOR-103) */<br/>/* If a whole node job requests GPUs and is matched to a machine with GPUs then set the job's RequestGPUs to all the GPUs on that machine */<br/>copy_RequestGPUs = "orig_RequestGPUs";<br/><br/>eval_set_OriginalGPUs = orig_RequestGPUs;<br/><br/>/* MATCH_EXP_JOB_GLIDEIN_GPUs will be based on JOB_GLIDEIN_GPUs (set below) once the routed job is matched to an HTCondor slot */<br/>set_GlideinGPUsIsGood = !isUndefined(MATCH_EXP_JOB_GLIDEIN_GPUs) &amp;&amp; (int(MATCH_EXP_JOB_GLIDEIN_GPUs) isnt error);<br/><br/>/* JobGPUs set below;<br/> TotalGPUs comes from the slot ad, WantWholeNode from the job ad */<br/>set_JOB_GLIDEIN_GPUs = "$$(ifThenElse(WantWholeNode is true, !isUndefined(TotalGPUs) ? TotalGPUs : JobGPUs, OriginalGPUs))";<br/><br/>set_JobGPUs = JobIsRunning ? int(MATCH_EXP_JOB_GLIDEIN_GPUs) : OriginalGPUs;<br/><br/>set_RequestGPUs = ifThenElse((WantWholeNode is true &amp;&amp; OriginalGPUs isnt undefined),<br/>(!isUndefined(TotalGPUs) &amp;&amp; TotalGPUs &gt; 0)? TotalGPUs : JobGPUs,<br/>OriginalGPUs);<br/><br/>/* HTCondor uses RequestCpus;<br/> blahp uses SMPGranularity and NodeNumber.  Default is 1 core. */<br/>copy_RequestCpus = "orig_RequestCpus";<br/><br/>eval_set_OriginalCpus = ifThenElse(xcount isnt undefined,<br/>xcount,<br/>ifThenElse(orig_RequestCpus isnt undefined,<br/>ifThenElse(orig_RequestCpus &gt; 1,<br/>orig_RequestCpus,<br/>ifThenElse(default_xcount isnt undefined,<br/>default_xcount,<br/>1)),<br/>ifThenElse(default_xcount isnt undefined,<br/>default_xcount,<br/>1)));<br/><br/>set_GlideinCpusIsGood = !isUndefined(MATCH_EXP_JOB_GLIDEIN_Cpus) &amp;&amp; (int(MATCH_EXP_JOB_GLIDEIN_Cpus) isnt error);<br/><br/>set_JOB_GLIDEIN_Cpus = "$$(ifThenElse(WantWholeNode is true, !isUndefined(TotalCpus) ? TotalCpus : JobCpus, OriginalCpus))";<br/><br/>set_JobIsRunning = (JobStatus =!= 1) &amp;&amp; (JobStatus =!= 5) &amp;&amp; GlideinCpusIsGood;<br/><br/>set_JobCpus = JobIsRunning ? int(MATCH_EXP_JOB_GLIDEIN_Cpus) : OriginalCpus;<br/><br/>set_RequestCpus = ifThenElse(WantWholeNode is true,<br/>!isUndefined(TotalCpus) ? TotalCpus : JobCpus,<br/>OriginalCpus);<br/><br/>eval_set_remote_SMPGranularity = ifThenElse(xcount isnt undefined,<br/>xcount,<br/>ifThenElse(default_xcount isnt undefined,<br/>default_xcount,<br/>1));<br/><br/>eval_set_remote_NodeNumber = ifThenElse(xcount isnt undefined,<br/>xcount,<br/>ifThenElse(default_xcount isnt undefined,<br/>default_xcount,<br/>1));<br/><br/>set_CondorCE = 1;<br/><br/>/* WallTime is in seconds but users configure default_maxWallTime and ROUTED_JOB_MAX_TIME in minutes */<br/>set_WallTime = ifThenElse(maxWallTime isnt undefined,<br/>60*maxWallTime,<br/>ifThenElse(default_maxWallTime isnt undefined,<br/>60*default_maxWallTime,<br/>60*2880));<br/><br/>eval_set_CERequirements = ifThenElse(default_CERequirements isnt undefined,<br/>strcat(default_CERequirements, ",Walltime,CondorCE"),<br/>"Walltime,CondorCE");<br/><br/>copy_OnExitHold = "orig_OnExitHold";<br/><br/>set_OnExitHold = ifThenElse(orig_OnExitHold isnt undefined,<br/>orig_OnExitHold,<br/>false) ||<br/>ifThenElse(minWalltime isnt undefined &amp;&amp; RemoteWallClockTime isnt undefined,<br/>RemoteWallClockTime &lt; 60*minWallTime,<br/>false);<br/><br/>copy_OnExitHoldReason = "orig_OnExitHoldReason";<br/><br/>set_OnExitHoldReason = ifThenElse((orig_OnExitHold isnt undefined) &amp;&amp; orig_OnExitHold,<br/>ifThenElse(orig_OnExitHoldReason isnt undefined,<br/>orig_OnExitHoldReason,<br/>strcat("The on_exit_hold expression (",<br/>unparse(orig_OnExitHold),<br/>") evaluated to TRUE.")),<br/>ifThenElse(minWalltime isnt undefined &amp;&amp;<br/>RemoteWallClockTime isnt undefined &amp;&amp;<br/>(RemoteWallClockTime &lt; 60*minWallTime),<br/>strcat("The job's wall clock time, ",<br/>int(RemoteWallClockTime/60),<br/>"min, is less than the minimum specified by the job (",<br/>minWalltime,<br/>")"),<br/>"Job held for unknown reason."));<br/><br/>copy_OnExitHoldSubCode = "orig_OnExitHoldSubCode";<br/><br/>set_OnExitHoldSubCode = ifThenElse((orig_OnExitHold isnt undefined) &amp;&amp; orig_OnExitHold,<br/>ifThenElse(orig_OnExitHoldSubCode isnt undefined,<br/>orig_OnExitHoldSubCode,<br/>1),<br/>42);<br/><br/>set_AccountingGroupOSG = ifThenElse(Owner == "osg", strcat("osg_user", ".", Owner), ifThenElse(Owner == "sbgrid", strcat("osg_user", ".", Owner), ifThenElse(Owner == "glow", strcat("osg_user", ".", Owner), ifThenElse(Owner == "uscms0495", strcat("cms_monitor", ".", Owner), ifThenElse(Owner == "cmsprod100", strcat("cms_monitor", ".", Owner), ifThenElse(Owner == "cmslocal", strcat("cms_user", ".", Owner), ifThenElse(Owner == "cmspilot", strcat("cms_user", ".", Owner), ifThenElse(Owner == "cmsprod", strcat("cms_monitor", ".", Owner), ifThenElse(Owner == "lcgadmin", strcat("cms_monitor", ".", Owner), ifThenElse(Owner == "cmsuser", strcat("cms_user", ".", Owner), ifThenElse(regexp("cms\\/Role=production", x509UserProxySubject), strcat("cms_production", ".", Owner), ifThenElse(x509UserProxyFirstFQAN isnt Undefined &amp;&amp; regexp("cms\\/Role=production", x509UserProxyFirstFQAN), strcat("cms_production", ".", Owner), ifThenElse(regexp("cms.*", x509UserProxySubject), strcat("cms_user", ".", Owner), ifThenElse(x509UserProxyFirstFQAN isnt Undefined &amp;&amp; regexp("cms.*", x509UserProxyFirstFQAN), strcat("cms_user", ".", Owner), Owner))))))))))))));<br/><br/>eval_set_AccountingGroup = AccountingGroupOSG;<br/><br/>] [set_RequestWalltime = 345600;<br/> set_RunAsOwner = True]</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(09:29:09)</span> <b>blin:</b></span> great, thanks!!<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:29:13)</span> <b>andrew.melo:</b></span> I have trouble reading condor-ese. What's the purpose of that config?<br/>
<span style="color: #43761b"><span style="font-size: small">(10:36:54)</span> <b>blin:</b></span> the CE supports pilots with <tt>WholeNodeJobs = True</tt>  to request whole nodes from a condor local batch<br/>
<span style="color: #43761b"><span style="font-size: small">(10:37:26)</span> <b>blin:</b></span> the config we've added, thanks to @justas.balcas really, makes sure that we also request and make use of any GPUs on the nodes requested this way<br/>
<span style="color: #7d414c"><span style="font-size: small">(10:59:40)</span> <b>bockjoo:</b></span> @blin We don't do wholenode at Florida. Is the Justas test enough for you?<br/>
<span style="color: #43761b"><span style="font-size: small">(11:00:45)</span> <b>blin:</b></span> Justas' test is sufficient for us<br/>
<span style="color: #7d414c"><span style="font-size: small">(11:29:07)</span> <b>bockjoo:</b></span> @justas.balcas Did you transfer all the PhEDEx local subscriptions in the slide #5 of <a href="https://indico.cern.ch/event/996183/contributions/4185935/attachments/2173397/3669521/Managing%20local%20data%20at%20sites%20with%20Rucio.pdf">https://indico.cern.ch/event/996183/contributions/4185935/attachments/2173397/3669521/Managing%20local%20data%20at%20sites%20with%20Rucio.pdf</a> using the rucio quota?<br/>
<span style="color: #de5f24"><span style="font-size: small">(11:30:38)</span> <b>justas.balcas:</b></span> what you mean by using rucio quota?<br/>
<span style="color: #de5f24"><span style="font-size: small">(11:32:10)</span> <b>justas.balcas:</b></span> we copied all from phedex local to rucio account &lt;sitename&gt;_local_users; rucio account &lt;sitename&gt;_local_users has it’s own quota set of 1PB.<br/>
<span style="color: #7d414c"><span style="font-size: small">(11:36:28)</span> <b>bockjoo:</b></span> That answers my question. Thanks!<br/>
<span style="color: #7d414c"><span style="font-size: small">(12:18:31)</span> <b>bockjoo:</b></span> Did you have to add only one rule to migrate from the PhEDEx local to the rucio account t2 local?<br/>
<span style="color: #7d414c"><span style="font-size: small">(12:19:15)</span> <b>bockjoo:</b></span> Will that be sufficient to trigger the other blocks to be moved to the rucio account?<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:27:54)</span> <b>justas.balcas:</b></span> Slide 5 - you can run this script and find all blocks subscribed under phedex local group; Once you get full list, you can use <tt>export RUCIO_ACCOUNT=&lt;sitename&gt;_local_users rucio add-rule ….</tt> or modify script from slide 5 and do add_replication_rule<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:28:22)</span> <b>justas.balcas:</b></span> Once I did this - I asked Transfer team to clean all local rules from Phedex local<br/>
<span style="color: #7d414c"><span style="font-size: small">(12:38:41)</span> <b>bockjoo:</b></span> I can see rucio list-account-usage t2_us_florida_local_users using RUCIO_ACCOUNT=t2_us_florida_local_users , but I can not see it using RUCIO_ACCOUNT=bockjoo :<br/>
<span style="color: #7d414c"><span style="font-size: small">(12:38:48)</span> <b>bockjoo:</b></span> <pre>2021-02-23 13:27:23,416	ERROR	Access to the requested resource denied.<br/>Details: Account bockjoo can not list account usage.</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(12:39:39)</span> <b>bockjoo:</b></span> Can you see the usage using either RUCIO_ACCOUNT?<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:39:57)</span> <b>justas.balcas:</b></span> rucio list-rse-usage T2_US_Florida --show-accounts ?<br/>
<span style="color: #7d414c"><span style="font-size: small">(12:41:36)</span> <b>bockjoo:</b></span> That command runs fine with either of RUCIO_ACCOUNT.<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:41:39)</span> <b>justas.balcas:</b></span> but there is permissions set there - I complained at some poinnt about this<br/>
<span style="color: #7d414c"><span style="font-size: small">(12:42:45)</span> <b>bockjoo:</b></span> I will ask the Rucio team to check it. Thanks!<br/>
<span style="color: #de5f24"><span style="font-size: small">(12:43:52)</span> <b>justas.balcas:</b></span> There is rucio cms slack :wink: I bug them there ;))<br/>
<span style="color: #7d414c"><span style="font-size: small">(12:50:28)</span> <b>bockjoo:</b></span> Do you have the link for the rucio cms slack?<br/>
<span style="color: #de5f24"><span style="font-size: small">(13:46:24)</span> <b>justas.balcas:</b></span> Eric V invited me - ping him<br/>
</body>
</html>

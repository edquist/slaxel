<!DOCTYPE html>
<html>
<head>
<title>Tue Mar 30, 2021 : #uscms (osg)</title>
</head>
<body>
<h3>Tue Mar 30, 2021 : #uscms (osg)</h3>
<span style="color: #7d414c"><span style="font-size: small">(13:47:57)</span> <b>bockjoo:</b></span> Worker nodes are communicating outside through NAT. How do I count the number of NAT address consumed by worker nodes?<br/>
<span style="color: #d1707d"><span style="font-size: small">(14:01:20)</span> <b>gattebury:</b></span> “addresses consumed” is entirely dependent on how the NAT is implemented and how many stateful connections are being tracked. With some hand waving, each connection needs at least one port per “address”, and being limited to the standard ~64k ports (even fewer in reality, maybe say ~62k at an absolute max per protocol per NAT device) you could determine how many addresses are needed.<br/><br/>Hard to put a fine number on it with bursts and all that, but you can ballpark it.<br/>
<span style="color: #d1707d"><span style="font-size: small">(14:02:47)</span> <b>gattebury:</b></span> Counting connections can be done in many ways, like <tt>netstat</tt> or <tt>ss -s</tt> or something.<br/>
<span style="color: #d1707d"><span style="font-size: small">(14:03:59)</span> <b>gattebury:</b></span> I can’t help but feel this is something those administering the NAT devices should be determining and providing — that’s where the definitive source of # of connections from what hosts lives.<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:09:26)</span> <b>bockjoo:</b></span> I used ss -rp and counted fnal and cern connections. The maximum number of connections per worker node is 84. Total connections are 11583 spread over 450 worker nodes. My network engineer told me we have 1270 addresses. We see NAT addresses are exhausted from time to time. So, I guess this means there are jobs running across 1270 worker nodes, but CMS can not run on that many worker nodes. So, I must be mis-counting something.<br/>
<span style="color: #a72f79"><span style="font-size: small">(14:10:02)</span> <b>andrew.melo:</b></span> Your network engineer can presumably look at the connections tables and see the source/destination IP/ports<br/>
<span style="color: #a72f79"><span style="font-size: small">(14:10:10)</span> <b>andrew.melo:</b></span> But, it sounds like you need a bigger NAT :slightly_smiling_face:<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:11:50)</span> <b>bockjoo:</b></span> Yeah, but I want to know why this is happening recently. As a solution, we will have to add the IPv6 capability on worker nodes.<br/>
<span style="color: #d1707d"><span style="font-size: small">(14:18:19)</span> <b>gattebury:</b></span> I’d guess your network engineer is using a different meaning of address, or your NAT devices are really limiting for some reason. 11k+ connections on a single NAT device with a single public facing address is certainly heavily loaded but it’s possible. Bigger (or more efficient) NAT is the easy solution of course.<br/><br/>As for why it’s happening recently, I’d be curious to investigate that as well - change in workflow or more reliance on things doing lots of connections (xrootd!) might be the easy answers there.<br/>
<span style="color: #a72f79"><span style="font-size: small">(14:21:59)</span> <b>andrew.melo:</b></span> Well, since a lot of our (USCMS') focus is on remote reads via data lakes/caches/etc..., the trend for external connectivity is only going to increase<br/>
<span style="color: #a72f79"><span style="font-size: small">(14:22:24)</span> <b>andrew.melo:</b></span> I'd first want to see that the outbound connections are to port 1094 before I jumped to conclusions<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:31:07)</span> <b>bockjoo:</b></span> @gattebury<br/><pre>we only care about source information since we are not doing <br/>PAT.  Thus if we could track the node count, that would be good.</pre><br/>says the lead network engineer.<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:33:15)</span> <b>bockjoo:</b></span> 1094 5<br/>4080 9<br/>9615 2<br/>9618 27<br/>9629 1<br/>9630 1<br/>9631 1<br/>9635 1<br/>9637 2<br/>9655 1<br/>9657 1<br/>9664 1<br/>9670 1<br/>9672 1<br/>9675 2<br/>9680 1<br/>9700 1<br/>9709 2<br/>9718 2<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:33:33)</span> <b>bockjoo:</b></span> port count in one worker node.<br/>
<span style="color: #7d414c"><span style="font-size: small">(14:55:54)</span> <b>bockjoo:</b></span> <pre>4080 is CRAB, 9615 and 9618 are gideinwms. I don't know what these are:</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(14:56:06)</span> <b>bockjoo:</b></span> <pre>9629 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9630 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9631 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9635 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9637 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9655 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9657 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9661 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9664 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9670 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9675 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9680 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9700 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9709 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a><br/>9718 <a href="http://cmssrv601.fnal.gov">cmssrv601.fnal.gov</a></pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(14:56:32)</span> <b>bockjoo:</b></span> @dmason should know I guess.<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:03:48)</span> <b>andrew.melo:</b></span> CMS has a lot of hosts :slightly_smiling_face:<br/>
<span style="color: #bc3663"><span style="font-size: small">(15:05:57)</span> <b>dmason:</b></span> well its a condor collector at least -- let me find out which one :slightly_smiling_face:<br/>
<span style="color: #de5f24"><span style="font-size: small">(15:05:59)</span> <b>justas.balcas:</b></span> @bockjoo dont know your NAT or what you run - but one of the things we installed was netdata - that kind of allowed us to identify what is the issue of NAT. (Due to other reasons we gave up on NAT and went all public)<br/>
<span style="color: #bc3663"><span style="font-size: small">(15:07:14)</span> <b>dmason:</b></span> here is a pretty big clue:  "Puppet Role: gwms_ccb/cms_global_ccb"<br/>
<span style="color: #7d414c"><span style="font-size: small">(15:32:19)</span> <b>bockjoo:</b></span> Thanks @dmason<br/>
<span style="color: #7d414c"><span style="font-size: small">(15:33:44)</span> <b>bockjoo:</b></span> @justas.balcas We know the issue, lack of addresses, but don't know what's causing it.<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:39:08)</span> <b>andrew.melo:</b></span> I'm not sure there's much of a solution outside of "get more addresses" though. It's not reasonable to expect CMS jobs to simply not communicate with remote hosts.<br/>
<span style="color: #de5f24"><span style="font-size: small">(15:52:12)</span> <b>justas.balcas:</b></span> looking at all cms jobs:<br/> ‘T2_US_Caltech’: {‘localdata’: 1561,<br/>                   ‘remotedata’: 980,<br/>                   ‘siteindesiredlist’: 1899,<br/>                   ‘sitenotindesiredlist’: 642},<br/> ‘T2_US_Florida’: {‘localdata’: 1192,<br/>                   ‘remotedata’: 146,<br/>                   ‘siteindesiredlist’: 1338,<br/>                   ‘sitenotindesiredlist’: 0},<br/> ‘T2_US_MIT’: {‘localdata’: 2853,<br/>               ‘remotedata’: 134,<br/>               ‘siteindesiredlist’: 2987,<br/>               ‘sitenotindesiredlist’: 0},<br/> ‘T2_US_Nebraska’: {‘localdata’: 1461,<br/>                    ‘remotedata’: 5568,<br/>                    ‘siteindesiredlist’: 1610,<br/>                    ‘sitenotindesiredlist’: 5419},<br/> ‘T2_US_Purdue’: {‘localdata’: 2760,<br/>                  ‘remotedata’: 145,<br/>                  ‘siteindesiredlist’: 2898,<br/>                  ‘sitenotindesiredlist’: 7},<br/> ‘T2_US_UCSD’: {‘localdata’: 1252,<br/>                ‘remotedata’: 447,<br/>                ‘siteindesiredlist’: 1699,<br/>                ‘sitenotindesiredlist’: 0},<br/> ‘T2_US_Vanderbilt’: {‘localdata’: 2486,<br/>                      ‘remotedata’: 447,<br/>                      ‘siteindesiredlist’: 2543,<br/>                      ‘sitenotindesiredlist’: 390},<br/> ‘T2_US_Wisconsin’: {‘localdata’: 2193,<br/>                     ‘remotedata’: 214,<br/>                     ‘siteindesiredlist’: 2407,<br/>                     ‘sitenotindesiredlist’: 0},<br/>
<span style="color: #de5f24"><span style="font-size: small">(15:52:24)</span> <b>justas.balcas:</b></span> nebraska runs most of jobs without data being locally<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:53:06)</span> <b>andrew.melo:</b></span> But it's not just file accesses, the pilots need to talk to the collectors, etc..<br/>
<span style="color: #a72f79"><span style="font-size: small">(15:53:16)</span> <b>andrew.melo:</b></span> There's some UDP packets flying around<br/>
<span style="color: #de5f24"><span style="font-size: small">(16:03:53)</span> <b>justas.balcas:</b></span> just to rule out one issue - so that not xrootd ddos. just random guess, but 1 con/per core + few for startd itself (?) htcondor experts would know better this (and because florida is not full node scheduling - 8core pilot requires ~2 more conns). And florida runs only 1338 jobs - so that should not be an issue either.<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:05:44)</span> <b>andrew.melo:</b></span> but they're not doing PAT, so there's a 1-1 mapping between private and public addreses<br/>
<span style="color: #a72f79"><span style="font-size: small">(16:06:58)</span> <b>andrew.melo:</b></span> It's not connections that's exhausting it, it's addresses<br/>
<span style="color: #9e3997"><span style="font-size: small">(18:30:41)</span> <b>bbockelm:</b></span> Sounds like maybe CMS is spread across more nodes than before?<br/>
<span style="color: #9e3997"><span style="font-size: small">(18:32:14)</span> <b>bbockelm:</b></span> If it’s consuming an address per worker node where CMS is using IPv4, then it’s not obvious what going to IPv6 buys (if you have to get everything to IPv6 to make it effective).<br/>
<span style="color: #7d414c"><span style="font-size: small">(18:39:35)</span> <b>bockjoo:</b></span> I am assuming CERN is covered by IPv6 and ipv4 only should be still handled by NAT<br/>
<span style="color: #a72f79"><span style="font-size: small">(19:21:06)</span> <b>andrew.melo:</b></span> .... until you get unlucky and you get a batch of jobs who talk to ipv4 resources<br/>
<span style="color: #a72f79"><span style="font-size: small">(19:24:58)</span> <b>andrew.melo:</b></span> And that assumption that CERN is covered by IPv6 isn't great:<br/><pre>[root@k8s001 ACCRE]# dig AAAA +short <a href="http://cmsweb-k8s-prod.cern.ch">cmsweb-k8s-prod.cern.ch</a><br/>[root@k8s001 ACCRE]# dig AAAA +short <a href="http://cmsweb.cern.ch">cmsweb.cern.ch</a><br/>[root@k8s001 ACCRE]#</pre><br/>
<span style="color: #7d414c"><span style="font-size: small">(20:51:46)</span> <b>bockjoo:</b></span> Theoretically, you might have to access those ipv4 machines, but in practice there is very little chance anything has to access those webs. Also we have enough NAT + IPv6-enabled CERN. If not, I will have to ask my network engineers to throw some more. But to ask that, we need to understand why the newly added NAT addresses evaporate very quickly.<br/>
<span style="color: #a72f79"><span style="font-size: small">(21:43:31)</span> <b>andrew.melo:</b></span> I don't think it's wise to under-provision and just hope that you won't exhaust your NAT address space.<br/>
<span style="color: #7d414c"><span style="font-size: small">(21:47:02)</span> <b>bockjoo:</b></span> Obviously something unexpected happened. This is not because of under-provision.<br/>
<span style="color: #a72f79"><span style="font-size: small">(21:47:50)</span> <b>andrew.melo:</b></span> It certainly is! If you have X hosts, and your NAT can only support Y hosts, and X &gt; Y, then your're under-provisioned<br/>
<span style="color: #7d414c"><span style="font-size: small">(21:49:00)</span> <b>bockjoo:</b></span> X(t1) &gt; Y (t1) but most of the time X &lt; Y.<br/>
<span style="color: #a72f79"><span style="font-size: small">(21:49:08)</span> <b>andrew.melo:</b></span> No, that's also not good<br/>
<span style="color: #a72f79"><span style="font-size: small">(21:49:13)</span> <b>andrew.melo:</b></span> "most of the time" isn't what we're looking for<br/>
<span style="color: #7d414c"><span style="font-size: small">(21:49:21)</span> <b>bockjoo:</b></span> We are trying to figure out what's causing this.<br/>
<span style="color: #7d414c"><span style="font-size: small">(21:50:56)</span> <b>bockjoo:</b></span> Otherwise, we have to keep throwing more and more addresses into it. We are trying. That's why I brought up the issue not trying to debate whether this is under-provisioned or not.<br/>
<span style="color: #a72f79"><span style="font-size: small">(21:51:38)</span> <b>andrew.melo:</b></span> I think your statement <tt>X(t1) &gt; Y (t1) but most of the time X &lt; Y.</tt> in itself says you're under-provisioned<br/>
<span style="color: #a72f79"><span style="font-size: small">(21:53:24)</span> <b>andrew.melo:</b></span> We don't have the capability to schedule jobs based on the available NAT resources at a site, so you have to assume the worst and provision for that<br/>
<span style="color: #7d414c"><span style="font-size: small">(21:55:59)</span> <b>bockjoo:</b></span> No, I don't agree with you.<br/>
<span style="color: #a72f79"><span style="font-size: small">(21:56:18)</span> <b>andrew.melo:</b></span> What am I misssing?<br/>
<span style="color: #7d414c"><span style="font-size: small">(21:58:08)</span> <b>bockjoo:</b></span> We are trying to figure out the reason. I am not sure what you are trying to extract from me.<br/>
<span style="color: #a72f79"><span style="font-size: small">(21:59:33)</span> <b>andrew.melo:</b></span> I guess "the reason" for using the NAT is that every CMS job needs to access the public internet. We spent some time looking at what exactly the CMS jobs are touching, but at the end of the day it really doesn't matter "what" they're doing on the public internet, each job needs that connection<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:01:14)</span> <b>andrew.melo:</b></span> If I'm understanding right, the assumption is "well, CMS must be doing something weird", but I'm pushing back because network connectivity is something that the sites need to provide<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:02:46)</span> <b>andrew.melo:</b></span> And it's a weird gamble to say "well, usually they don't need to access the public internet that much", when that depends on the makeup of what jobs happen to be running at any given time<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:03:02)</span> <b>bockjoo:</b></span> I don't know if CMS must be doing something wrong. As I mentioned, we are trying to debug the issue.<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:03:30)</span> <b>bockjoo:</b></span> I never said anything about 'gamble'.<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:04:37)</span> <b>bockjoo:</b></span> I only said why you need to access cmsweb for cms jobs. I don't see any job is accessing cmsweb right now.<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:05:37)</span> <b>andrew.melo:</b></span> That was in response to you saying <tt>I am assuming CERN is covered by IPv6 and ipv4 only should be still handled by NAT</tt> -- there are plenty of hosts and services at CERN that aren't IPv6-enabled, so that's not a safe assumption if you're trying to skimp on the NAT<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:06:24)</span> <b>bockjoo:</b></span> If jobs should access cmsweb, cmsweb can enable ipv6 anytime, but I didn't even mean that I would rely on that either.<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:07:13)</span> <b>andrew.melo:</b></span> &gt;  If jobs should access cmsweb, cmsweb can enable ipv6 anytime<br/>Sure, but my point is that you can't just hope that every service a job depends on has ipv6 enabled<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:08:58)</span> <b>bockjoo:</b></span> Well, based on SAM tests, the error is only due to the missing ipv6 address on our worker nodes.<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:09:53)</span> <b>andrew.melo:</b></span> I think you would agree that a test that runs every 15 minutes and doesn't actually run a "real" CMS job isn't exactly a representative test for the issue we're talking about :slightly_smiling_face:<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:11:25)</span> <b>andrew.melo:</b></span> For example, SAM tests don't run glideins at all, so if part of the issue is IPv4 connections to the collectors, it's not gonna find that, right?<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:14:11)</span> <b>bockjoo:</b></span> We do not know how many addresses have to be provisioned. According to the SAM test result, we see lack of addresses but that currently does not make sense.<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:16:27)</span> <b>bockjoo:</b></span> If there are ipv4 collectors, I would like to know which ones they are. That may show some hint to the issue.<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:16:43)</span> <b>andrew.melo:</b></span> I"m sorry, which sam test result are you talking about?<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:17:27)</span> <b>andrew.melo:</b></span> The only issues I see on your sam board are an xrootd fallback test that tried to go to beijing and failed, which *shrug*<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:18:13)</span> <b>bockjoo:</b></span> WN-mc-basic and xrootd fallback.<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:18:14)</span> <b>andrew.melo:</b></span> &gt;  We do not know how many addresses have to be provisioned<br/>If I understand your configuration right, you need to have as many public IPs as you have private IPs. Do you see it differently?<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:19:14)</span> <b>bockjoo:</b></span> No.<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:22:40)</span> <b>andrew.melo:</b></span> So, because you're doing 1:1 NAT, you're blowing through 1270 public addresses?<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:23:11)</span> <b>bockjoo:</b></span> I guess.<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:23:43)</span> <b>bockjoo:</b></span> But CMS jobs are running only on less than 450 nodes when the fallback error happened.<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:24:18)</span> <b>andrew.melo:</b></span> Hm. And nothing else on your internal network ever uses the public internet?<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:24:29)</span> <b>bockjoo:</b></span> WN-mc-basic test is the true teller because it tries to connect through ipv6. But there has been no error with WN-mc-basic today.<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:25:03)</span> <b>bockjoo:</b></span> Except for CMS, I don't think so. HPC people said the same thing.<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:25:22)</span> <b>bockjoo:</b></span> But who knows?<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:26:42)</span> <b>andrew.melo:</b></span> So, if there's 450 nodes who are absolutely using the public internet, and the HPC says they're not using it but still you're blowing through 1270 public addresses in your NAT, then I guess there's really 2 options<br/>1. the NAT is misconfigured<br/>2. the HPC is mistaken<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:29:37)</span> <b>bockjoo:</b></span> True and I have been asking them. But they are either debugging it (1) or too busy to check it (2). Just my guess.<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:30:01)</span> <b>andrew.melo:</b></span> Fair enough<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:32:09)</span> <b>andrew.melo:</b></span> But drilling down into which hosts in CMS use ipv4 or ipv6 doesn't solve that, right? You're already upfront saying "these 450 nodes will hit v4 hosts". That doesn't bring down the ~800 or so other nodes that are hitting it<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:33:14)</span> <b>andrew.melo:</b></span> .. which, by the HPC's assertion, they're not using it<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:35:07)</span> <b>andrew.melo:</b></span> Jumping straight ahead to "well let's just deploy ipv6 internally" doesn't make sense to me, because unless you find out who those other 800 nodes are, it doesn't actually fix it<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:35:38)</span> <b>bockjoo:</b></span> Correct and that's what I was trying to point out.<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:35:40)</span> <b>andrew.melo:</b></span> (not that deploying IPv6 is a bad idea, it's just not the right hammer for this particular nail)<br/>
<span style="color: #7d414c"><span style="font-size: small">(22:36:25)</span> <b>bockjoo:</b></span> Tomorrow, I will ask them again if they don't email me first.<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:40:11)</span> <b>andrew.melo:</b></span> OK, in that case, "what" CMS is accessing doesn't matter ... those 450 nodes need ipv4 connectivity, and it's not unreasonable to expect that from the hosting facility. This whole dance of trying to figure out which IPs are doing what is counter-productive because "we" dictate where the connections go, and it's their responsibility to provide outbound connectivity<br/>
<span style="color: #a72f79"><span style="font-size: small">(22:42:09)</span> <b>andrew.melo:</b></span> And especially as we move to more containerized/agile/etc... services, they won't be at fixed addreses, so we're not going to commit that the working set of addresses jobs connect to stays fixed either<br/>
</body>
</html>

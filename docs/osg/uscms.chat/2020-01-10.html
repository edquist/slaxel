<!DOCTYPE html>
<html>
<head>
<title>Fri Jan 10, 2020 : #uscms (osg)</title>
</head>
<body>
<h3>Fri Jan 10, 2020 : #uscms (osg)</h3>
<span style="color: #e06b56"><span style="font-size: small">(09:19:59)</span> <b>jthiltges:</b></span> @andrew.melo For replies to your ceph as /home question, I'd be interested in a rough summary, if you'd be willing to share it.<br/>
<span style="color: #a72f79"><span style="font-size: small">(09:21:09)</span> <b>andrew.melo:</b></span> for sure -- I guess I can just post the Q here, since most (all?) of the T2 admins hang here<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:09:24)</span> <b>andrew.melo:</b></span> So, my situation is like this: Our cluster /home filesystem is GPFS which sucks because the per-socket licensing fees basically make it so we can only have X sockets in our cluster running GPFS (and since it's IBM, the price per-socket is only increasing). When I got a bunch of nodes to build my spark/jupyter cluster, I couldn't use GPFS, so as a stopgap (and because I assumed most data would be stored on our larger filesystems), I made an NFS-HA setup out of a couple 10G hosts and SSDs I had sitting around.<br/><br/>Now we're getting more local users whose applications are quite large with many small files (there's apparently some popular medical packages that are conda-esque in their size), and instead of trying to expand the existing NFS setup (who is about to run out of drive bays), I'm investigating migrating to CEPH. We use it for VM block storage for our infra HV cluster and it appears to work pretty well**, so I'm looking into setting up another independent CEPH cluster for /user home directories<br/><br/>I was wondering about your experience w/the following (since it's hard to filter out the wheat from the chaff):<br/>• How does the small file performance of CEPH hold up? I notice that NFS drags pretty hard handling loading these apps (I presume because of all the metadata calls?)<br/>• How much effort does it take operationally (i.e. do the mounts barf regularly, are upgrades/rolling reboots a pain, how much do things fail with a missing drive/node, is adding/removing nodes truely as painless as the docs suggest) <br/>And thinking about deployment slash to calibrate your experiences above:<br/>• Do you allow access to /home from the entire cluster, or just the interactive nodes<br/>• Do you use replication or erasure coding, do you bother with a cache pool?<br/>• How beefy are your MDS servers (the recommendations<br/>**there's something happening where occasionally the root devices on those VMs get a device error and remount read-only, the guys who manage that cluster haven't been able to hunt that gremlin down yet<br/>
<span style="color: #43761b"><span style="font-size: small">(10:16:04)</span> <b>blin:</b></span> you should reach out to @lincoln, who's been using CEPH for years, and @tslauson who's investigating setting it up here at the CHTC<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:19:32)</span> <b>andrew.melo:</b></span> $$ This conversation started on the cms-t2 list and Justas recommended I message him @ slack, then John was curious as well, so I put it into a non-private chat.<br/><br/>I'm very curious to hear about everyone's experiences. I should say that because IBM keeps cranking their licensing to increasingly unsustainable levels, we're investigating migrating our main cluster's ~PB-ish GPFS to somewhere else, so this much smaller project might evolve into something larger over time<br/>
<span style="color: #e06b56"><span style="font-size: small">(10:31:40)</span> <b>jthiltges:</b></span> I can't contribute useful Ceph answers, but here's our (Nebraska/HCC) model today:<br/>* Small /home shared via NFS, from a single ZFS server<br/>    * Currently mounted r/o on workers<br/>    * We'd like to move it to all flash and consider r/w, but not there yet<br/>* Larger /work on BeeGFS (or Lustre, but BeeGFS seems less harrowing)<br/><br/>We've treated /home as the high-reliability filesystem, with regular snapshots and off-site replicas. I'm not sure if Ceph lines up with that goal.<br/><br/>At least with older Ceph (jewel) as an object store, recovery and rebalancing has been rough on I/O performance. Should be better with mimic and beyond, but we're not yet upgraded.<br/>
<span style="color: #43761b"><span style="font-size: small">(10:35:30)</span> <b>blin:</b></span> could you bring this to #software and ping lincoln/tim there?<br/>
<span style="color: #43761b"><span style="font-size: small">(10:35:37)</span> <b>blin:</b></span> no reason to keep this CMS-specific<br/>
<span style="color: #a72f79"><span style="font-size: small">(10:35:44)</span> <b>andrew.melo:</b></span> Good point, sure thing<br/>
</body>
</html>

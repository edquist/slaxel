<!DOCTYPE html>
<html>
<head>
<title>Tue Jan 19, 2021 : #uscms (osg)</title>
</head>
<body>
<h3>Tue Jan 19, 2021 : #uscms (osg)</h3>
<span style="color: #a72f79"><span style="font-size: small">(17:14:56)</span> <b>andrew.melo:</b></span> Not specifically USCMS-related, but I'm trying to spin up a testing CEPH cluster on k8s (w/rook) and failing miserably at mounting the deployed CEPH install. Even if I use the admin keyring, I keep getting authentication errors. How do I actually find the relevant logs for that?<br/>
<span style="color: #a72f79"><span style="font-size: small">(17:15:56)</span> <b>andrew.melo:</b></span> I don't mind digging through logs, but I can't even find logs that look relevant<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:55:32)</span> <b>justas.balcas:</b></span> Dont know in k8s (w/rook) - but that would be first test:<br/><pre>ceph --debug --verbose</pre><br/>or<br/><pre>ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health</pre><br/>and check configs are correct here:<br/><pre>cat /etc/ceph/admin.secret<br/>XXXXXXX==<br/>[root@transfer-10 storage]# cat /etc/ceph/ceph.client.admin.keyring<br/>[client.admin]<br/>	key = XXXXXXXX==<br/>	caps mds = "allow *"<br/>	caps mgr = "allow *"<br/>	caps mon = "allow *"<br/>	caps osd = "allow *"<br/>[root@transfer-10 storage]# cat /etc/ceph/ceph.conf<br/>[global]<br/>fsid = XXXXXXX<br/>mon_initial_members = node01, node02, node03, node04, node05, node06<br/>mon_host = 10.3.16.4,10.3.16.117,10.3.18.48,10.3.12.6,10.3.11.37,10.3.20.87<br/>auth_cluster_required = cephx<br/>auth_service_required = cephx<br/>auth_client_required = cephx<br/>public network = 10.0.0.0/8</pre><br/>You should look at your mons for auth issues. client will call monitor to authenticate<br/>
<span style="color: #a72f79"><span style="font-size: small">(18:03:37)</span> <b>andrew.melo:</b></span> OK, that's helpful, I can start looking through the mon logs for more<br/>
<span style="color: #a72f79"><span style="font-size: small">(18:03:38)</span> <b>andrew.melo:</b></span> thanks!<br/>
<span style="color: #a72f79"><span style="font-size: small">(18:04:47)</span> <b>andrew.melo:</b></span> BTW, you guys use an EC pool for your shared home, right?<br/>
<span style="color: #de5f24"><span style="font-size: small">(18:12:40)</span> <b>justas.balcas:</b></span> not for home directories yet<br/>
<span style="color: #a72f79"><span style="font-size: small">(18:19:18)</span> <b>andrew.melo:</b></span> replication + caching?<br/>
<span style="color: #de5f24"><span style="font-size: small">(18:51:14)</span> <b>justas.balcas:</b></span> only replication (rep 2), metadata - rep 4 on nvmes<br/>
<span style="color: #a72f79"><span style="font-size: small">(19:03:14)</span> <b>andrew.melo:</b></span> Do you put the bluestore WAL on SSD too, or is the data pool straight spinning disks?<br/>
<blockquote>
<span style="color: #de5f24"><span style="font-size: small">(2021-01-22 11:31:00)</span> <b>justas.balcas:</b></span> We are not separating it for analysis facility. Most nodes are only spinning disks. The only thing we keep on SSD (DC P4610 - total 7 of them) is metadata. Test cluster is only HDDs, and metadata is on HDD also. This will change for CMS FS, but not planning to separate DB/WAL. CMS Pool will have more SSDs for metadata with bigger replication. For another pool (SDN) - also no point to separate - it is all SSDs<br/>
<span style="color: #a72f79"><span style="font-size: small">(2021-01-22 11:32:00)</span> <b>andrew.melo:</b></span> Your AF is like 300-400tB usable, right? How large does the metadata pool get?<br/>
<span style="color: #a72f79"><span style="font-size: small">(2021-01-22 11:35:32)</span> <b>andrew.melo:</b></span> I'm using some spare hardware to build up a ceph install to get some experience, and I have like 5-600TB of raw disk. I could probably get some $ for a couple of SSDs, I just kinda wanna know what to ask for<br/>
<span style="color: #de5f24"><span style="font-size: small">(2021-01-22 11:41:24)</span> <b>justas.balcas:</b></span> 297 RAW, Rep 2 for data, used 56tb(112 with rep 2); Metadata - 175 MB used with replica - x3.<br/>
<span style="color: #a72f79"><span style="font-size: small">(2021-01-22 11:41:41)</span> <b>andrew.melo:</b></span> oh hell, that little for metadata?<br/>
<span style="color: #de5f24"><span style="font-size: small">(2021-01-22 11:42:59)</span> <b>justas.balcas:</b></span> <br/>
<span style="color: #de5f24"><span style="font-size: small">(2021-01-22 11:43:04)</span> <b>justas.balcas:</b></span> yep, and pretty low activity on it<br/>
<span style="color: #a72f79"><span style="font-size: small">(2021-01-22 11:43:36)</span> <b>andrew.melo:</b></span> huh, and that's for your home directories who I assume have a lot of small files?<br/>
<span style="color: #de5f24"><span style="font-size: small">(2021-01-22 11:47:00)</span> <b>justas.balcas:</b></span> yes, I did not looked at file distribution, but yes. If I look at user who has most files - 4.4mln, uses 10tb - ~2.2mb/file<br/>
<span style="color: #de5f24"><span style="font-size: small">(2021-01-22 11:54:53)</span> <b>justas.balcas:</b></span> The only complain we had, whenever one user overloads all FS - reading like 3GB/s. Then it becomes slow to do git, vi or etc.. But 3GB/s - is maxing out all HDDs at once<br/>
<span style="color: #a72f79"><span style="font-size: small">(2021-01-22 11:55:49)</span> <b>andrew.melo:</b></span> I think I have 60 or so spindles, so hopefully that should be sufficient<br/>
<span style="color: #de5f24"><span style="font-size: small">(2021-01-22 12:03:56)</span> <b>justas.balcas:</b></span> yea - and in AF - all is pretty default Ceph config. - we did some config changes lately to decrease memory usage - and that mainly affects  replication.<br/>For CMS Pool - now I am testing diff stripe/object size - as this can improve read/write. Not sure yet where the sweet spot is.<br/>
<span style="color: #a72f79"><span style="font-size: small">(2021-01-22 15:07:22)</span> <b>andrew.melo:</b></span> If you don't mind me asking you questions :slightly_smiling_face: -- suppose I set up a quota for each dir in /home via xattr -- how do users see the current usage?<br/>
<span style="color: #de5f24"><span style="font-size: small">(2021-01-22 15:47:33)</span> <b>justas.balcas:</b></span> <pre>-bash-4.2$ getfattr -dm- /storage/<br/>getfattr: Removing leading '/' from absolute path names<br/># file: storage/<br/>ceph.dir.entries="6"<br/>ceph.dir.files="0"<br/>ceph.dir.rbytes="62045696660156"<br/>ceph.dir.rctime="1907051487.090"<br/>ceph.dir.rentries="32398296"<br/>ceph.dir.rfiles="31183577"<br/>ceph.dir.rsubdirs="1214719"<br/>ceph.dir.subdirs="6"<br/><br/>-bash-4.2$ getfattr -dm- /storage/user/jbalcas/<br/>getfattr: Removing leading '/' from absolute path names<br/># file: storage/user/jbalcas/<br/>ceph.dir.entries="52"<br/>ceph.dir.files="28"<br/>ceph.dir.rbytes="1022357216515"<br/>ceph.dir.rctime="1611351977.09406065178"<br/>ceph.dir.rentries="335415"<br/>ceph.dir.rfiles="322783"<br/>ceph.dir.rsubdirs="12632"<br/>ceph.dir.subdirs="24"</pre><br/>
<span style="color: #a72f79"><span style="font-size: small">(2021-01-22 15:51:23)</span> <b>andrew.melo:</b></span> ahhhh, so the "p" permission is just to set the attrs, not read them<br/>
</blockquote>
</body>
</html>

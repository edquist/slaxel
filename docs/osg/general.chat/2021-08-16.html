<!DOCTYPE html>
<html>
<head>
<title>Mon Aug 16, 2021 : #general (osg)</title>
</head>
<body>
<h3>Mon Aug 16, 2021 : #general (osg)</h3>
<span style="color: #bd9336"><span style="font-size: small">(19:57:26)</span> <b>mkandes:</b></span> @rynge - When you get the chance, can you check with @ershockley that his current Pegasus workflow is running properly? I was pinged by our security guy about some OSG glideins running on Expanse.<br/><br/><pre>OSG is doing something weird on 198.202.101.111. A python script on that host has been making connections to 90.147.112.18 ports [27015, 27017, 27018] at about 200,000 connections per hour.</pre><br/>
<blockquote>
<span style="color: #99a949"><span style="font-size: small">(19:59:34)</span> <b>ershockley:</b></span> Hmm yeah that doesn’t seem right at all. I will take a look at what it could be. Sorry!<br/>
<span style="color: #bd9336"><span style="font-size: small">(19:59:48)</span> <b>mkandes:</b></span> Thanks Evan.<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:00:00)</span> <b>mkandes:</b></span> Some glidein/job info ....<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:00:04)</span> <b>mkandes:</b></span> <pre>[osg@exp-12-52 glide_IuZtpN]$ less glidein_config<br/><br/>xpanse-Xenon<br/>GLIDEIN_UUID 63c89206-e5e0-492e-bc41-caa556dc6e27<br/>GLIDEIN_Factory OSGIR /tmp/glide_osg_97tX0P<br/>GLIDEIN_Name gfactory_instance3128<br/>GLIDEIN_Entry_Name OSG_US_SDSC-Expanse-Xenon<br/>GLIDECLIENT_Name osg-flock-grid-iu-edu_OSG_gWMSFrontend.alloc-Expanse-CPU-xenon1t<br/>GLIDECLIENT_Group alloc-Expanse-CPU-xenon1t<br/>GLIDEIN_CredentialIdentifier ZMLOYLUS<br/>CONDORG_CLUSTER 5622046<br/>CONDORG_SUBCLUSTER 0<br/>CONDORG_SCHEDD <a href="mailto:schedd_glideins5@gfactory-2.opensciencegrid.org">schedd_glideins5@gfactory-2.opensciencegrid.org</a><br/>DEBUG_MODE 1<br/>GLIDEIN_STARTUP_PID 80015<br/>GLIDEIN_WORK_DIR /scratch/osg/job_5314881/glide_IuZtpN/main<br/>GLIDEIN_ENTRY_WORK_DIR /scratch/osg/job_5314881/glide_IuZtpN/entry_OSG_US_SDSC-Expanse-Xenon<br/>TMP_DIR /scratch/osg/job_5314881/glide_IuZtpN/tmp</pre><br/>
<span style="color: #bd9336"><span style="font-size: small">(20:02:32)</span> <b>mkandes:</b></span> <pre>08/16/21 17:58:43 (pid:97910) slot1_6: Request accepted.<br/>08/16/21 17:58:43 (pid:97910) slot1_6: Remote owner is <a href="mailto:ershockley@services.ci-connect.net">ershockley@services.ci-connect.net</a><br/>08/16/21 17:58:43 (pid:97910) slot1_6: State change: claiming protocol successful<br/>08/16/21 17:58:43 (pid:97910) slot1_6: Changing state: Owner -&gt; Claimed<br/>08/16/21 17:58:43 (pid:97910) slot1_6: Got activate_claim request from shadow (192.170.227.173)<br/>08/16/21 17:58:43 (pid:97910) slot1_6: REQ_CLASSAD:<br/>        AccountingGroup = "group_opportunistic.xenon1t.ershockley"<br/>        Arguments = ""<br/>        AutoClusterAttrs = "DESIRED_Sites,DynamicSlot,HasJava,ITB_Factory,ITB_Sites,LastHeardFrom,MachineLastMatchTime,Memory,PartitionableSlot,ProjectName,Rank,Remo<br/>teOwner,RequestCpus,RequestDisk,RequestGPUs,RequestMemory,SleepSlot,Slot1_SelfMonitorAge,Slot1_TotalTimeClaimedBusy,SUBMITTER_stash_osgstorage_org_REVISION,TotalJobR<br/>untime,UNDESIRED_Sites,User,WantsStashCvmfs,ConcurrencyLimits,FlockTo,Requirements,_cp_orig_RequestCpus,_cp_orig_RequestDisk,_cp_orig_RequestMemory,CVMFS_stash_osgst<br/>orage_org_REVISION,FirstUpdateUptimeGPUsSeconds,JobUniverse,LastCheckpointPlatform,LastUpdateUptimeGPUsSeconds,NumCkpts,OSG_NODE_VALIDATED,RunOnSubmitNode,Slot10_Rem<br/>oteOwner,Slot1_RemoteOwner,Slot1_TotalTimeUnclaimedIdle,Slot2_RemoteOwner,Slot3_RemoteOwner,Slot4_RemoteOwner,Slot5_RemoteOwner,Slot6_RemoteOwner,Slot7_RemoteOwner,S<br/>lot8_RemoteOwner,Slot9_RemoteOwner,StartOfJobUptimeGPUsSeconds,undeined,UptimeGPUsSeconds,WantTigerBackfill,XENON_DESIRED_Sites,Desired_Allocations,IDTOKEN,WantsXSED<br/>E,QDate,DAGNodeRetry,GLIDEIN_Country,GLIDEIN_Site,HAS_AVX,HAS_AVX2,HAS_CVMFS_xenon_opensciencegrid_org,HAS_SINGULARITY"<br/>        AutoClusterId = 2320<br/>        ClusterId = 7127896<br/>        Cmd = "/scratch/ershockley/workflows/runs/multiples-xent_024104_global_ONLINE/00/47/events_ID0002548.sh"<br/>        CommittedSlotTime = 0<br/>        CommittedSuspensionTime = 0<br/>        CommittedTime = 0<br/>        CompletionDate = 0<br/>        CondorPlatform = "$CondorPlatform: X86_64-CentOS_7.9 $"<br/>        CondorVersion = "$CondorVersion: 9.1.2 Jul 29 2021 PackageID: 9.1.2-1.1 $"<br/>        ConnectWrapper = "2.1"<br/>        CoreSize = 0<br/>        CpusProvisioned = 4<br/>        CumulativeRemoteSysCpu = 0.0<br/>        CumulativeRemoteUserCpu = 0.0<br/>        CumulativeSlotTime = 0<br/>        CumulativeSuspensionTime = 0<br/>        CurrentHosts = 1<br/>        DAGManJobId = 7124403<br/>        DAGManNodesLog = "/scratch/ershockley/workflows/runs/multiples-xent_024104_global_ONLINE/./xenonnt-0.dag.nodes.log"<br/>        DAGManNodesMask = "0,1,2,4,5,7,9,10,11,12,13,16,17,24,27,35,36"<br/>        DAGNodeName = "events_ID0002548"<br/>        DAGNodeRetry = 1<br/>        DAGParentNodeNames = "stage_in_local_staging_0_2,stage_in_local_staging_0_1,stage_in_local_staging_0_5,stage_in_local_staging_0_4,combine_ID0002533"<br/>        Desired_Allocations = "Expanse"<br/>        DiskProvisioned = 9113893<br/>        DiskUsage = 12500<br/>        DiskUsage_RAW = 10547<br/>        EncryptExecuteDirectory = false<br/>        EnteredCurrentStatus = 1629161923<br/>        Environment = "PEGASUS_SUBMITTING_USER=ershockley PEGASUS_DAG_JOB_ID=events_ID0002548 GLOBUS_LOCATION= PEGASUS_SITE=condorpool CONDOR_JOBID=7127896.0 OSG_LOCATION= PYTHONPATH= LANG=en_US.UTF-8 PEGASUS_WF_LABEL=xenonnt PEGASUS_WF_UUID=094f4dc0-2058-4bed-96e0-5f0e847a5cfb RUCIO_LOGGING_FORMAT=%(asctime)s'  '%(levelname)s'  '%(message)s LD_LIBRARY_PATH= PERL5LIB= RUCIO_ACCOUNT=production"<br/>        Err = "/scratch/ershockley/workflows/runs/multiples-xent_024104_global_ONLINE//00/47/events_ID0002548.err"<br/>...<br/>       JobBatchId = "7124403.0"<br/>        JobBatchName = "xenonnt-0.dag+7124403"<br/>        JobCurrentStartDate = 1629161923<br/>        JOBGLIDEIN_ResourceName = "SDSC-Expanse"<br/>        JobLeaseDuration = 2400<br/>        JobNotification = 0<br/>        JobPrio = 620863760<br/>        JobRunCount = 1<br/>        JobStartDate = 1629161923<br/>        JobStatus = 2<br/>        JobUniverse = 5<br/>        KeepClaimIdle = 20<br/>        LastJobLeaseRenewal = 1629161923<br/>        LastJobStatus = 1<br/>        LastMatchTime = 1629161923<br/>        LastRejMatchReason = "no match found "<br/>        LastRejMatchTime = 1629161922</pre><br/>
<span style="color: #bd9336"><span style="font-size: small">(20:06:20)</span> <b>mkandes:</b></span> Hmmm ... this would not be a valid path on Expanse ... <tt>/scratch/ershockley/workflows</tt>  .... any chance that is the problem?<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:07:49)</span> <b>mkandes:</b></span> The local /scratch for the glidein is<br/><pre>/scratch/osg/job_5314881/glide_IuZtpN/</pre><br/><br/>
<span style="color: #99a949"><span style="font-size: small">(20:10:51)</span> <b>ershockley:</b></span> That is the path on the submit host in Chicago. I have to say I’m not an expert on how the jobs get passed from OSG to Expanse  (would defer to @rynge). But based on the ports it looks like it is something related to queries to our mongo database used for our project (which would definitely be my fault). It’s just not jumping out at me where it could be yet. I’ll keep looking and kill my workflows for now<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:12:08)</span> <b>mkandes:</b></span> Ah. Okay, that makes sense then. Yes, it's always a bit difficult to untangle what paths are relative to what filesystem on what machine.<br/>
<span style="color: #bd9336"><span style="font-size: small">(20:13:29)</span> <b>mkandes:</b></span> If it's a a bunch of database queries, maybe check if they are succeeding and/or being rate limited ... either could cause excessive retries.<br/>
<span style="color: #99a949"><span style="font-size: small">(20:18:51)</span> <b>ershockley:</b></span> Thanks for the suggestion — yeah the connections were not succeeding because the mongo port of the main database host (90.147.112.18) is not globally accessible. Dumb mistake on my part, my bad! I will only use the other mirrors we have<br/>
<span style="color: #bd9336"><span style="font-size: small">(2021-08-17 11:52:23)</span> <b>mkandes:</b></span> :+1:<br/>
</blockquote>
</body>
</html>

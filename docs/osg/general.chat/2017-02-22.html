<!DOCTYPE html>
<html>
<head>
<title>Wed Feb 22, 2017 : #general (osg)</title>
</head>
<body>
<h3>Wed Feb 22, 2017 : #general (osg)</h3>
<span style="color: #b14cbc"><span style="font-size: small">(12:18:26)</span> <b>tmartin:</b></span> Hi folks, have an odd error with SAM regarding our gridftp server. <br/><br/>2017-02-22T07:32:49 : Creating output directory...<br/>2017-02-22T07:32:49 : Running the stage out...<br/>lcg-cp exit status: 1<br/>Non-zero lcg-cp Exit status!!!<br/>Cleaning up failed file:<br/>2017-02-22T07:32:50 : Command exited with status: 151<br/><br/>The thing is I can reproduce that on the command line every time I use try to push to the gridftp server with lcg-cp.<br/><br/>$ lcg-cp -b -D srmv2 --vo cms --srm-timeout 2400 --sendreceive-timeout 2400 --connect-timeout 300 --verbose  file:/home/users/tmartin/sm.zero <a href="gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero</a><br/>Using grid catalog type: UNKNOWN<br/>Using grid catalog : (null)<br/>VO name: cms<br/>Checksum type: None<br/><a href="gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero</a>: Invalid argument<br/>lcg_cp: Invalid argument<br/>
<span style="color: #b14cbc"><span style="font-size: small">(12:19:53)</span> <b>tmartin:</b></span> If I swap the arguments...<br/><br/>[1017] tmartin@uaf-8 ~$ lcg-cp -b -D srmv2 --vo cms --srm-timeout 2400 --sendreceive-timeout 2400 --connect-timeout 300 --verbose  <a href="gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero</a>  file:/home/users/tmartin/sm.zero<br/>Using grid catalog type: UNKNOWN<br/>Using grid catalog : (null)<br/>VO name: cms<br/>Checksum type: None<br/>Source URL: <a href="gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero</a><br/>File size: 125829120<br/>Source URL for copy: <a href="gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero</a><br/>Destination URL: file:/home/users/tmartin/sm.zero<br/># streams: 1<br/>            0 bytes      0.00 KB/sec avg      0.00 KB/sec inst<br/>            0 bytes      0.00 KB/sec avg      0.00 KB/sec inst<br/>     27262976 bytes  13311.53 KB/sec avg  13311.53 KB/sec inst<br/>     27262976 bytes  13311.53 KB/sec avg  13311.53 KB/sec inst<br/>    125829120 bytes  30718.13 KB/sec avg  48123.82 KB/sec inst<br/>    125829120 bytes  30718.13 KB/sec avg  48123.82 KB/sec inst<br/><br/>Transfer took 5040 ms<br/>
<span style="color: #8d4b84"><span style="font-size: small">(12:33:44)</span> <b>kherner:</b></span> In the failed case what happens if you do file:///home/users/tmartin/sm.zero<br/>
<span style="color: #b14cbc"><span style="font-size: small">(12:33:57)</span> <b>tmartin:</b></span> That does not matter<br/>
<span style="color: #8d4b84"><span style="font-size: small">(12:34:13)</span> <b>kherner:</b></span> Just in case the code expects protocol colon slash slash<br/>
<span style="color: #b14cbc"><span style="font-size: small">(12:34:19)</span> <b>tmartin:</b></span> I know, slashes right? First thing I tried, and I use that example because that is what the SAM test reports<br/>
<span style="color: #b14cbc"><span style="font-size: small">(12:35:01)</span> <b>tmartin:</b></span> [1034] tmartin@uaf-8 ~$ lcg-cp -b -D srmv2 --vo cms --srm-timeout 2400 --sendreceive-timeout 2400 --connect-timeout 300 --verbose  file:///home/users/tmartin/sm.zero <a href="gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero</a><br/>Using grid catalog type: UNKNOWN<br/>Using grid catalog : (null)<br/>VO name: cms<br/>Checksum type: None<br/><a href="gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu:2811/hadoop/cms/store/user/tmartin/sm.zero</a>: Invalid argument<br/>lcg_cp: Invalid argument<br/>[1034] tmartin@uaf-8 ~$<br/>
<span style="color: #b14cbc"><span style="font-size: small">(12:35:05)</span> <b>tmartin:</b></span> Nope<br/>
<span style="color: #b14cbc"><span style="font-size: small">(12:35:49)</span> <b>tmartin:</b></span> Technically the SAM test also excludes the port so...<br/>[1034] tmartin@uaf-8 ~$ lcg-cp -b -D srmv2 --vo cms --srm-timeout 2400 --sendreceive-timeout 2400 --connect-timeout 300 --verbose  file:///home/users/tmartin/sm.zero <a href="gsiftp://gftp.t2.ucsd.edu/hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu/hadoop/cms/store/user/tmartin/sm.zero</a><br/>Using grid catalog type: UNKNOWN<br/>Using grid catalog : (null)<br/>VO name: cms<br/>Checksum type: None<br/><a href="gsiftp://gftp.t2.ucsd.edu/hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu/hadoop/cms/store/user/tmartin/sm.zero</a>: Invalid argument<br/>lcg_cp: Invalid argument<br/>[1035] tmartin@uaf-8 ~$<br/>
<span style="color: #8d4b84"><span style="font-size: small">(12:36:55)</span> <b>kherner:</b></span> And just for completeness what about two slashes after the port number<br/>
<span style="color: #b14cbc"><span style="font-size: small">(12:37:42)</span> <b>tmartin:</b></span> [1037] tmartin@uaf-8 ~$ lcg-cp -b -D srmv2 --vo cms --srm-timeout 2400 --sendreceive-timeout 2400 --connect-timeout 300 --verbose  file:///home/users/tmartin/sm.zero <a href="gsiftp://gftp.t2.ucsd.edu:2811//hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu:2811//hadoop/cms/store/user/tmartin/sm.zero</a><br/>Using grid catalog type: UNKNOWN<br/>Using grid catalog : (null)<br/>VO name: cms<br/>Checksum type: None<br/><a href="gsiftp://gftp.t2.ucsd.edu:2811//hadoop/cms/store/user/tmartin/sm.zero">gsiftp://gftp.t2.ucsd.edu:2811//hadoop/cms/store/user/tmartin/sm.zero</a>: Invalid argument<br/>lcg_cp: Invalid argument<br/>[1037] tmartin@uaf-8 ~$<br/>
<span style="color: #b14cbc"><span style="font-size: small">(12:38:01)</span> <b>tmartin:</b></span> 3 either<br/>
<span style="color: #8d4b84"><span style="font-size: small">(12:47:21)</span> <b>kherner:</b></span> Strange things are afoot at the Circle K...<br/>
<span style="color: #b14cbc"><span style="font-size: small">(12:47:55)</span> <b>tmartin:</b></span> I know, I suspect swapping to gfal2 will solve this issue, it is weird though<br/>
<span style="color: #e96699"><span style="font-size: small">(14:40:43)</span> <b>ian_cancercomputer:</b></span> Thanks Terrence!<br/>
<span style="color: #b14cbc"><span style="font-size: small">(14:53:30)</span> <b>tmartin:</b></span> No worries :slightly_smiling_face:<br/>
<span style="color: #e96699"><span style="font-size: small">(14:53:53)</span> <b>ian_cancercomputer:</b></span> So, i've been maintaining a CE and WMs for a little while now, everything's been pretty straight-forward but I haven't worked out the best way to manage jobs when I need to do maintenance or emergency shutdowns.  Have tried manipulating my local batch system and manipulating the jobs on condor-ce, both seem to not handle things as cleanly as I'd like.  Was wondering what people's best practices are in this regard?<br/>
<span style="color: #b14cbc"><span style="font-size: small">(14:54:20)</span> <b>tmartin:</b></span> So what is the specific concern?<br/>
<span style="color: #b14cbc"><span style="font-size: small">(14:54:26)</span> <b>tmartin:</b></span> Most jobs are run in pilots these days<br/>
<span style="color: #e96699"><span style="font-size: small">(14:54:55)</span> <b>ian_cancercomputer:</b></span> Yep, and all of mine seem to be as well.  essentially I want to be able to shutdown WMs without losing any work.<br/>
<span style="color: #b14cbc"><span style="font-size: small">(14:55:20)</span> <b>tmartin:</b></span> WMs are worker nodes?<br/>
<span style="color: #e96699"><span style="font-size: small">(14:55:21)</span> <b>ian_cancercomputer:</b></span> And not causing any grief to the pilot system either<br/>
<span style="color: #e96699"><span style="font-size: small">(14:55:26)</span> <b>ian_cancercomputer:</b></span> yes.<br/>
<span style="color: #e96699"><span style="font-size: small">(14:55:33)</span> <b>ian_cancercomputer:</b></span> sorry, WNs<br/>
<span style="color: #235e5b"><span style="font-size: small">(14:55:34)</span> <b>dweitzel:</b></span> What Batch system do you have?<br/>
<span style="color: #e96699"><span style="font-size: small">(14:55:48)</span> <b>ian_cancercomputer:</b></span> I'm using torque (PBS) for the local batch.<br/>
<span style="color: #b14cbc"><span style="font-size: small">(14:56:13)</span> <b>tmartin:</b></span> Ok, well in the case of condor CE, you can patch the CE and then restart it without losing jobs on the nodes, as long as condor comes back in a few minutes.<br/>
<span style="color: #b14cbc"><span style="font-size: small">(14:56:26)</span> <b>tmartin:</b></span> On the batch side that will be batch system dependent<br/>
<span style="color: #b14cbc"><span style="font-size: small">(14:56:56)</span> <b>tmartin:</b></span> There is some slack built in though on the pilot side and HTCondor that can take an update without too much grief<br/>
<span style="color: #e96699"><span style="font-size: small">(14:56:58)</span> <b>ian_cancercomputer:</b></span> Usually putting the nodes offline in torque lets everything finish without scheduling anything new, but condor-ce tends to keep the jobs around so I think they're not getting staged out properly or something<br/>
<span style="color: #235e5b"><span style="font-size: small">(14:57:33)</span> <b>dweitzel:</b></span> eh, I think you are doing it right.<br/>
<span style="color: #235e5b"><span style="font-size: small">(14:57:57)</span> <b>dweitzel:</b></span> That is exactly what we do here.  We set a system wide "Reservation", drains the cluster while not starting new jobs.<br/>
<span style="color: #e96699"><span style="font-size: small">(14:58:33)</span> <b>ian_cancercomputer:</b></span> that's just on the local batch system? or do you set that on condor-ce?<br/>
<span style="color: #b14cbc"><span style="font-size: small">(14:58:55)</span> <b>tmartin:</b></span> Here is my take on it. If you drain a node you lose a lot of potential CPU, if the last job and the first job to drain have a large delta. I have generally preferred utilization maximization over any one jobs success.<br/>
<span style="color: #235e5b"><span style="font-size: small">(14:59:08)</span> <b>dweitzel:</b></span> We set it on the local batch system, since we have local and grid jobs.<br/>
<span style="color: #e96699"><span style="font-size: small">(14:59:31)</span> <b>ian_cancercomputer:</b></span> gotchya, thanks Derek.<br/>
<span style="color: #b14cbc"><span style="font-size: small">(14:59:35)</span> <b>tmartin:</b></span> If you have local jobs, that are not pilots, then that will affect that approach. We are 100% pilots here, even for local users<br/>
<span style="color: #e96699"><span style="font-size: small">(14:59:50)</span> <b>ian_cancercomputer:</b></span> I'm 100% pilots also, nothing local.<br/>
<span style="color: #235e5b"><span style="font-size: small">(15:00:18)</span> <b>dweitzel:</b></span> if you only have grid jobs though on your cluster, I would take Terrance's advice, kill all grid jobs, do your maintenance, set your cluster back to online.<br/>
<span style="color: #e96699"><span style="font-size: small">(15:01:08)</span> <b>ian_cancercomputer:</b></span> Can do.  And just to confirm, i should kill these on the local batch or via condor-ce ?<br/>
<span style="color: #b14cbc"><span style="font-size: small">(15:01:10)</span> <b>tmartin:</b></span> The pilot system is very good at getting users jobs executed, even if interrupted.<br/>
<span style="color: #b14cbc"><span style="font-size: small">(15:01:21)</span> <b>tmartin:</b></span> If you patch a node, just stop batch on it<br/>
<span style="color: #b14cbc"><span style="font-size: small">(15:01:44)</span> <b>tmartin:</b></span> if you patch the CE, patch it and restart it, at least on the HTCondor side condor picks up where it left off and the pilot system just sees it is back<br/>
<span style="color: #e96699"><span style="font-size: small">(15:02:07)</span> <b>ian_cancercomputer:</b></span> ok.  I've got a script to work back the batch jobs to their condor-ce job-ids so i could remove it from either side.<br/>
<span style="color: #b14cbc"><span style="font-size: small">(15:02:23)</span> <b>tmartin:</b></span> You do not need to ever remove pilots from the CE<br/>
<span style="color: #e96699"><span style="font-size: small">(15:02:42)</span> <b>ian_cancercomputer:</b></span> 'k, noted.<br/>
<span style="color: #b14cbc"><span style="font-size: small">(15:03:45)</span> <b>tmartin:</b></span> It really was designed to be very tolerant to a CE bouncing. Condor just kind of picks up where it left off and reconnects the pilots, and if they do not reconnect, they are just pilots. :slightly_smiling_face:<br/>
<span style="color: #e96699"><span style="font-size: small">(15:04:08)</span> <b>ian_cancercomputer:</b></span> That makes the emergency shutdown solution a lot easier, too.  Not that i should need to do this, but it seems construction workers tend to cut power to the building on the same days the generator is down for maintenance, so...<br/>
<span style="color: #b14cbc"><span style="font-size: small">(15:04:21)</span> <b>tmartin:</b></span> The user jobs get all of their stuff back to their base of operations at the end of the job on the node, the CE no longer takes part in that<br/>
<span style="color: #b14cbc"><span style="font-size: small">(15:06:04)</span> <b>tmartin:</b></span> Oh ya, I have done a condor_rm and a condor_ce_rm after a cooling or power cut, but often I do not unless I want to just confirm something. Generally I just start the CE first and then the batch system once storage checks out.<br/>
<span style="color: #e96699"><span style="font-size: small">(15:12:01)</span> <b>ian_cancercomputer:</b></span> Thanks!  Its good to know I haven't been totally messing things up.<br/>
<span style="color: #e96699"><span style="font-size: small">(15:17:16)</span> <b>ian_cancercomputer:</b></span> SO, I haven't bumped to 3.3.21 yet, anything to be concened about while pilot jobs are still running?<br/>
<span style="color: #b14cbc"><span style="font-size: small">(15:23:04)</span> <b>tmartin:</b></span> No they have their own condor. If you restart the CE they will not care, if you restart a node they will restart<br/>
<span style="color: #b14cbc"><span style="font-size: small">(16:19:26)</span> <b>tmartin:</b></span> Welcome Kevin. :slightly_smiling_face:<br/>
<span style="color: #e0a729"><span style="font-size: small">(16:26:26)</span> <b>kevin_buterbaugh:</b></span> Hi Terrence...:sweat_smile:<br/>
</body>
</html>

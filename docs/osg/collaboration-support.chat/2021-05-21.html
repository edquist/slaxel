<!DOCTYPE html>
<html>
<head>
<title>Fri May 21, 2021 : #collaboration-support (osg)</title>
</head>
<body>
<h3>Fri May 21, 2021 : #collaboration-support (osg)</h3>
<span style="color: #a63024"><span style="font-size: small">(14:23:37)</span> <b>paschos:</b></span> for georgia tech--- i see cpu jobs flowing in but for OSG_US_GATech_osg-sched2_gpu --- jobs are held… @bbockelm @jpeterson or @marian can you let me know if you see somehting on your end that explains this? are pilots not matching still ?<br/>
<span style="color: #a63024"><span style="font-size: small">(14:24:44)</span> <b>paschos:</b></span> <a href="http://gfactory-2.opensciencegrid.org/factory/monitor/factoryEntryStatusNow.html?entry=OSG_US_GATech_osg-sched2_gpu">http://gfactory-2.opensciencegrid.org/factory/monitor/factoryEntryStatusNow.html?entry=OSG_US_GATech_osg-sched2_gpu</a><br/>
<span style="color: #a63024"><span style="font-size: small">(14:25:42)</span> <b>paschos:</b></span> @rynge as well as it is also on the OSG front end<br/>
<span style="color: #3c8c69"><span style="font-size: small">(14:32:54)</span> <b>jpeterson:</b></span> my first check of graphs don’t show anything running on either regular or GPU since the 18/19th<br/>
<span style="color: #3c8c69"><span style="font-size: small">(14:33:24)</span> <b>jpeterson:</b></span> a lot held for both<br/>
<span style="color: #3c8c69"><span style="font-size: small">(14:34:13)</span> <b>jpeterson:</b></span> <pre>gpu:<br/>6012843.0   gfactory        5/20 16:13 held remotely with no hold reason<br/>6013382.2   gfactory        5/20 17:46 held remotely with no hold reason<br/>6013466.5   gfactory        5/20 18:00 held remotely with no hold reason<br/>6014355.5   gfactory        5/20 20:42 held remotely with no hold reason<br/>6014371.2   gfactory        5/20 20:44 held remotely with no hold reason<br/>6014754.3   gfactory        5/20 21:55 held remotely with no hold reason<br/>6014805.5   gfactory        5/20 22:04 held remotely with no hold reason<br/>6014839.4   gfactory        5/20 22:11 held remotely with no hold reason<br/>6015092.3   gfactory        5/20 22:53 held remotely with no hold reason</pre><br/>
<span style="color: #3c8c69"><span style="font-size: small">(14:34:39)</span> <b>jpeterson:</b></span> same for non-gpu on the OSG factory<br/>
<span style="color: #a63024"><span style="font-size: small">(14:44:52)</span> <b>paschos:</b></span> yeah — i had the wrong cpu liink for gatech--- has the factory removed the hold due to their maintenance ? i was under the impression that they put in a cancel request<br/>
<span style="color: #a63024"><span style="font-size: small">(14:49:19)</span> <b>paschos:</b></span> yeah i still see EndTime: May 22, 2021 04:00 +0000 in <a href="https://github.com/opensciencegrid/topology/blob/b6255c4732da16fae9713f37737864ea6560d791/topology/Georgia%20Institute%20of%20Technology/Georgia%20Tech/Georgia%20Tech%20PACE%20OSG%202_downtime.yaml#L122">https://github.com/opensciencegrid/topology/blob/b6255c4732da16fae9713f37737864ea6[…]gy/Georgia%20Tech/Georgia%20Tech%20PACE%20OSG%202_downtime.yaml</a><br/>
<span style="color: #3c8c69"><span style="font-size: small">(15:00:37)</span> <b>jpeterson:</b></span> a <tt>gwms-factory status</tt> on both says running.<br/>
<span style="color: #a63024"><span style="font-size: small">(15:55:29)</span> <b>paschos:</b></span> <a href="https://github.com/opensciencegrid/topology/pull/1842">https://github.com/opensciencegrid/topology/pull/1842</a><br/>
<span style="color: #a63024"><span style="font-size: small">(16:01:09)</span> <b>paschos:</b></span> there was pull request yesterday that declared the downttiime complete. what. does held remotely with no hold reason mean? is it on the factory or the site<br/>
<span style="color: #43761b"><span style="font-size: small">(17:02:14)</span> <b>blin:</b></span> @semir.sarajlic we're seeing<br/><pre>5116069.5   gfactory        5/21 14:34 held remotely with no hold reason</pre><br/>submitting to GT from the factory side<br/>
<span style="color: #43761b"><span style="font-size: small">(17:02:27)</span> <b>blin:</b></span> what does the CE/local batch show?<br/>
<span style="color: #a63024"><span style="font-size: small">(17:02:48)</span> <b>paschos:</b></span> @ruben.lara<br/>
<span style="color: #a63024"><span style="font-size: small">(17:02:53)</span> <b>paschos:</b></span> @mehmet.belgin<br/>
<span style="color: #a63024"><span style="font-size: small">(17:03:34)</span> <b>paschos:</b></span> Brian --- if you give a minute --- i can loogin and run commands<br/>
<span style="color: #c386df"><span style="font-size: small">(17:05:46)</span> <b>ruben.lara:</b></span> Any specific command for me to run?<br/>
<span style="color: #c386df"><span style="font-size: small">(17:05:59)</span> <b>ruben.lara:</b></span> <pre># condor_q<br/><br/><br/>-- Schedd: <a href="http://osg-login2.pace.gatech.edu">osg-login2.pace.gatech.edu</a> : &lt;128.61.254.78:28029?... @ 05/21/21 18:04:44<br/>OWNER     BATCH_NAME    SUBMITTED   DONE   RUN    IDLE  TOTAL JOB_IDS<br/>jclark308 ID: 360567   5/21 16:59   3528   1176    296   5000 360567.3528-4999<br/>ligo      ID: 360593   5/21 17:35      _      _      1      1 360593.0<br/>icecube   ID: 360594   5/21 17:35      _      _      1      1 360594.0<br/>ligo      ID: 360595   5/21 17:35      _      _      1      1 360595.0<br/>ligo      ID: 360596   5/21 17:35      _      _      1      1 360596.0<br/>osg       ID: 360597   5/21 17:35      _      _      1      1 360597.0<br/>osg       ID: 360598   5/21 17:35      _      _      1      1 360598.0<br/>osg       ID: 360599   5/21 17:35      _      _      1      1 360599.0<br/>icecube   ID: 360600   5/21 17:35      _      _      1      1 360600.0<br/>ligo      ID: 360601   5/21 17:35      _      _      1      1 360601.0<br/>icecube   ID: 360602   5/21 17:35      _      _      1      1 360602.0<br/>osg       ID: 360603   5/21 17:35      _      _      1      1 360603.0<br/>icecube   ID: 360604   5/21 17:35      _      _      1      1 360604.0<br/>icecube   ID: 360605   5/21 17:35      _      _      1      1 360605.0<br/>osg       ID: 360606   5/21 17:35      _      _      1      1 360606.0<br/>icecube   ID: 360607   5/21 17:35      _      _      1      1 360607.0<br/>icecube   ID: 360608   5/21 17:35      _      _      1      1 360608.0<br/>icecube   ID: 360609   5/21 17:35      _      _      1      1 360609.0<br/>icecube   ID: 360610   5/21 17:35      _      _      1      1 360610.0<br/>osg       ID: 360611   5/21 17:35      _      _      1      1 360611.0<br/>icecube   ID: 360612   5/21 17:35      _      _      1      1 360612.0<br/>ligo      ID: 360613   5/21 17:35      _      _      1      1 360613.0<br/>icecube   ID: 360614   5/21 17:35      _      _      1      1 360614.0<br/>icecube   ID: 360615   5/21 17:35      _      _      1      1 360615.0<br/>ligo      ID: 360616   5/21 17:35      _      _      1      1 360616.0<br/><br/>Total for query: 1496 jobs; 0 completed, 0 removed, 320 idle, 1176 running, 0 held, 0 suspended<br/>Total for all users: 1496 jobs; 0 completed, 0 removed, 320 idle, 1176 running, 0 held, 0 suspended</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(17:06:49)</span> <b>blin:</b></span> what about <tt>condor_ce_q</tt> on the CE?<br/>
<span style="color: #c386df"><span style="font-size: small">(17:07:24)</span> <b>ruben.lara:</b></span> mmmm… That would be on the container running the HTCondorCE?<br/>
<span style="color: #c386df"><span style="font-size: small">(17:10:23)</span> <b>ruben.lara:</b></span> <pre>[root@osg-hosted-ce-osg-gatech-dev-649669c769-225dq /]# condor_ce_q<br/><br/><br/>-- Schedd: <a href="http://osg-sched2.pace.gatech.edu">osg-sched2.pace.gatech.edu</a> : &lt;192.168.219.126:34004?... @ 05/21/21 22:10:06<br/>OWNER   BATCH_NAME    SUBMITTED   DONE   RUN    IDLE   HOLD  TOTAL JOB_IDS<br/>ligo    ID: 1010     5/20 23:07      _      _      _      1      1 1010.0<br/>ligo    ID: 1024     5/20 23:08      _      _      _      1      1 1024.0<br/>osg     ID: 3509     5/21 03:51      _      _      _      1      1 3509.0<br/>osg     ID: 3533     5/21 03:52      _      _      _      1      1 3533.0<br/>osg     ID: 3637     5/21 04:07      _      _      _      1      1 3637.0<br/>osg     ID: 3641     5/21 04:08      _      _      _      1      1 3641.0<br/>osg     ID: 3793     5/21 04:29      _      _      _      1      1 3793.0<br/>osg     ID: 3799     5/21 04:29      _      _      _      1      1 3799.0<br/>ligo    ID: 4522     5/21 06:08      _      _      _      1      1 4522.0<br/>ligo    ID: 4530     5/21 06:08      _      _      _      1      1 4530.0<br/>ligo    ID: 5176     5/21 07:48      _      _      _      1      1 5176.0<br/>ligo    ID: 5181     5/21 07:48      _      _      _      1      1 5181.0<br/>osg     ID: 5250     5/21 07:59      _      _      _      1      1 5250.0<br/>osg     ID: 5265     5/21 08:00      _      _      _      1      1 5265.0<br/>ligo    ID: 5311     5/21 08:08      _      _      _      1      1 5311.0<br/>ligo    ID: 5313     5/21 08:09      _      _      _      1      1 5313.0<br/>osg     ID: 5936     5/21 09:46      _      _      _      1      1 5936.0<br/>osg     ID: 5948     5/21 09:46      _      _      _      1      1 5948.0<br/>osg     ID: 7497     5/21 13:41      _      _      _      1      1 7497.0<br/>osg     ID: 7502     5/21 13:42      _      _      _      1      1 7502.0<br/>osg     ID: 7588     5/21 13:55      _      _      _      1      1 7588.0<br/>osg     ID: 7604     5/21 13:56      _      _      _      1      1 7604.0<br/>osg     ID: 7719     5/21 14:16      _      _      _      1      1 7719.0<br/>osg     ID: 7731     5/21 14:17      _      _      _      1      1 7731.0<br/>osg     ID: 7980     5/21 14:55      _      _      _      1      1 7980.0<br/>osg     ID: 7998     5/21 14:56      _      _      _      1      1 7998.0<br/>ligo    ID: 8755     5/21 16:56      _      _      _      1      1 8755.0<br/>ligo    ID: 8757     5/21 16:57      _      _      _      1      1 8757.0<br/>osg     ID: 9237     5/21 18:13      _      _      _      1      1 9237.0<br/>osg     ID: 9246     5/21 18:13      _      _      _      1      1 9246.0<br/>osg     ID: 9726     5/21 19:30      _      _      _      1      1 9726.0<br/>osg     ID: 9732     5/21 19:30      _      _      _      1      1 9732.0<br/>ligo    ID: 9914     5/21 19:58      _      _      _      1      1 9914.0<br/>ligo    ID: 9921     5/21 19:59      _      _      _      1      1 9921.0<br/>osg     ID: 10076    5/21 20:23      _      _      _      1      1 10076.0<br/>osg     ID: 10082    5/21 20:24      _      _      _      1      1 10082.0<br/>osg     ID: 10136    5/21 20:30      _      _      _      1      1 10136.0<br/>osg     ID: 10145    5/21 20:31      _      _      _      1      1 10145.0<br/>osg     ID: 10240    5/21 20:41      _      _      _      1      1 10240.0<br/>osg     ID: 10248    5/21 20:42      _      _      _      1      1 10248.0<br/>osg     ID: 10433    5/21 21:35      _      _      _      1      1 10433.0<br/>osg     ID: 10434    5/21 21:35      _      _      _      1      1 10434.0<br/>osg     ID: 10435    5/21 21:35      _      _      _      1      1 10435.0<br/>osg     ID: 10436    5/21 21:35      _      _      _      1      1 10436.0<br/>osg     ID: 10437    5/21 21:35      _      _      _      1      1 10437.0<br/>icecube ID: 10438    5/21 21:35      _      _      _      1      1 10438.0<br/>icecube ID: 10439    5/21 21:35      _      _      _      1      1 10439.0<br/>icecube ID: 10440    5/21 21:35      _      _      _      1      1 10440.0<br/>icecube ID: 10441    5/21 21:35      _      _      _      1      1 10441.0<br/>icecube ID: 10442    5/21 21:35      _      _      _      1      1 10442.0<br/>icecube ID: 10443    5/21 21:35      _      _      _      1      1 10443.0<br/>osg     ID: 10444    5/21 21:35      _      _      _      1      1 10444.0<br/>icecube ID: 10445    5/21 21:35      _      _      _      1      1 10445.0<br/>icecube ID: 10446    5/21 21:35      _      _      _      1      1 10446.0<br/>icecube ID: 10447    5/21 21:35      _      _      _      1      1 10447.0<br/>icecube ID: 10448    5/21 21:35      _      _      _      1      1 10448.0<br/>icecube ID: 10449    5/21 21:35      _      _      _      1      1 10449.0<br/>icecube ID: 10450    5/21 21:35      _      _      _      1      1 10450.0<br/>ligo    ID: 10451    5/21 21:35      _      _      _      1      1 10451.0<br/>ligo    ID: 10452    5/21 21:35      _      _      _      1      1 10452.0<br/>ligo    ID: 10453    5/21 21:35      _      _      _      1      1 10453.0<br/>ligo    ID: 10454    5/21 21:35      _      _      _      1      1 10454.0<br/>ligo    ID: 10455    5/21 21:35      _      _      _      1      1 10455.0<br/>ligo    ID: 10456    5/21 21:35      _      _      _      1      1 10456.0<br/>ligo    ID: 10457    5/21 21:35      _      _      _      1      1 10457.0<br/>osg     ID: 10458    5/21 21:35      _      _      _      1      1 10458.0<br/>icecube ID: 10459    5/21 21:35      _      _      _      1      1 10459.0<br/>ligo    ID: 10460    5/21 21:35      _      _      _      1      1 10460.0<br/>icecube ID: 10461    5/21 21:35      _      _      _      1      1 10461.0<br/>icecube ID: 10462    5/21 21:35      _      _      _      1      1 10462.0<br/>ligo    ID: 10463    5/21 21:35      _      _      _      1      1 10463.0<br/>icecube ID: 10464    5/21 21:35      _      _      _      1      1 10464.0<br/>icecube ID: 10465    5/21 21:35      _      _      _      1      1 10465.0<br/>icecube ID: 10466    5/21 21:35      _      _      _      1      1 10466.0<br/>icecube ID: 10467    5/21 21:35      _      _      _      1      1 10467.0<br/>osg     ID: 10468    5/21 21:35      _      _      _      1      1 10468.0<br/>icecube ID: 10469    5/21 21:35      _      _      _      1      1 10469.0<br/>osg     ID: 10470    5/21 21:35      _      _      _      1      1 10470.0<br/>icecube ID: 10471    5/21 21:35      _      _      _      1      1 10471.0<br/>osg     ID: 10472    5/21 21:35      _      _      _      1      1 10472.0<br/>ligo    ID: 10473    5/21 21:35      _      _      _      1      1 10473.0<br/>osg     ID: 10474    5/21 21:35      _      _      _      1      1 10474.0<br/>osg     ID: 10475    5/21 21:35      _      _      _      1      1 10475.0<br/>ligo    ID: 10476    5/21 21:35      _      _      _      1      1 10476.0<br/>ligo    ID: 10477    5/21 21:35      _      _      _      1      1 10477.0<br/>icecube ID: 10478    5/21 21:35      _      _      _      1      1 10478.0<br/>icecube ID: 10479    5/21 21:35      _      _      _      1      1 10479.0<br/>icecube ID: 10480    5/21 21:35      _      _      _      1      1 10480.0<br/>ligo    ID: 10481    5/21 22:07      _      _      _      1      1 10481.0<br/>ligo    ID: 10482    5/21 22:08      _      _      _      1      1 10482.0<br/>ligo    ID: 10483    5/21 22:09      _      _      _      1      1 10483.0<br/><br/>Total for query: 91 jobs; 0 completed, 0 removed, 0 idle, 0 running, 91 held, 0 suspended<br/>Total for all users: 91 jobs; 0 completed, 0 removed, 0 idle, 0 running, 91 held, 0 suspended</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(17:10:36)</span> <b>blin:</b></span> gotcha. what does <tt>condor_ce_q -held</tt> look like?<br/>
<span style="color: #c386df"><span style="font-size: small">(17:11:00)</span> <b>ruben.lara:</b></span> <pre>[root@osg-hosted-ce-osg-gatech-dev-649669c769-225dq /]# condor_ce_q -held<br/><br/><br/>-- Schedd: <a href="http://osg-sched2.pace.gatech.edu">osg-sched2.pace.gatech.edu</a> : &lt;192.168.219.126:34004?... @ 05/21/21 22:10:45<br/> ID       OWNER          HELD_SINCE  HOLD_REASON<br/> 1010.0   ligo            5/20 23:09 held remotely with no hold reason<br/> 1024.0   ligo            5/20 23:09 held remotely with no hold reason<br/> 3509.0   osg             5/21 03:53 held remotely with no hold reason<br/> 3533.0   osg             5/21 03:53 held remotely with no hold reason<br/> 3637.0   osg             5/21 04:09 held remotely with no hold reason<br/> 3641.0   osg             5/21 04:09 held remotely with no hold reason<br/> 3793.0   osg             5/21 04:31 held remotely with no hold reason<br/> 3799.0   osg             5/21 04:31 held remotely with no hold reason<br/> 4522.0   ligo            5/21 06:10 held remotely with no hold reason<br/> 4530.0   ligo            5/21 06:09 held remotely with no hold reason<br/> 5176.0   ligo            5/21 07:50 held remotely with no hold reason<br/> 5181.0   ligo            5/21 07:50 held remotely with no hold reason<br/> 5250.0   osg             5/21 08:01 held remotely with no hold reason<br/> 5265.0   osg             5/21 08:01 held remotely with no hold reason<br/> 5311.0   ligo            5/21 08:10 held remotely with no hold reason<br/> 5313.0   ligo            5/21 08:10 held remotely with no hold reason<br/> 5936.0   osg             5/21 09:48 held remotely with no hold reason<br/> 5948.0   osg             5/21 09:48 held remotely with no hold reason<br/> 7497.0   osg             5/21 13:44 held remotely with no hold reason<br/> 7502.0   osg             5/21 13:43 held remotely with no hold reason<br/> 7588.0   osg             5/21 13:57 held remotely with no hold reason<br/> 7604.0   osg             5/21 13:57 held remotely with no hold reason<br/> 7719.0   osg             5/21 14:18 held remotely with no hold reason<br/> 7731.0   osg             5/21 14:18 held remotely with no hold reason<br/> 7980.0   osg             5/21 14:57 held remotely with no hold reason<br/> 7998.0   osg             5/21 14:57 held remotely with no hold reason<br/> 8755.0   ligo            5/21 16:58 held remotely with no hold reason<br/> 8757.0   ligo            5/21 16:58 held remotely with no hold reason<br/> 9237.0   osg             5/21 18:15 held remotely with no hold reason<br/> 9246.0   osg             5/21 18:14 held remotely with no hold reason<br/> 9726.0   osg             5/21 19:32 held remotely with no hold reason<br/> 9732.0   osg             5/21 19:32 held remotely with no hold reason<br/> 9914.0   ligo            5/21 20:00 held remotely with no hold reason<br/> 9921.0   ligo            5/21 20:00 held remotely with no hold reason<br/>10076.0   osg             5/21 20:25 held remotely with no hold reason<br/>10082.0   osg             5/21 20:25 held remotely with no hold reason<br/>10136.0   osg             5/21 20:32 held remotely with no hold reason<br/>10145.0   osg             5/21 20:32 held remotely with no hold reason<br/>10240.0   osg             5/21 20:43 held remotely with no hold reason<br/>10248.0   osg             5/21 20:43 held remotely with no hold reason<br/>10433.0   osg             5/21 22:07 held remotely with no hold reason<br/>10434.0   osg             5/21 22:07 held remotely with no hold reason<br/>10435.0   osg             5/21 22:07 held remotely with no hold reason<br/>10436.0   osg             5/21 22:07 held remotely with no hold reason<br/>10437.0   osg             5/21 22:07 held remotely with no hold reason<br/>10438.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10439.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10440.0   icecube         5/21 22:08 held remotely with no hold reason<br/>10441.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10442.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10443.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10444.0   osg             5/21 22:07 held remotely with no hold reason<br/>10445.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10446.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10447.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10448.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10449.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10450.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10451.0   ligo            5/21 22:08 held remotely with no hold reason<br/>10452.0   ligo            5/21 22:07 held remotely with no hold reason<br/>10453.0   ligo            5/21 22:07 held remotely with no hold reason<br/>10454.0   ligo            5/21 22:07 held remotely with no hold reason<br/>10455.0   ligo            5/21 22:07 held remotely with no hold reason<br/>10456.0   ligo            5/21 22:07 held remotely with no hold reason<br/>10457.0   ligo            5/21 22:06 held remotely with no hold reason<br/>10458.0   osg             5/21 22:06 held remotely with no hold reason<br/>10459.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10460.0   ligo            5/21 22:06 held remotely with no hold reason<br/>10461.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10462.0   icecube         5/21 22:06 held remotely with no hold reason<br/>10463.0   ligo            5/21 22:06 held remotely with no hold reason<br/>10464.0   icecube         5/21 22:06 held remotely with no hold reason<br/>10465.0   icecube         5/21 22:06 held remotely with no hold reason<br/>10466.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10467.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10468.0   osg             5/21 22:07 held remotely with no hold reason<br/>10469.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10470.0   osg             5/21 22:06 held remotely with no hold reason<br/>10471.0   icecube         5/21 22:06 held remotely with no hold reason<br/>10472.0   osg             5/21 22:07 held remotely with no hold reason<br/>10473.0   ligo            5/21 22:07 held remotely with no hold reason<br/>10474.0   osg             5/21 22:06 held remotely with no hold reason<br/>10475.0   osg             5/21 22:06 held remotely with no hold reason<br/>10476.0   ligo            5/21 22:07 held remotely with no hold reason<br/>10477.0   ligo            5/21 22:07 held remotely with no hold reason<br/>10478.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10479.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10480.0   icecube         5/21 22:07 held remotely with no hold reason<br/>10481.0   ligo            5/21 22:09 held remotely with no hold reason<br/>10482.0   ligo            5/21 22:09 held remotely with no hold reason<br/><br/>Total for query: 90 jobs; 0 completed, 0 removed, 0 idle, 0 running, 90 held, 0 suspended<br/>Total for all users: 104 jobs; 0 completed, 0 removed, 14 idle, 0 running, 90 held, 0 suspended</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(17:11:44)</span> <b>blin:</b></span> err, and you ran <tt>condor_q</tt> from the host that the CE is submitting into?<br/>
<span style="color: #c386df"><span style="font-size: small">(17:12:03)</span> <b>ruben.lara:</b></span> Yes.<br/>
<span style="color: #c386df"><span style="font-size: small">(17:12:12)</span> <b>ruben.lara:</b></span> <pre>[root@osg-login2 condor]# condor_q<br/><br/><br/>-- Schedd: <a href="http://osg-login2.pace.gatech.edu">osg-login2.pace.gatech.edu</a> : &lt;128.61.254.78:28029?... @ 05/21/21 18:11:18<br/>OWNER     BATCH_NAME    SUBMITTED   DONE   RUN    IDLE   HOLD  TOTAL JOB_IDS<br/>jclark308 ID: 360567   5/21 16:59   4704    296      _      _   5000 360567.4704-4999<br/>icecube   ID: 360618   5/21 18:10      _      _      _      1      1 360618.0<br/>icecube   ID: 360619   5/21 18:10      _      _      _      1      1 360619.0<br/>ligo      ID: 360620   5/21 18:10      _      _      _      1      1 360620.0<br/>icecube   ID: 360621   5/21 18:10      _      _      _      1      1 360621.0<br/>icecube   ID: 360622   5/21 18:10      _      _      _      1      1 360622.0<br/>icecube   ID: 360623   5/21 18:10      _      _      _      1      1 360623.0<br/>icecube   ID: 360624   5/21 18:10      _      _      _      1      1 360624.0<br/><br/>Total for query: 303 jobs; 0 completed, 0 removed, 0 idle, 296 running, 7 held, 0 suspended<br/>Total for all users: 303 jobs; 0 completed, 0 removed, 0 idle, 296 running, 7 held, 0 suspended</pre><br/>
<span style="color: #c386df"><span style="font-size: small">(17:13:17)</span> <b>ruben.lara:</b></span> The queue is clean, BTW:<br/><pre>[root@osg-login2 condor]# condor_q<br/><br/><br/>-- Schedd: <a href="http://osg-login2.pace.gatech.edu">osg-login2.pace.gatech.edu</a> : &lt;128.61.254.78:28029?... @ 05/21/21 18:12:57<br/>OWNER     BATCH_NAME    SUBMITTED   DONE   RUN    IDLE  TOTAL JOB_IDS<br/>jclark308 ID: 360567   5/21 16:59   4704    296      _   5000 360567.4704-4999<br/><br/>Total for query: 296 jobs; 0 completed, 0 removed, 0 idle, 296 running, 0 held, 0 suspended<br/>Total for all users: 296 jobs; 0 completed, 0 removed, 0 idle, 296 running, 0 held, 0 suspended</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(17:13:46)</span> <b>blin:</b></span> we should open a ticket for this at this point. i'm curious what the hold reasons are for those jobs but i imagine that we'll see more in /var/log/condor/SchedLog on the login host<br/>
<span style="color: #c386df"><span style="font-size: small">(17:13:47)</span> <b>ruben.lara:</b></span> But on the CE the number of held jobs is steady at 84.<br/>
<span style="color: #c386df"><span style="font-size: small">(17:16:41)</span> <b>ruben.lara:</b></span> For one of the jobs that was held:<br/><pre>The Requirements expression for job 360631.000 is<br/><br/>    (TARGET.Arch == "X86_64") &amp;&amp; (TARGET.OpSys == "LINUX") &amp;&amp; (TARGET.Disk &gt;= RequestDisk) &amp;&amp;<br/>    (TARGET.Memory &gt;= RequestMemory) &amp;&amp; (TARGET.HasFileTransfer)<br/><br/>Job 360631.000 defines the following attributes:<br/><br/>    RequestDisk = 1<br/>    RequestMemory = 4000<br/><br/>The Requirements expression for job 360631.000 reduces to these conditions:<br/><br/>         Slots<br/>Step    Matched  Condition<br/>-----  --------  ---------<br/>[0]         345  TARGET.Arch == "X86_64"<br/>[1]         345  TARGET.OpSys == "LINUX"<br/>[3]         345  TARGET.Disk &gt;= RequestDisk<br/>[5]          49  TARGET.Memory &gt;= RequestMemory<br/><br/><br/>360631.000:  Job is held.<br/><br/>Hold reason: The job attribute PeriodicRemove expression 'JobStatus == 2 &amp;&amp; time() - JobCurrentStartExecutingDate &gt; 86400' evaluated to UNDEFINED<br/><br/>Last successful match: Fri May 21 18:15:00 2021<br/><br/><br/>360631.000:  Run analysis summary ignoring user priority.  Of 49 machines,<br/>      0 are rejected by your job's requirements<br/>      0 reject your job because of their own requirements<br/>      0 match and are already running your jobs<br/>      0 match but are serving other users<br/>     49 are able to run your job</pre><br/>
<span style="color: #c386df"><span style="font-size: small">(17:16:50)</span> <b>ruben.lara:</b></span> After a few moments, it ran.<br/>
<span style="color: #a63024"><span style="font-size: small">(17:17:33)</span> <b>paschos:</b></span> @blin i see message like this one on Schedlog:<br/>5/21/21 18:15:01 (D_ALWAYS) Putting job 360625.0 on *hold*<br/>05/21/21 18:15:01 (D_ALWAYS) Putting job 360626.0 on *hold*<br/>05/21/21 18:15:01 (D_ALWAYS) Putting job 360627.0 on *hold*<br/>05/21/21 18:15:01 (D_ALWAYS) Putting job 360628.0 on *hold*<br/>05/21/21 18:15:01 (D_ALWAYS) Putting job 360631.0 on *hold*<br/>05/21/21 18:15:01 (D_ALWAYS) Putting job 360630.0 on *hold*<br/>05/21/21 18:15:01 (D_ALWAYS) Putting job 360629.0 on *hold*<br/>05/21/21 18:15:57 (D_ALWAYS:2) abort_job_myself: 360625.0 action:Remove log_*hold*:true<br/>05/21/21 18:15:58 (D_ALWAYS:2) abort_job_myself: 360627.0 action:Remove log_*hold*:true<br/>05/21/21 18:15:58 (D_ALWAYS:2) abort_job_myself: 360631.0 action:Remove log_*hold*:true<br/>05/21/21 18:15:58 (D_ALWAYS:2) abort_job_myself: 360628.0 action:Remove log_*hold*:true<br/>05/21/21 18:15:58 (D_ALWAYS:2) abort_job_myself: 360630.0 action:Remove log_*hold*:true<br/>05/21/21 18:15:58 (D_ALWAYS:2) abort_job_myself: 360626.0 action:Remove log_*hold*:true<br/>05/21/21 18:15:59 (D_ALWAYS:2) abort_job_myself: 360629.0 action:Remove log_*hold*:true<br/>
<span style="color: #43761b"><span style="font-size: small">(17:18:39)</span> <b>blin:</b></span> i'd be curious to see messages in between. seems like jobs are thrashing<br/>
<span style="color: #a63024"><span style="font-size: small">(17:19:38)</span> <b>paschos:</b></span> cat /var/log/condor/SchedLog | grep 360618<br/>05/21/21 18:10:31 (cid:121915) (D_AUDIT) Submitting new job *360618*.0<br/>05/21/21 18:10:31 (D_ALWAYS:2) schedd: NewCluster rval *360618* errno 0<br/>05/21/21 18:10:31 (D_SECURITY) ATTRS: SetAttribute *360618*.-1 x509UserProxyFQAN=“/DC=org/DC=incommon/C=US/ST=Wisconsin/L=Madison/O=University of Wisconsin-Madison/OU=OCIS/CN=<a href="http://vo-frontend-glow.chtc.wisc.edu">vo-frontend-glow.chtc.wisc.edu</a>,/icecube/Role=pilot/Capability=NULL,/icecube/Role=NULL/Capability=NULL”<br/>05/21/21 18:10:31 (D_SECURITY) ATTRS: SetAttribute *360618*.-1 x509UserProxyFirstFQAN=“/icecube/Role=pilot/Capability=NULL”<br/>05/21/21 18:10:31 (D_SECURITY) ATTRS: SetAttribute *360618*.-1 x509UserProxyVOName=“icecube”<br/>05/21/21 18:10:31 (D_SECURITY) ATTRS: SetAttribute *360618*.-1 x509userproxysubject=“/DC=org/DC=incommon/C=US/ST=Wisconsin/L=Madison/O=University of Wisconsin-Madison/OU=OCIS/CN=<a href="http://vo-frontend-glow.chtc.wisc.edu">vo-frontend-glow.chtc.wisc.edu</a>”<br/>05/21/21 18:10:31 (D_SECURITY) ATTRS: SetAttribute *360618*.-1 x509UserProxyExpiration=1621721431<br/>05/21/21 18:10:31 (D_ALWAYS:2) (*360618*.0) job_transforms: Setting LigoSearchTag = “None”<br/>05/21/21 18:10:31 (D_ALWAYS:2) (*360618*.0) job_transforms: Setting LigoSearchUser = “icecube”<br/>05/21/21 18:10:31 (D_ALWAYS) job_transforms for *360618*.0: 2 considered, 2 applied (TagJob,RemoveAcctGroup)<br/>05/21/21 18:10:31 (D_ALWAYS:2) New job: *360618*.0<br/>05/21/21 18:10:31 (D_ALWAYS:2) New job: *360618*.0, Duplicate Keys: 2, Total Keys: 5 <br/>05/21/21 18:10:36 (D_ALWAYS:2) Sent job *360618*.0 (autocluster=17 resources_requested=2) to the negotiator<br/>05/21/21 18:10:36 (D_ALWAYS:2) Partitionable slot <a href="mailto:slot1@atl1-1-02-014-13-l.pace.gatech.edu">slot1@atl1-1-02-014-13-l.pace.gatech.edu</a> adjusted for job *360618*.0: cpus = 1, memory = 4000, disk = 14094476<br/>05/21/21 18:10:36 (D_ALWAYS:2) Job *360618*.0: is runnable<br/>05/21/21 18:10:36 (D_ALWAYS:2) record for job *360618*.0 skipped until PrioRec rebuild (already matched)<br/>05/21/21 18:10:36 (D_ALWAYS:2) Scheduler::start_std - job=*360618*.0 on &lt;172.27.147.110:9618?addrs=172.27.147.110-9618&amp;noUDP&amp;sock=2139_c12a_3&gt;<br/>05/21/21 18:10:36 (D_ALWAYS:2) Cleared dirty attributes for job *360618*.0<br/>05/21/21 18:10:36 (D_ALWAYS:2) Queueing job *360618*.0 in runnable job queue<br/>05/21/21 18:10:36 (D_ALWAYS:2) Match (<a href="mailto:slot1@atl1-1-02-014-13-l.pace.gatech.edu">slot1@atl1-1-02-014-13-l.pace.gatech.edu</a> &lt;172.27.147.110:9618?addrs=172.27.147.110-9618&amp;noUDP&amp;sock=2139_c12a_3&gt;#1621452701#5201#... for icecube) - running *360618*.0<br/>05/21/21 18:10:36 (D_ALWAYS:2) Job prep for *360618*.0 will not block, calling aboutToSpawnJobHandler() directly<br/>05/21/21 18:10:36 (D_ALWAYS:2) aboutToSpawnJobHandler() completed for job *360618*.0, attempting to spawn job handler<br/>05/21/21 18:10:36 (D_ALWAYS) Starting add_shadow_birthdate(*360618*.0)<br/>05/21/21 18:10:36 (D_ALWAYS:2) Added shadow record for PID 1436350, job (*360618*.0)<br/>05/21/21 18:10:36 (D_ALWAYS) Started shadow for job *360618*.0 on <a href="mailto:slot1@atl1-1-02-014-13-l.pace.gatech.edu">slot1@atl1-1-02-014-13-l.pace.gatech.edu</a> &lt;172.27.147.110:9618?addrs=172.27.147.110-9618&amp;noUDP&amp;sock=2139_c12a_3&gt;#1621452701#5201#... for icecube, (shadow pid = 1436350)<br/>05/21/21 18:10:36 (D_ALWAYS:2) TransferQueueManager: enqueueing &lt;128.61.254.78:1533&gt; uploading job *360618*.0 for Owner_icecube (sandbox size 0.087635 MB, initial file /storage/home/hicecube1/icecube/bosco/sandbox/0b3b/0b3b2d3e/osg-sched2.pace.gatech.edu_<a href="http://9619_osg-sched2.pace.gatech.edu">9619_osg-sched2.pace.gatech.edu</a>_10492.0_1621635022/OSG_US_GATech_osg-sched2_gpu.idtoken).<br/>05/21/21 18:10:36 (D_ALWAYS:2) TransferQueueManager: sending GoAhead to &lt;128.61.254.78:1533&gt; uploading job *360618*.0 for Owner_icecube (sandbox size 0.087635 MB, initial file /storage/home/hicecube1/icecube/bosco/sandbox/0b3b/0b3b2d3e/osg-sched2.pace.gatech.edu_<a href="http://9619_osg-sched2.pace.gatech.edu">9619_osg-sched2.pace.gatech.edu</a>_10492.0_1621635022/OSG_US_GATech_osg-sched2_gpu.idtoken).<br/>05/21/21 18:10:36 (D_ALWAYS:2) TransferQueueManager: dequeueing &lt;128.61.254.78:1533&gt; uploading job *360618*.0 for Owner_icecube (sandbox size 0.087635 MB, initial file /storage/home/hicecube1/icecube/bosco/sandbox/0b3b/0b3b2d3e/osg-sched2.pace.gatech.edu_<a href="http://9619_osg-sched2.pace.gatech.edu">9619_osg-sched2.pace.gatech.edu</a>_10492.0_1621635022/OSG_US_GATech_osg-sched2_gpu.idtoken).<br/>05/21/21 18:10:37 (D_ALWAYS) Shadow pid 1436350 for job *360618*.0 exited with status 112<br/>
<span style="color: #a63024"><span style="font-size: small">(17:19:57)</span> <b>paschos:</b></span> for one of the jobs that is on hold<br/>
<span style="color: #c386df"><span style="font-size: small">(17:21:04)</span> <b>ruben.lara:</b></span> <br/>
<span style="color: #43761b"><span style="font-size: small">(17:24:35)</span> <b>blin:</b></span> what version of condor's installed on this host?<br/>
<span style="color: #c386df"><span style="font-size: small">(17:24:52)</span> <b>ruben.lara:</b></span> <pre>[root@osg-login2 condor]# condor_version<br/>$CondorVersion: 9.0.1 May 18 2021 PackageID: 9.0.1-1 $<br/>$CondorPlatform: X86_64-CentOS_7.9 $</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(17:26:06)</span> <b>blin:</b></span> this isn't great:<br/><pre>  15589:05/21/21 18:17:22 (D_ALWAYS) Shadow pid 1440285 for job 360632.0 exited with status 112</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(17:26:11)</span> <b>blin:</b></span> what's in the ShadowLog?<br/>
<span style="color: #c386df"><span style="font-size: small">(17:29:38)</span> <b>ruben.lara:</b></span> <br/>
<span style="color: #c386df"><span style="font-size: small">(17:32:40)</span> <b>ruben.lara:</b></span> <pre>05/21/21 18:17:21 (D_ALWAYS:2) (360632.0) (1440285): Updating Job Queue: SetAttribute(HoldReason = "The job attribute PeriodicRemove expression 'JobStatus == 2 &amp;&amp; time() - JobCurrentStartExecutingDate &gt; 86400' evaluated to UNDEFINED")</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(17:33:22)</span> <b>blin:</b></span> ok, that's a CE/tarball problem<br/>
<span style="color: #43761b"><span style="font-size: small">(17:33:38)</span> <b>blin:</b></span> what CE tag + TarballURL are you using?<br/>
<span style="color: #c386df"><span style="font-size: small">(17:34:30)</span> <b>ruben.lara:</b></span> One moment…<br/>
<span style="color: #a63024"><span style="font-size: small">(17:34:32)</span> <b>paschos:</b></span> <pre>Containers:<br/>      osg-hosted-ce<br/>        State: running since 2021-05-19T20:56:19Z<br/>        Ready: true<br/>        Restarts: 0<br/>        Image: opensciencegrid/hosted-ce:release-20210502-0134<br/>        ImageID: fa22229a5506440bc0d1243c03cb5083b7f8dc15ecc7decd4ed51fdf237f8edc</pre><br/><br/>
<span style="color: #43761b"><span style="font-size: small">(17:35:39)</span> <b>blin:</b></span> yeah, i wouldn't use that one. if you really want an immutable tag, you need to use a more up-to-date one<br/>
<span style="color: #43761b"><span style="font-size: small">(17:35:48)</span> <b>blin:</b></span> if you don't care about immutability, just use <tt>release</tt><br/>
<span style="color: #c386df"><span style="font-size: small">(17:36:49)</span> <b>ruben.lara:</b></span> mmmm….<br/>
<span style="color: #43761b"><span style="font-size: small">(17:37:19)</span> <b>blin:</b></span> but just as important: what <tt>TarballURL</tt> is being used?<br/>
<span style="color: #43761b"><span style="font-size: small">(17:37:45)</span> <b>blin:</b></span> you should be using the May 2 one from here that corresponds to your OS: <a href="https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/daily/">https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/daily/</a><br/>
<span style="color: #a63024"><span style="font-size: small">(17:37:45)</span> <b>paschos:</b></span> followed this recommendation frm the ticket “redeploy with the opensciencegrid/hosted-ce:release image and use the appropriate 20210502”<br/>
<span style="color: #43761b"><span style="font-size: small">(17:38:46)</span> <b>blin:</b></span> yup, the <tt>release</tt> image + 20210502 daily from <a href="https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/daily/">https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/daily/</a><br/>
<span style="color: #a63024"><span style="font-size: small">(17:38:51)</span> <b>paschos:</b></span> ahh<br/>
<span style="color: #c386df"><span style="font-size: small">(17:38:58)</span> <b>ruben.lara:</b></span> We only changed the tag. We didn’t specify a tarball.<br/>
<span style="color: #c386df"><span style="font-size: small">(17:39:17)</span> <b>ruben.lara:</b></span> Can you provide the parameter to use in the YAML file?<br/>
<span style="color: #43761b"><span style="font-size: small">(17:39:18)</span> <b>blin:</b></span> that functionality was added recently so you may not have it in your values file<br/>
<span style="color: #c386df"><span style="font-size: small">(17:39:27)</span> <b>ruben.lara:</b></span> Indeed, we don’t.<br/>
<span style="color: #43761b"><span style="font-size: small">(17:39:55)</span> <b>blin:</b></span> <pre>BoscoOverrides:<br/>  [...]<br/>  # Bosco tarball URL to use for installation on the remote login host<br/>  # instead of the default determined by the Hosted CE<br/>  TarballURL: null</pre><br/><br/>
<span style="color: #43761b"><span style="font-size: small">(17:40:23)</span> <b>blin:</b></span> when we release condor 9.0.1, you should be able to unset that tarball URL since 9.0.1 should have the necessary bug fixes<br/>
<span style="color: #c386df"><span style="font-size: small">(17:40:55)</span> <b>ruben.lara:</b></span> I have this:<br/><pre>BoscoOverrides:<br/>  Enabled: false<br/>  GitEndpoint: <a href="https://github.com/slateci/bosco-override-template">https://github.com/slateci/bosco-override-template</a><br/>  # If GitEndpoint requires authentication, create a SLATE secret with<br/>  # 'git.key' containing the private SSH key that can access<br/>  # it. Specify the name of the secret in GitKeySecret:<br/>  GitKeySecret: null</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(17:41:18)</span> <b>blin:</b></span> yeah, go ahead and add the <tt>TarballURL</tt> param<br/>
<span style="color: #43761b"><span style="font-size: small">(17:41:22)</span> <b>blin:</b></span> what OS is your login host?<br/>
<span style="color: #c386df"><span style="font-size: small">(17:41:59)</span> <b>ruben.lara:</b></span> This one? <a href="https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/daily/condor-9.0.1-20210502-x86_64_CentOS7-stripped.tar.gz">https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/daily/condor-9.0.1-20210502-x86_64_CentOS7-stripped.tar.gz</a><br/>
<span style="color: #43761b"><span style="font-size: small">(17:42:19)</span> <b>blin:</b></span> yup!<br/>
<span style="color: #c386df"><span style="font-size: small">(17:42:28)</span> <b>ruben.lara:</b></span> The tag, should it be <tt>release</tt> only?<br/>
<span style="color: #43761b"><span style="font-size: small">(17:42:57)</span> <b>blin:</b></span> yeah, that's a good bet<br/>
<span style="color: #c386df"><span style="font-size: small">(17:43:09)</span> <b>ruben.lara:</b></span> ok. Rebuilding the CE now…<br/>
<span style="color: #c386df"><span style="font-size: small">(17:46:06)</span> <b>ruben.lara:</b></span> <pre>Pods:<br/>  osg-hosted-ce-osg-gatech-dev-6f5fcf4f57-stq7j<br/>    Status: Running<br/>    Created: 2021-05-21T22:45:12Z<br/>    Host: <a href="http://osg-slate-control.pace.gatech.edu">osg-slate-control.pace.gatech.edu</a><br/>    Host IP: 128.61.166.231<br/>    Conditions: [2021-05-21T22:45:12Z] Initialized<br/>                [2021-05-21T22:45:12Z] PodScheduled<br/>                [2021-05-21T22:45:28Z] Ready<br/>                [2021-05-21T22:45:28Z] ContainersReady<br/>    Events: Scheduled: Successfully assigned slate-group-gatech-dev/osg-hosted-ce-osg-gatech-dev-6f5<br/>            fcf4f57-stq7j to <a href="http://osg-slate-control.pace.gatech.edu">osg-slate-control.pace.gatech.edu</a><br/>            [2021-05-21T22:45:13Z] Pulling: Pulling image "opensciencegrid/hosted-ce:release"<br/>            [2021-05-21T22:45:27Z] Pulled: Successfully pulled image "opensciencegrid/hosted-ce:rele<br/>            ase"<br/>            [2021-05-21T22:45:27Z] Created: Created container osg-hosted-ce<br/>            [2021-05-21T22:45:27Z] Started: Started container osg-hosted-ce<br/>    Containers:<br/>      osg-hosted-ce<br/>        State: running since 2021-05-21T22:45:27Z<br/>        Ready: true<br/>        Restarts: 0<br/>        Image: opensciencegrid/hosted-ce:release<br/>        ImageID: c7b32aca29798fe5418cfc4ca99fe98e0171bfba8c648791f2d02796f2e2cf10</pre><br/>
<span style="color: #c386df"><span style="font-size: small">(17:46:20)</span> <b>ruben.lara:</b></span> <pre>ContainerTags:<br/>  HostedCE: release<br/>HTTPLogger:<br/>  Enabled: false<br/>VoRemoteUserMapping:<br/>  - !&lt;!&gt; /osg/Role=NULL/Capability=NULL: osg<br/>  - !&lt;!&gt; /osg/ligo/Role=NULL/Capability=NULL: ligo<br/>  - !&lt;!&gt; /virgo/ligo/Role=NULL/Capability=NULL: ligo<br/>  - !&lt;!&gt; /icecube/Role=pilot/Capability=NULL: icecube<br/>RemoteCluster:<br/>  MemoryPerNode: 196608<br/>  MaxWallTime: 86400<br/>  LoginHost: <a href="http://osg-login2.pace.gatech.edu">osg-login2.pace.gatech.edu</a><br/>  PrivateKeySecret: gatech-lp-hostedce-privkey<br/>  BoscoDir: bosco<br/>  GridDir: /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.5/current/el7-x86_64<br/>  WorkerNodeTemp: /scratch<br/>  Squid: 128.61.166.231:31200<br/>  Batch: condor<br/>  CoresPerNode: 24<br/>BoscoOverrides:<br/>  TarballURL: <a href="https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/daily/condor-9.0.1-20210502-x86_64_CentOS7-stripped.tar.gz">https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/daily/condor-9.0.1-20210502-x86_64_CentOS7-stripped.tar.gz</a><br/>  GitKeySecret: ~<br/>  Enabled: false<br/>  GitEndpoint: <a href="https://github.com/slateci/bosco-override-template">https://github.com/slateci/bosco-override-template</a></pre><br/>
<span style="color: #c386df"><span style="font-size: small">(17:48:57)</span> <b>ruben.lara:</b></span> <tt>condor-ce</tt> is not starting on the container…<br/>
<span style="color: #c386df"><span style="font-size: small">(17:49:15)</span> <b>ruben.lara:</b></span> I think we need to use the same tag that matches the tarball.<br/>
<span style="color: #43761b"><span style="font-size: small">(17:50:13)</span> <b>blin:</b></span> nope, that is not the case<br/>
<span style="color: #c386df"><span style="font-size: small">(17:50:31)</span> <b>ruben.lara:</b></span> ok. the CE didn’t start.<br/>
<span style="color: #43761b"><span style="font-size: small">(17:51:05)</span> <b>blin:</b></span> there are a lot of patches in the most recent CE image that were added so that it could work with that specific tarball<br/>
<span style="color: #43761b"><span style="font-size: small">(17:51:22)</span> <b>blin:</b></span> what do you see from the <tt>slate instance logs</tt>?<br/>
<span style="color: #43761b"><span style="font-size: small">(17:51:35)</span> <b>blin:</b></span> or if you have kubectl access <tt>kubectl logs -c osg-hosted-ce &lt;pod name&gt;</tt><br/>
<span style="color: #c386df"><span style="font-size: small">(17:54:22)</span> <b>ruben.lara:</b></span> One moment…<br/>
<span style="color: #c386df"><span style="font-size: small">(17:55:21)</span> <b>ruben.lara:</b></span> <br/>
<span style="color: #43761b"><span style="font-size: small">(17:56:14)</span> <b>blin:</b></span> that's the full log? it looks to me like it's still in the middle of setup<br/>
<span style="color: #43761b"><span style="font-size: small">(17:56:32)</span> <b>blin:</b></span> or alternatively it's hanging on ssh<br/><pre>++ ssh -o PasswordAuthentication=no -vvv <a href="mailto:ligo@osg-login2.pace.gatech.edu">ligo@osg-login2.pace.gatech.edu</a> pwd</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(17:57:23)</span> <b>blin:</b></span> the real worry is if you see the pod going into a CrashLoopBackOff<br/>
<span style="color: #c386df"><span style="font-size: small">(18:01:25)</span> <b>ruben.lara:</b></span> According to this, it’s running:<br/><pre>Pods:<br/>  osg-hosted-ce-osg-gatech-dev-6f5fcf4f57-stq7j<br/>    Status: Running<br/>    Created: 2021-05-21T22:45:12Z<br/>    Host: <a href="http://osg-slate-control.pace.gatech.edu">osg-slate-control.pace.gatech.edu</a><br/>    Host IP: 128.61.166.231<br/>    Conditions: [2021-05-21T22:45:12Z] Initialized<br/>                [2021-05-21T22:45:12Z] PodScheduled<br/>                [2021-05-21T22:45:28Z] Ready<br/>                [2021-05-21T22:45:28Z] ContainersReady<br/>    Events: Scheduled: Successfully assigned slate-group-gatech-dev/osg-hosted-ce-osg-gatech-dev-6f5fcf4f57-stq7j to osg-slate-control.pace.gat<br/>            <a href="http://ech.edu">ech.edu</a><br/>            [2021-05-21T22:45:13Z] Pulling: Pulling image "opensciencegrid/hosted-ce:release"<br/>            [2021-05-21T22:45:27Z] Pulled: Successfully pulled image "opensciencegrid/hosted-ce:release"<br/>            [2021-05-21T22:45:27Z] Created: Created container osg-hosted-ce<br/>            [2021-05-21T22:45:27Z] Started: Started container osg-hosted-ce<br/>    Containers:<br/>      osg-hosted-ce<br/>        State: running since 2021-05-21T22:45:27Z<br/>        Ready: true<br/>        Restarts: 0<br/>        Image: opensciencegrid/hosted-ce:release<br/>        ImageID: c7b32aca29798fe5418cfc4ca99fe98e0171bfba8c648791f2d02796f2e2cf10</pre><br/>
<span style="color: #c386df"><span style="font-size: small">(18:02:14)</span> <b>ruben.lara:</b></span> Full configuration:<br/><pre>Configuration:<br/>Instance: !&lt;!&gt; osg-gatech-dev<br/>Topology:<br/>  Resource: Georgia_Tech_PACE_CE_2<br/>  ResourceGroup: Georgia Tech PACE OSG 2<br/>  Latitude: 33.7756<br/>  Longitude: -84.3963<br/>  Contact: Andre McNeill<br/>  City: Atlanta<br/>  ContactEmail: <a href="mailto:andre.mcneill@gatech.edu">andre.mcneill@gatech.edu</a><br/>  Sponsor: ligo:100<br/>  Country: US<br/>HostCredentials:<br/>  HostKeySecret: gatech-hostedce-osg-sched-2-gridcert-key<br/>  HostCertSecret: gatech-hostedce-osg-sched-2-gridcert<br/>DnRemoteUserMapping:<br/>  - !&lt;!&gt; /DC=org/DC=cilogon/C=US/O=Georgia Institute of Technology/CN=Andre McNeill A95801: ligo<br/>  - !&lt;!&gt; /DC=org/DC=cilogon/C=US/O=Georgia Institute of Technology/CN=Andre McNeill A95801: osg<br/>Developer:<br/>  Enabled: false<br/>Networking:<br/>  Hostname: !&lt;!&gt; <a href="http://osg-sched2.pace.gatech.edu">osg-sched2.pace.gatech.edu</a><br/>  RequestIP: 128.61.166.235<br/>  ServiceType: !&lt;!&gt; LoadBalancer<br/>HTCondorCeConfig: !&lt;!&gt; "ALL_DEBUG = D_ALWAYS:2 D_CAT\n\n"<br/>ContainerTags:<br/>  HostedCE: release<br/>HTTPLogger:<br/>  Enabled: false<br/>VoRemoteUserMapping:<br/>  - !&lt;!&gt; /osg/Role=NULL/Capability=NULL: osg<br/>  - !&lt;!&gt; /osg/ligo/Role=NULL/Capability=NULL: ligo<br/>  - !&lt;!&gt; /virgo/ligo/Role=NULL/Capability=NULL: ligo<br/>  - !&lt;!&gt; /icecube/Role=pilot/Capability=NULL: icecube<br/>RemoteCluster:<br/>  MemoryPerNode: 196608<br/>  MaxWallTime: 86400<br/>  LoginHost: <a href="http://osg-login2.pace.gatech.edu">osg-login2.pace.gatech.edu</a><br/>  PrivateKeySecret: gatech-lp-hostedce-privkey<br/>  BoscoDir: bosco<br/>  GridDir: /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.5/current/el7-x86_64<br/>  WorkerNodeTemp: /scratch<br/>  Squid: 128.61.166.231:31200<br/>  Batch: condor<br/>  CoresPerNode: 24<br/>BoscoOverrides:<br/>  TarballURL: <a href="https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/daily/condor-9.0.1-20210502-x86_64_CentOS7-stripped.tar.gz">https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/daily/condor-9.0.1-20210502-x86_64_CentOS7-stripped.tar.gz</a><br/>  GitKeySecret: ~<br/>  Enabled: false<br/>  GitEndpoint: <a href="https://github.com/slateci/bosco-override-template">https://github.com/slateci/bosco-override-template</a></pre><br/>
<span style="color: #c386df"><span style="font-size: small">(18:06:31)</span> <b>ruben.lara:</b></span> Nothing.<br/>
<span style="color: #43761b"><span style="font-size: small">(18:07:19)</span> <b>blin:</b></span> the factory has treated the CE as effectively down, we may need @jdost321 to weigh in how things look from the factory side<br/>
<span style="color: #c386df"><span style="font-size: small">(18:07:46)</span> <b>ruben.lara:</b></span> <tt>condor_ce</tt> is NOT being configured in the container.<br/>
<span style="color: #c386df"><span style="font-size: small">(18:08:34)</span> <b>ruben.lara:</b></span> This is from within the container:<br/><pre>[root@osg-hosted-ce-osg-gatech-dev-6f5fcf4f57-7zwc6 condor-ce]# pwd<br/>/var/log/condor-ce<br/>[root@osg-hosted-ce-osg-gatech-dev-6f5fcf4f57-7zwc6 condor-ce]# ll<br/>total 4<br/>drwxrwxrwt 2 condor condor 4096 Apr 19 17:09 user</pre><br/>
<span style="color: #c386df"><span style="font-size: small">(18:09:07)</span> <b>ruben.lara:</b></span> Should I try with the tag matching the tar ball?<br/>
<span style="color: #43761b"><span style="font-size: small">(18:09:18)</span> <b>blin:</b></span> no, that will certainly not work<br/>
<span style="color: #43761b"><span style="font-size: small">(18:09:52)</span> <b>blin:</b></span> what does the tail of the CE instance logs look like?<br/>
<span style="color: #c386df"><span style="font-size: small">(18:10:32)</span> <b>ruben.lara:</b></span> <pre>++ echo Agent pid 756<br/>+ bosco_ssh_start<br/>+ '[' 0 -eq 0 ']'<br/>+ ssh_master_connection=759<br/>+ return 0<br/>+ ssh_opts='-o PasswordAuthentication=no'<br/>+ [[ 1 = 1 ]]<br/>+ ssh_opts='-o PasswordAuthentication=no -vvv'<br/>+ ssh -o ControlMaster=auto -o ControlPath=/tmp/bosco_ssh_control.%r@%h:%p -MNn <a href="mailto:ligo@osg-login2.pace.gatech.edu">ligo@osg-login2.pace.gatech.edu</a><br/>++ ssh -o PasswordAuthentication=no -vvv <a href="mailto:ligo@osg-login2.pace.gatech.edu">ligo@osg-login2.pace.gatech.edu</a> pwd</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(18:10:35)</span> <b>blin:</b></span> also, how about <tt>/var/log/supervisor/</tt> ? are there any htcondor-ce files in there?<br/>
<span style="color: #43761b"><span style="font-size: small">(18:10:49)</span> <b>blin:</b></span> yeah. something's stuck with ssh to the login host<br/>
<span style="color: #43761b"><span style="font-size: small">(18:11:35)</span> <b>blin:</b></span> <tt>pwd</tt> isn't returning anything over the SSH connection<br/>
<span style="color: #c386df"><span style="font-size: small">(18:12:06)</span> <b>ruben.lara:</b></span> Running manually, it does:<br/><pre>[root@osg-hosted-ce-osg-gatech-dev-6f5fcf4f57-7zwc6 /]# ssh -o PasswordAuthentication=no -vvv <a href="mailto:ligo@osg-login2.pace.gatech.edu">ligo@osg-login2.pace.gatech.edu</a> pwd<br/>OpenSSH_7.4p1, OpenSSL 1.0.2k-fips  26 Jan 2017<br/>debug1: Reading configuration data /etc/ssh/ssh_config<br/>debug1: /etc/ssh/ssh_config line 1: Applying options for <a href="http://osg-login2.pace.gatech.edu">osg-login2.pace.gatech.edu</a><br/>debug1: auto-mux: Trying existing master<br/>debug2: fd 3 setting O_NONBLOCK<br/>debug2: mux_client_hello_exchange: master version 4<br/>debug3: mux_client_forwards: request forwardings: 0 local, 0 remote<br/>debug3: mux_client_request_session: entering<br/>debug3: mux_client_request_alive: entering<br/>debug3: mux_client_request_alive: done pid = 767<br/>debug3: mux_client_request_session: session request sent<br/>debug1: mux_client_request_session: master session id: 2<br/>/storage/home/hligo2/ligo<br/>debug3: mux_client_read_packet: read header failed: Broken pipe<br/>debug2: Received exit status from master 0</pre><br/>
<span style="color: #c386df"><span style="font-size: small">(18:12:16)</span> <b>ruben.lara:</b></span> (that’s from inside the container)<br/>
<span style="color: #c386df"><span style="font-size: small">(18:13:16)</span> <b>ruben.lara:</b></span> <tt>/var/log/supervisor</tt> is empty.<br/>
<span style="color: #43761b"><span style="font-size: small">(18:13:26)</span> <b>blin:</b></span> could be a difference between ssh recognizing a tty<br/>
<span style="color: #c386df"><span style="font-size: small">(18:14:44)</span> <b>ruben.lara:</b></span> I see no errors on the other end (<tt>osg-login2</tt>)<br/>
<span style="color: #43761b"><span style="font-size: small">(18:14:53)</span> <b>blin:</b></span> in your values file, try setting<br/><pre>RemoteCluster:<br/>  [...]<br/>  SSHBatchMode: False</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(18:14:55)</span> <b>blin:</b></span> and redeploying<br/>
<span style="color: #c386df"><span style="font-size: small">(18:16:39)</span> <b>ruben.lara:</b></span> In progress.<br/>
<span style="color: #c386df"><span style="font-size: small">(18:19:02)</span> <b>ruben.lara:</b></span> Same result.<br/>
<span style="color: #43761b"><span style="font-size: small">(18:20:19)</span> <b>blin:</b></span> Whelp, I'm stumped. I'm not sure why that SSH command is hanging in setup but working fine manually<br/>
<span style="color: #a63024"><span style="font-size: small">(18:22:27)</span> <b>paschos:</b></span> there was a redeployment issue two days agoo due to a nologin that prevented non roott logins … with similar symptoms but this should have been removed<br/>
<span style="color: #c386df"><span style="font-size: small">(18:23:16)</span> <b>ruben.lara:</b></span> It has been removed.<br/>
<span style="color: #c386df"><span style="font-size: small">(18:23:27)</span> <b>ruben.lara:</b></span> May I try to go to the previous configuration?<br/>
<span style="color: #c386df"><span style="font-size: small">(18:23:57)</span> <b>ruben.lara:</b></span> No tarball, and tag = <tt>stable</tt><br/>
<span style="color: #a63024"><span style="font-size: small">(18:26:15)</span> <b>paschos:</b></span> it’s late — perhaps we can setup a screen share monday with jeff and brian? @blin would you mae some time?<br/>
<span style="color: #c386df"><span style="font-size: small">(18:26:43)</span> <b>ruben.lara:</b></span> I am the one that don’t have time on Monday.<br/>
<span style="color: #43761b"><span style="font-size: small">(18:26:56)</span> <b>blin:</b></span> You could give it a try but there be dragons. We don't use <tt>stable</tt> at all anymore and I'm not sure it has the latest security updates<br/>
<span style="color: #c386df"><span style="font-size: small">(18:27:32)</span> <b>ruben.lara:</b></span> I’m going to try with just <tt>release</tt> then, no tarball.<br/>
<span style="color: #43761b"><span style="font-size: small">(18:27:40)</span> <b>blin:</b></span> I don't have my calendar right now but I also believe that my Monday is tough<br/>
<span style="color: #c386df"><span style="font-size: small">(18:28:13)</span> <b>ruben.lara:</b></span> Could I try with a different tar ball? <a href="https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/release/">https://research.cs.wisc.edu/htcondor/tarball/9.0/9.0.1/release/</a>?<br/>
<span style="color: #c386df"><span style="font-size: small">(18:28:28)</span> <b>ruben.lara:</b></span> that one is newer.<br/>
<span style="color: #43761b"><span style="font-size: small">(18:29:02)</span> <b>blin:</b></span> Try <tt>testing</tt> with no tarball. I'm 99% sure <tt>release</tt> with no tarball doesn't work<br/>
<span style="color: #43761b"><span style="font-size: small">(18:30:13)</span> <b>blin:</b></span> <tt>testing</tt> will bring in that release tarball by default<br/>
<span style="color: #c386df"><span style="font-size: small">(18:31:31)</span> <b>ruben.lara:</b></span> ok…<br/>
<span style="color: #385a86"><span style="font-size: small">(18:32:02)</span> <b>jdost321:</b></span> from the factory side i see a single idle pilot but just as Brian said, factory thinks its down<br/>
<span style="color: #385a86"><span style="font-size: small">(18:32:05)</span> <b>jdost321:</b></span> <pre>000 (5116285.000.000) 05/21 15:44:35 Job submitted from host: &lt;169.228.38.43:9618?addrs=169.228.38.43-9618&amp;noUDP&amp;sock=2909650_76d3_6&gt;<br/>...<br/>020 (5116285.000.000) 05/21 15:45:14 Detected Down Globus Resource<br/>    RM-Contact: <a href="http://osg-sched2.pace.gatech.edu">osg-sched2.pace.gatech.edu</a><br/>...<br/>026 (5116285.000.000) 05/21 15:45:14 Detected Down Grid Resource<br/>    GridResource: condor <a href="http://osg-sched2.pace.gatech.edu">osg-sched2.pace.gatech.edu</a> <a href="http://osg-sched2.pace.gatech.edu:9619">osg-sched2.pace.gatech.edu:9619</a></pre><br/>
<span style="color: #385a86"><span style="font-size: small">(18:35:18)</span> <b>jdost321:</b></span> i'll check again later if you're redeploying at the moment<br/>
<span style="color: #c386df"><span style="font-size: small">(18:35:40)</span> <b>ruben.lara:</b></span> Redeployed.<br/>
<span style="color: #c386df"><span style="font-size: small">(18:35:45)</span> <b>ruben.lara:</b></span> Can you please take a look?<br/>
<span style="color: #c386df"><span style="font-size: small">(18:35:54)</span> <b>ruben.lara:</b></span> On this end, it looks more or less the same.<br/>
<span style="color: #385a86"><span style="font-size: small">(18:36:10)</span> <b>jdost321:</b></span> let me clear the factory queue to ensure fresh pilots are submitting<br/>
<span style="color: #385a86"><span style="font-size: small">(18:37:42)</span> <b>jdost321:</b></span> yeah, no improvement on the factory side<br/><pre>000 (5116361.000.000) 05/21 16:36:51 Job submitted from host: &lt;169.228.38.43:9618?addrs=169.228.38.43-9618&amp;noUDP&amp;sock=2909650_76d3_6&gt;<br/>...<br/>020 (5116361.000.000) 05/21 16:36:54 Detected Down Globus Resource<br/>    RM-Contact: <a href="http://osg-sched2.pace.gatech.edu">osg-sched2.pace.gatech.edu</a><br/>...<br/>026 (5116361.000.000) 05/21 16:36:54 Detected Down Grid Resource<br/>    GridResource: condor <a href="http://osg-sched2.pace.gatech.edu">osg-sched2.pace.gatech.edu</a> <a href="http://osg-sched2.pace.gatech.edu:9619">osg-sched2.pace.gatech.edu:9619</a></pre><br/>
<span style="color: #c386df"><span style="font-size: small">(18:41:11)</span> <b>ruben.lara:</b></span> I see connections for icecube and ligo on the submit node:<br/><pre>root     1486400       1  0 19:33 ?        00:00:00 sshd: icecube [priv]<br/>icecube  1486402 1486400  0 19:33 ?        00:00:00 sshd: icecube@notty<br/>root     1486426       1  0 19:33 ?        00:00:00 sshd: icecube [priv]<br/>icecube  1486465 1486426  0 19:33 ?        00:00:00 sshd: icecube@notty<br/>root     1486569       1  0 19:33 ?        00:00:00 sshd: ligo [priv]<br/>root     1486570       1  0 19:33 ?        00:00:00 sshd: ligo [priv]<br/>ligo     1486573 1486570  0 19:33 ?        00:00:00 sshd: ligo@notty<br/>ligo     1486574 1486569  0 19:33 ?        00:00:00 sshd: ligo@notty</pre><br/>
<span style="color: #c386df"><span style="font-size: small">(18:41:16)</span> <b>ruben.lara:</b></span> Nothing from osg.<br/>
<span style="color: #385a86"><span style="font-size: small">(18:42:28)</span> <b>jdost321:</b></span> i haven't seen the osg vo trying to submit from the factory side at the moment. what you see is the vos trying to submit that i'm seeing<br/>
<span style="color: #c386df"><span style="font-size: small">(18:43:36)</span> <b>ruben.lara:</b></span> @blin any other ideas? I can reboot the submit node.<br/>
<span style="color: #385a86"><span style="font-size: small">(18:43:49)</span> <b>jdost321:</b></span> i think i saw an osg connect outage notification they could be having other issues which would explain no osg vo pilots<br/>
<span style="color: #c386df"><span style="font-size: small">(18:45:31)</span> <b>ruben.lara:</b></span> @jdost321: that was removed a few hours ago.<br/>
<span style="color: #385a86"><span style="font-size: small">(18:47:48)</span> <b>jdost321:</b></span> i'm not sure if we're talking about the same outage? UChicago data center still seems to be having issues as far as i can tell <a href="https://status.opensciencegrid.org/">https://status.opensciencegrid.org/</a><br/>
<span style="color: #c386df"><span style="font-size: small">(18:50:37)</span> <b>ruben.lara:</b></span> I really need to leave now. As a side note, I tried to go back to the original tag (<tt>stable</tt>) and the result is the same.<br/>
<span style="color: #c386df"><span style="font-size: small">(18:56:16)</span> <b>ruben.lara:</b></span> My apologies. We’ll try to reconnect on monday. Nothing is working.<br/>
</body>
</html>

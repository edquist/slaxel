<!DOCTYPE html>
<html>
<head>
<title>Mon Mar 18, 2019 : #net-service (osg)</title>
</head>
<body>
<h3>Mon Mar 18, 2019 : #net-service (osg)</h3>
<span style="color: #a63024"><span style="font-size: small">(11:10:53)</span> <b>efajardo:</b></span> The problem from GEANT to Amsterdam Cache is fixed<br/>
<span style="color: #a63024"><span style="font-size: small">(11:10:56)</span> <b>efajardo:</b></span> <pre>MSG: ec: 0<br/>MSG: Closest XCACHE <a href="root://fiona.uvalight.net">root://fiona.uvalight.net</a><br/>MSG: tracepath :  <a href="http://fiona.uvalight.net">fiona.uvalight.net</a><br/>MSG:  1?: [LOCALHOST]                                         pmtu 1500<br/>MSG:  1:  <a href="http://lyon-corea.in2p3.fr">lyon-corea.in2p3.fr</a>                                   0.454ms <br/>MSG:  1:  <a href="http://lyon-corea.in2p3.fr">lyon-corea.in2p3.fr</a>                                   0.443ms <br/>MSG:  2:  <a href="http://lyon-ext-inter.in2p3.fr">lyon-ext-inter.in2p3.fr</a>                               0.608ms <br/>MSG:  3:  no reply<br/>MSG:  4:  193.51.180.20                                         0.469ms <br/>MSG:  5:  <a href="http://renater.mx1.gen.ch.geant.net">renater.mx1.gen.ch.geant.net</a>                          2.710ms <br/>MSG:  6:  <a href="http://ae1.mx1.fra.de.geant.net">ae1.mx1.fra.de.geant.net</a>                             11.237ms <br/>MSG:  7:  <a href="http://ae1.mx1.ams.nl.geant.net">ae1.mx1.ams.nl.geant.net</a>                             17.724ms <br/>MSG:  8:  <a href="http://surfnet-gw.mx1.ams.nl.geant.net">surfnet-gw.mx1.ams.nl.geant.net</a>                      17.772ms </pre><br/>
<span style="color: #a63024"><span style="font-size: small">(11:11:03)</span> <b>efajardo:</b></span> but now they have MTU 1500<br/>
<span style="color: #a63024"><span style="font-size: small">(11:11:08)</span> <b>efajardo:</b></span> :disappointed:<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(15:03:15)</span> <b>smckee:</b></span> @efajardo I guess that was ONE way to fix it (easiest but not best)<br/>
<span style="color: #a63024"><span style="font-size: small">(15:04:00)</span> <b>efajardo:</b></span> Well that is just one of the problems I am dealing<br/>
<span style="color: #a63024"><span style="font-size: small">(15:04:29)</span> <b>efajardo:</b></span> @smckee I have two hosts inside the i2 backbone not that far KC and Chicago both with 100Gbps networks<br/>
<span style="color: #a63024"><span style="font-size: small">(15:04:39)</span> <b>efajardo:</b></span> adn I cannot get to 30Gbps<br/>
<span style="color: #a63024"><span style="font-size: small">(15:04:42)</span> <b>efajardo:</b></span> <pre>[root@osg /]# iperf3 -4 -c <a href="http://osg.kans.nrp.internet2.edu">osg.kans.nrp.internet2.edu</a> -p 5201 -V -t 10<br/>iperf 3.1.7<br/>Linux <a href="http://osg.chic.nrp.internet2.edu">osg.chic.nrp.internet2.edu</a> 4.15.15-1.el7.x86_64 #1 SMP Thu Oct 4 07:42:41 UTC 2018 x86_64<br/>Control connection MSS 8948<br/>Time: Mon, 18 Mar 2019 19:25:41 GMT<br/>Connecting to host <a href="http://osg.kans.nrp.internet2.edu">osg.kans.nrp.internet2.edu</a>, port 5201<br/>      Cookie: osg.chic.nrp.internet2.edu.155293714<br/>      TCP MSS: 8948 (default)<br/>[  4] local 163.253.70.2 port 34722 connected to 163.253.71.2 port 5201<br/>Starting Test: protocol: TCP, 1 streams, 131072 byte blocks, omitting 0 seconds, 10 second test<br/>[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd<br/>[  4]   0.00-1.00   sec  2.67 GBytes  22.9 Gbits/sec    0    193 MBytes       <br/>[  4]   1.00-2.00   sec  2.99 GBytes  25.7 Gbits/sec  3666   59.8 MBytes       <br/>[  4]   2.00-3.00   sec  2.74 GBytes  23.6 Gbits/sec    0   59.9 MBytes       <br/>[  4]   3.00-4.00   sec  3.20 GBytes  27.5 Gbits/sec    0   60.2 MBytes       <br/>[  4]   4.00-5.00   sec  3.06 GBytes  26.3 Gbits/sec    0   60.7 MBytes       <br/>[  4]   5.00-6.00   sec  3.11 GBytes  26.7 Gbits/sec    0   61.3 MBytes       <br/>[  4]   6.00-7.00   sec  3.26 GBytes  28.0 Gbits/sec    0   62.0 MBytes       <br/>[  4]   7.00-8.00   sec  2.85 GBytes  24.5 Gbits/sec    0   63.2 MBytes       <br/>[  4]   8.00-9.00   sec  3.29 GBytes  28.3 Gbits/sec    0   64.8 MBytes       <br/>[  4]   9.00-10.00  sec  3.31 GBytes  28.4 Gbits/sec    0   66.2 MBytes       <br/>- - - - - - - - - - - - - - - - - - - - - - - - -<br/>Test Complete. Summary Results:<br/>[ ID] Interval           Transfer     Bandwidth       Retr<br/>[  4]   0.00-10.00  sec  30.5 GBytes  26.2 Gbits/sec  3666             sender<br/>[  4]   0.00-10.00  sec  30.5 GBytes  26.2 Gbits/sec                  receiver<br/>CPU Utilization: local/sender 90.9% (0.4%u/90.5%s), remote/receiver 23.5% (0.3%u/23.2%s)</pre><br/>
<span style="color: #4ec0d6"><span style="font-size: small">(15:05:15)</span> <b>smckee:</b></span> Try a 60 second test...<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(15:05:54)</span> <b>smckee:</b></span> Also note you are close to maxing out the CPU<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(15:06:35)</span> <b>smckee:</b></span> On that same path, it would be good to run a latency test to verify that it is "clean" in terms of packet loss.<br/>
<span style="color: #a63024"><span style="font-size: small">(15:09:10)</span> <b>efajardo:</b></span> hmm interesting<br/>
<span style="color: #a63024"><span style="font-size: small">(15:16:44)</span> <b>efajardo:</b></span> thanks @smckee I gave mroe cpus to the pods<br/>
<span style="color: #a63024"><span style="font-size: small">(15:17:10)</span> <b>efajardo:</b></span> but same result<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(16:24:07)</span> <b>smckee:</b></span> So check the FasterData page for some tunings:  <a href="http://fasterdata.es.net/host-tuning/linux/">http://fasterdata.es.net/host-tuning/linux/</a><br/>
<span style="color: #4ec0d6"><span style="font-size: small">(16:28:35)</span> <b>smckee:</b></span> @efajardo adding more CPUs for a single flow test won't (generally) help.  Instead you want to make sure you have the highest CPU clock speed on the core running the test.   More CPUs could be used by multi-flow tests.   Also which CPU is pinned to the transfer and where the interrupts are for the NIC may matter.<br/>
<span style="color: #a63024"><span style="font-size: small">(16:32:34)</span> <b>efajardo:</b></span> Well I tried with more<br/>
<span style="color: #a63024"><span style="font-size: small">(16:32:39)</span> <b>efajardo:</b></span> parallel<br/>
<span style="color: #a63024"><span style="font-size: small">(16:32:46)</span> <b>efajardo:</b></span> and that shows same result<br/>
<span style="color: #a63024"><span style="font-size: small">(16:32:58)</span> <b>efajardo:</b></span> but since I am doing this inside a pod<br/>
<span style="color: #a63024"><span style="font-size: small">(16:33:19)</span> <b>efajardo:</b></span> I am even less confortable that those can be achieved<br/>
<span style="color: #e96699"><span style="font-size: small">(16:44:31)</span> <b>lincoln:</b></span> Wonder if you are hitting pod networking limitations<br/>
<span style="color: #e96699"><span style="font-size: small">(16:45:58)</span> <b>lincoln:</b></span> I've only seen published #s for 10G<br/>
<span style="color: #a63024"><span style="font-size: small">(16:46:11)</span> <b>efajardo:</b></span> In theory I should not since I am doing <a href="https://github.com/opensciencegrid/prp-stashcache/blob/master/k8s/stashcache-greatplains.yaml#L15">https://github.com/opensciencegrid/prp-stashcache/blob/master/k8s/stashcache-greatplains.yaml#L15</a><br/>
<span style="color: #a63024"><span style="font-size: small">(16:46:22)</span> <b>efajardo:</b></span> I am binding to the hostnetwork<br/>
<span style="color: #e96699"><span style="font-size: small">(16:46:28)</span> <b>lincoln:</b></span> Ah ok<br/>
<span style="color: #a63024"><span style="font-size: small">(16:46:28)</span> <b>efajardo:</b></span> so should be no limits<br/>
<span style="color: #e96699"><span style="font-size: small">(16:46:37)</span> <b>lincoln:</b></span> :thumbsup: <br/>
<span style="color: #a63024"><span style="font-size: small">(16:46:39)</span> <b>efajardo:</b></span> but I am gonna aks the I2 folks<br/>
<span style="color: #a63024"><span style="font-size: small">(16:46:45)</span> <b>efajardo:</b></span> if they can do some tests in the baremetal<br/>
<span style="color: #a63024"><span style="font-size: small">(19:09:30)</span> <b>efajardo:</b></span> Btw @smckee the average packet loss is 0<br/>
<span style="color: #a63024"><span style="font-size: small">(19:09:31)</span> <b>efajardo:</b></span> <a href="https://perfsonar.nautilus.optiputer.net/maddash-webui/details.cgi?uri=/maddash/grids/Nautilus+Mesh+-+Latency+-+Loss/osg.chic.nrp.internet2.edu_internal/osg.kans.nrp.internet2.edu_internal/Packet+Loss">https://perfsonar.nautilus.optiputer.net/maddash-webui/details.cgi?uri=/maddash/grids/Nautilus+Mesh+-+Latency+-+Loss/osg.chic.nrp.internet2.edu_internal/osg.kans.nrp.internet2.edu_internal/Packet+Loss</a><br/>
</body>
</html>

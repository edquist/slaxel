<!DOCTYPE html>
<html>
<head>
<title>Thu Feb 14, 2019 : #net-service (osg)</title>
</head>
<body>
<h3>Thu Feb 14, 2019 : #net-service (osg)</h3>
<span style="color: #9e3997"><span style="font-size: small">(08:25:12)</span> <b>bbockelm:</b></span> (I'm OK with that)<br/>
<span style="color: #9e3997"><span style="font-size: small">(08:26:34)</span> <b>bbockelm:</b></span> @jthiltges -&gt; I don't have a SVG renderer.  Can you convert that to a 32x32 PNG and send it as a PR?<br/>
<span style="color: #e06b56"><span style="font-size: small">(09:32:26)</span> <b>jthiltges:</b></span> Right before the SVG is a PNG. And PR opened.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:16:20)</span> <b>dweitzel:</b></span> @smckee should we interactive debug here the network pipeline issue.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:16:37)</span> <b>dweitzel:</b></span> @ivukotic same ^<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:17:37)</span> <b>dweitzel:</b></span> so everything went up by a factor of ~9.  Including things not generated by us here in the U.S.<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:28:22)</span> <b>smckee:</b></span> Yes, it would be a good idea to do this.  I am on yet another meeting for another ~10 minutes but could participate after (till me next meeting at 12:30 PM Eastern)<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:33:37)</span> <b>dweitzel:</b></span> ok, looking back at my email, the 25th is when I said that we are now running a parallel data pipeline.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:33:44)</span> <b>dweitzel:</b></span> maybe it's not as parallel as I thought.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:33:46)</span> <b>dweitzel:</b></span> investigating<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:34:30)</span> <b>smckee:</b></span> We can update and refer to <a href="https://docs.google.com/document/d/1Zy27YC3Hg5_1he8Wehg2IR91PHmaduTLbieA_TrvbVU/edit#heading=h.ha7vjc1n4vrz">https://docs.google.com/document/d/1Zy27YC3Hg5_1he8Wehg2IR91PHmaduTLbieA_TrvbVU/edit#heading=h.ha7vjc1n4vrz</a> as we make progress<br/>
<span style="color: #84b22f"><span style="font-size: small">(10:35:00)</span> <b>ivukotic:</b></span> @smckee didn't we notice 3 owd measuring processes for one and the same link running at AGLT2. That means problem is not in collectors<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:35:32)</span> <b>smckee:</b></span> Hi Ilija, turns out there are 3 processes PER latency check<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:35:49)</span> <b>smckee:</b></span> However I am seeing 6 processes (two distinct latency tests):<br/>
<span style="color: #84b22f"><span style="font-size: small">(10:35:58)</span> <b>ivukotic:</b></span> :slightly_smiling_face:<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:36:21)</span> <b>smckee:</b></span> [root@psum01 ~]# ps auxww | grep uct2-net<br/>pschedu+ 2821600  0.0  0.0   7952   324 ?        Ss   11:34   0:00 /usr/bin/powstream -p -d /var/lib/pscheduler/tool/powstream/6bca5e14-3cc5-458e-8b76-b81f49b6228d-2019Feb13T165919504712 -P 8760-9960 -c 600 -i 0.1 -t -S <a href="http://psum01.aglt2.org">psum01.aglt2.org</a> [<a href="http://uct2-net1.mwt2.org">uct2-net1.mwt2.org</a>]:861<br/>pschedu+ 2822165  0.0  0.0   7952   324 ?        Ss   11:34   0:00 /usr/bin/powstream -p -d /var/lib/pscheduler/tool/powstream/c51bfc11-b299-40c4-8409-545ccda26cf0-2019Feb13T165919378200 -P 8760-9960 -c 600 -i 0.1 -t -S <a href="http://psum01.aglt2.org">psum01.aglt2.org</a> [<a href="http://uct2-net1.mwt2.org">uct2-net1.mwt2.org</a>]:861<br/>pschedu+ 2823584  0.0  0.0   7956   328 ?        Ss   11:35   0:00 /usr/bin/powstream -p -d /var/lib/pscheduler/tool/powstream/6bca5e14-3cc5-458e-8b76-b81f49b6228d-2019Feb13T165919504712 -P 8760-9960 -c 600 -i 0.1 -t -S <a href="http://psum01.aglt2.org">psum01.aglt2.org</a> [<a href="http://uct2-net1.mwt2.org">uct2-net1.mwt2.org</a>]:861<br/>pschedu+ 2824155  0.0  0.0   7960   332 ?        Ss   11:35   0:00 /usr/bin/powstream -p -d /var/lib/pscheduler/tool/powstream/c51bfc11-b299-40c4-8409-545ccda26cf0-2019Feb13T165919378200 -P 8760-9960 -c 600 -i 0.1 -t -S <a href="http://psum01.aglt2.org">psum01.aglt2.org</a> [<a href="http://uct2-net1.mwt2.org">uct2-net1.mwt2.org</a>]:861<br/>root     2825071  0.0  0.0 112712   976 pts/0    S+   11:36   0:00 grep --color=auto uct2-net<br/>pschedu+ 3683372  0.0  0.0   7960  1612 ?        S    Feb13   0:06 /usr/bin/powstream -p -d /var/lib/pscheduler/tool/powstream/c51bfc11-b299-40c4-8409-545ccda26cf0-2019Feb13T165919378200 -P 8760-9960 -c 600 -i 0.1 -t -S <a href="http://psum01.aglt2.org">psum01.aglt2.org</a> [<a href="http://uct2-net1.mwt2.org">uct2-net1.mwt2.org</a>]:861<br/>pschedu+ 3683418  0.0  0.0   7956  1604 ?        S    Feb13   0:06 /usr/bin/powstream -p -d /var/lib/pscheduler/tool/powstream/6bca5e14-3cc5-458e-8b76-b81f49b6228d-2019Feb13T165919504712 -P 8760-9960 -c 600 -i 0.1 -t -S <a href="http://psum01.aglt2.org">psum01.aglt2.org</a> [<a href="http://uct2-net1.mwt2.org">uct2-net1.mwt2.org</a>]:861<br/>
<span style="color: #84b22f"><span style="font-size: small">(10:36:37)</span> <b>ivukotic:</b></span> so you never see 9 processes?<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:36:41)</span> <b>smckee:</b></span> See there are two distinct GUIDs involved<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:36:58)</span> <b>smckee:</b></span> c51bf....    and   6bca5....<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:37:23)</span> <b>smckee:</b></span> I don't see 9 processes<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:38:12)</span> <b>smckee:</b></span> So Andy Lake confirmed there is an issue with the scheduler doing too many tests in some cases.  A fix might be out next week.  But this (x2) rate doesn't account for x9 in the data.<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:38:47)</span> <b>smckee:</b></span> Best guess is the "parallel" pipeline which Derek is checking.<br/>
<span style="color: #84b22f"><span style="font-size: small">(10:39:08)</span> <b>ivukotic:</b></span> let's fix that x2 for start<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:39:09)</span> <b>smckee:</b></span> (Plus the x2 should have been happening since we started...didn't change on the 25th)<br/>
<span style="color: #84b22f"><span style="font-size: small">(10:39:58)</span> <b>ivukotic:</b></span> are you sure this x2 is not due to Andy's bug?<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:42:47)</span> <b>smckee:</b></span> There are "redundant" latency tests happening and they have been happening since 4.1.5 (or before)<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:42:58)</span> <b>smckee:</b></span> This is the bug Andy found.<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:43:08)</span> <b>smckee:</b></span> 4.1.5 came out early December 2018<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:48:35)</span> <b>smckee:</b></span> @dweitzel anything interesting in the parallel pipeline setup?  Could you describe how you set it up?<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:49:46)</span> <b>dweitzel:</b></span> nothing that I can see.<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:50:27)</span> <b>smckee:</b></span> Another question related to your email:  Why would "At 12:18 AM last night, one of the 2 HA rabbitmq message bus’s died.  Not sure why, I’ve opened a ticket with the provider.  But I have rebooted it through the web interface, and things seem to be getting back to normal." have an impact on only some of the perfSONAR topics?   HA doesn't mean what I think it does?<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:52:09)</span> <b>dweitzel:</b></span> that's a good question.  I'm not sure on the behavior of rabbitmq in HA mode.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:52:24)</span> <b>dweitzel:</b></span> it's possible that some queue's "Primary" are on the failed node, others are not.<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:52:46)</span> <b>smckee:</b></span> Right but they didn't fail-over to the other (running) one?<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:53:35)</span> <b>dweitzel:</b></span> Also, I saw this in the log of the collectors:<br/><pre>urllib3.exceptions.ConnectTimeoutError: (&lt;urllib3.connection.HTTPConnection object at 0x7f5b16da4630&gt;, 'Connection to <a href="http://atlas-kibana.mwt2.org">atlas-kibana.mwt2.org</a> timed out. (connect timeout=120)')</pre><br/>
<span style="color: #235e5b"><span style="font-size: small">(10:54:06)</span> <b>dweitzel:</b></span> I know last night I saw some network interruption errors on check_mk for some other services on anvil.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:54:10)</span> <b>dweitzel:</b></span> at UNL<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:55:43)</span> <b>smckee:</b></span> Derek, where the production collectors changed/updated on the 25th (in addition to setting up the parallel testing pipeline)?<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:55:48)</span> <b>smckee:</b></span> were<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:57:11)</span> <b>dweitzel:</b></span> no, they were unchanged.<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:57:47)</span> <b>smckee:</b></span> Also no changes on <a href="http://psrsv.opensciencegrid.org">psrsv.opensciencegrid.org</a>, right?<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:57:55)</span> <b>smckee:</b></span> (The data gatherer for the pipeline)<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:58:06)</span> <b>dweitzel:</b></span> so you said that even the "status" messages went up by 9x?<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:58:25)</span> <b>dweitzel:</b></span> that's shoveled from the CERN active MQ, and I certainly can't change that?<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:59:25)</span> <b>smckee:</b></span> Yes, I agree that is really strange.  However, as @ivukotic pointed out, these messages are not duplicates and are distinct<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:59:29)</span> <b>smckee:</b></span> OK	proc_simplevisor	Open the action menu	OK - 1 processes 203.8 MB virtual, 3.3 MB physical, 0.0% CPU (15 min average: 0.1%), running for 189 days	168 m	64 s	<br/>1<br/>OK	proc_stompclt	Open the action menu	OK - 3 processes 648.5 MB virtual, 7.2 MB physical, 0.0% CPU (15 min average: 0.1%), youngest running for 186 days, oldest running for 186 days	168 m	64 s	<br/>3<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(10:59:57)</span> <b>smckee:</b></span> Check_mk shows these psrsv processes have been running for almost 1/2 yeaer<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(11:01:51)</span> <b>smckee:</b></span> I will play around on Kibana with the ps_status data to see where the "extra" data is coming from.    It may be that the data we are getting now is just much more complete and we had a problem before the 25h.  *Still* I want to know what changed on the 25th :disappointed:<br/>
<span style="color: #a63024"><span style="font-size: small">(12:00:50)</span> <b>efajardo:</b></span> @smckee I can take a look.<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(12:01:03)</span> <b>smckee:</b></span> @efajardo Thanks!<br/>
<span style="color: #a63024"><span style="font-size: small">(12:01:10)</span> <b>efajardo:</b></span> any idea where to star?<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(12:01:29)</span> <b>smckee:</b></span> See if you see anything strange.   Well it would be good to verify psrsv is operating as expected.<br/>
<span style="color: #a63024"><span style="font-size: small">(12:01:42)</span> <b>efajardo:</b></span> ok<br/>
<span style="color: #a63024"><span style="font-size: small">(12:01:51)</span> <b>efajardo:</b></span> code wise nothing has changed since apr 24<br/>
<span style="color: #a63024"><span style="font-size: small">(12:02:27)</span> <b>efajardo:</b></span> I do rembember on the 25th<br/>
<span style="color: #a63024"><span style="font-size: small">(12:02:31)</span> <b>efajardo:</b></span> we played with pssconfig<br/>
<span style="color: #a63024"><span style="font-size: small">(12:02:35)</span> <b>efajardo:</b></span> adding the stashcache mesh<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(12:02:54)</span> <b>smckee:</b></span> The other option is to explore the data we are receiving via Kibana:  <a href="http://atlas-kibana-dev.mwt2.org/app/kibana#/discover?_g=(refreshInterval:(pause:!t,value:0),time:(from:now-30d,mode:quick,to:now))&amp;_a=(columns:!(dest_site),index:e1236be0-1852-11e8-8f2f-ab6704660c79,interval:auto,query:(language:lucene,query:''),sort:!(timestamp,desc))">http://atlas-kibana-dev.mwt2.org/app/kibana#/discover?_g=(refreshInterval:(pause:!t,value:0),time:(from:now-30d,mode:quick,to:now))&amp;_a=(columns:!(dest_site),index:e1236be0-1852-11e8-8f2f-ab6704660c79,interval:auto,query:(language:lucene,query:''),sort:!(timestamp,desc))</a><br/>
<span style="color: #4ec0d6"><span style="font-size: small">(12:03:19)</span> <b>smckee:</b></span> I keep having meetings getting in the way of doing work...<br/>
<span style="color: #4ec0d6"><span style="font-size: small">(12:26:23)</span> <b>smckee:</b></span> New email alerts on data issues on the bus.  Here is the last 4 hours to UC:  <a href="http://atlas-kibana.mwt2.org:5601/app/kibana#/visualize/edit/58bf3e80-18d1-11e8-8f2f-ab6704660c79?_g=(refreshInterval:(pause:!t,value:0),time:(from:now-4h,mode:quick,to:now))&amp;_a=(filters:!(),linked:!f,query:(language:lucene,query:(match_all:())),uiState:(),vis:(aggs:!(),params:(expression:'.es(index%3D!'ps_owd!',%20q%3D!'*!').label(!'OWD!'),%20.es(index%3D!'ps_throughput!',%20q%3D!'*!').label(!'throughput!').yaxis(2),%20.es(index%3D!'ps_meta!',%20q%3D!'*!').label(!'metadata!').yaxis(2),%20.es(index%3D!'ps_status!',%20q%3D!'*!').label(!'status!').yaxis(2),%20.es(index%3D!'ps_trace!',%20q%3D!'*!').label(!'traceroute!'),%20.es(index%3D!'ps_packet_loss!',%20q%3D!'*!').label(!'packet%20loss!'),%0A.es(index%3D!'ps_retransmits!',%20q%3D!'*!').label(!'retransmits!').yaxis(2)',interval:auto,type:timelion),title:'PerfSONAR%20ingest',type:timelion))">http://atlas-kibana.mwt2.org:5601/app/kibana#/visualize/edit/58bf3e80-18d1-11e8-8f2f-ab6704660c79?_g=(refreshInterval:(pause:!t,value:0),time:(from:now-4h,mode:quick,to:now))&amp;_a=(filters:!(),linked:!f,query:(language:lucene,query:(match_all:())),uiState:(),vis:(aggs:!(),params:(expression:'.es(index%3D!'ps_owd!',%20q%3D!'*!').label(!'OWD!'),%20.es(index%3D!'ps_throughput!',%20q%3D!'*!').label(!'throughput!').yaxis(2),%20.es(index%3D!'ps_meta!',%20q%3D!'*!').label(!'metadata!').yaxis(2),%20.es(index%3D!'ps_status!',%20q%3D!'*!').label(!'status!').yaxis(2),%20.es(index%3D!'ps_trace!',%20q%3D!'*!').label(!'traceroute!'),%20.es(index%3D!'ps_packet_loss!',%20q%3D!'*!').label(!'packet%20loss!'),%0A.es(index%3D!'ps_retransmits!',%20q%3D!'*!').label(!'retransmits!').yaxis(2)',interval:auto,type:timelion),title:'PerfSONAR%20ingest',type:timelion))</a><br/>
<span style="color: #4ec0d6"><span style="font-size: small">(12:26:55)</span> <b>smckee:</b></span> Is someone doing something that might be causing this?<br/>
</body>
</html>

<!DOCTYPE html>
<html>
<head>
<title>Tue Feb 20, 2018 : #xcache (osg)</title>
</head>
<body>
<h3>Tue Feb 20, 2018 : #xcache (osg)</h3>
<span style="color: #674b1b"><span style="font-size: small">(12:04:27)</span> <b>rynge:</b></span> @ivukotic I still see the errors<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:04:29)</span> <b>rynge:</b></span> 2018-02-20 07:29:29,313    INFO:  2018-02-20T07:29:29+0000 root         WARNING  Error posting to ES: HTTP Error 504: Gateway Time-out<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:05:17)</span> <b>rynge:</b></span> I also would like to revisit the performance issues. Are we just hitting the caches so hard that we are out of resources?<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:06:02)</span> <b>rynge:</b></span> The stuff I'm working on should cache really well<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:06:16)</span> <b>rynge:</b></span> So I'm surprised we see such a poor performance<br/>
<span style="color: #84b22f"><span style="font-size: small">(12:23:59)</span> <b>ivukotic:</b></span> we are not talking about collection of xrootd summary stream?<br/>
<span style="color: #84b22f"><span style="font-size: small">(12:24:05)</span> <b>ivukotic:</b></span> that one works:<br/>
<span style="color: #84b22f"><span style="font-size: small">(12:24:06)</span> <b>ivukotic:</b></span> 2018-02-20 12:16:53,954 - SummaryCollector - WARNING - Program: cmsd should not be sending summary information. Source: <a href="http://stash.osgconnect.net:0">stash.osgconnect.net:0</a><br/>2018-02-20 12:17:03,959 - SummaryCollector - WARNING - Program: cmsd should not be sending summary information. Source: <a href="http://redirector-itb2.grid.iu.edu:1213">redirector-itb2.grid.iu.edu:1213</a><br/>2018-02-20 12:17:48,583 - SummaryCollector - INFO - Thread-2    inserted: 20    errors: []<br/>2018-02-20 12:17:53,957 - SummaryCollector - WARNING - Program: cmsd should not be sending summary information. Source: <a href="http://stash.osgconnect.net:0">stash.osgconnect.net:0</a><br/>2018-02-20 12:18:53,955 - SummaryCollector - WARNING - Program: cmsd should not be sending summary information. Source: <a href="http://stash.osgconnect.net:0">stash.osgconnect.net:0</a><br/>2018-02-20 12:19:26,347 - SummaryCollector - INFO - Thread-1    inserted: 20    errors: []<br/>2018-02-20 12:19:53,956 - SummaryCollector - WARNING - Program: cmsd should not be sending summary information. Source: <a href="http://stash.osgconnect.net:0">stash.osgconnect.net:0</a><br/>2018-02-20 12:20:53,956 - SummaryCollector - WARNING - Program: cmsd should not be sending summary information. Source: <a href="http://stash.osgconnect.net:0">stash.osgconnect.net:0</a><br/>2018-02-20 12:21:26,666 - SummaryCollector - WARNING - Program: cmsd should not be sending summary information. Source: <a href="http://redirector1.grid.iu.edu:1213">redirector1.grid.iu.edu:1213</a><br/>2018-02-20 12:21:53,956 - SummaryCollector - WARNING - Program: cmsd should not be sending summary information. Source: <a href="http://stash.osgconnect.net:0">stash.osgconnect.net:0</a><br/>/var/log &gt;date                                                                                                                                                 12:22<br/>Tue Feb 20 12:22:54 CST 2018<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:27:01)</span> <b>rynge:</b></span> We are talking about "stashcp" command<br/>
<span style="color: #84b22f"><span style="font-size: small">(12:27:47)</span> <b>ivukotic:</b></span> can you remind me where do they send info to?<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:28:12)</span> <b>dweitzel:</b></span> <a href="http://uct2-collectd.mwt2.org:9951">http://uct2-collectd.mwt2.org:9951</a><br/>
<span style="color: #84b22f"><span style="font-size: small">(12:31:11)</span> <b>ivukotic:</b></span> a lot of errors in logs: {:timestamp=&gt;"2018-02-20T04:02:06.629000-0600", :message=&gt;"A plugin had an unrecoverable error. Will restart this plugin.\n  Plugin: &lt;LogStash::Inputs::Tcp host=&gt;\"0.0.0.0\", port=&gt;9951, type=&gt;\"rec\", codec=&gt;&lt;LogStash::Codecs::Line charset=&gt;\"UTF-8\", delimiter=&gt;\"\\n\"&gt;, data_timeout=&gt;-1, mode=&gt;\"server\", ssl_enable=&gt;false, ssl_verify=&gt;true, ssl_key_passphrase=&gt;&lt;password&gt;&gt;\n  Error: closed stream", :level=&gt;:error}<br/>
<span style="color: #84b22f"><span style="font-size: small">(12:31:21)</span> <b>ivukotic:</b></span> I will restart logstash<br/>
<span style="color: #84b22f"><span style="font-size: small">(12:37:03)</span> <b>ivukotic:</b></span> restarted... can you try one stashcp now?<br/>
<span style="color: #84b22f"><span style="font-size: small">(12:38:10)</span> <b>ivukotic:</b></span> it works, you don't have to do test<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:41:04)</span> <b>rynge:</b></span> Looks good here also<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:41:21)</span> <b>rynge:</b></span> Now, the performance issues - any ideas?<br/>
<span style="color: #84b22f"><span style="font-size: small">(12:41:45)</span> <b>ivukotic:</b></span> transfer performance?<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:42:44)</span> <b>rynge:</b></span> Yeah, seems OSG wide to me, with some sites being worse than others<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:43:52)</span> <b>dweitzel:</b></span> well, at Nebraska at least, it's just heavily loaded: <a href="http://hcc-ganglia.unl.edu/?c=red-infrastructure&amp;h=hcc-stash.unl.edu&amp;m=load_report&amp;r=hour&amp;s=by%20name&amp;hc=4&amp;mc=2">http://hcc-ganglia.unl.edu/?c=red-infrastructure&amp;h=hcc-stash.unl.edu&amp;m=load_report&amp;r=hour&amp;s=by%20name&amp;hc=4&amp;mc=2</a><br/>
<span style="color: #235e5b"><span style="font-size: small">(12:44:01)</span> <b>dweitzel:</b></span> probably won't get great performance out of Nebraska's.<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:44:29)</span> <b>rynge:</b></span> What about the other caches? Are they all equally loaded?<br/>
<span style="color: #84b22f"><span style="font-size: small">(12:44:35)</span> <b>ivukotic:</b></span> have no idea. this is all cvmfs. not sure I understand how it even works :slightly_smiling_face:<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:44:44)</span> <b>dweitzel:</b></span> It's all XrootD<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:44:59)</span> <b>dweitzel:</b></span> @marian any other links to monitoring of caches?<br/>
<span style="color: #84b22f"><span style="font-size: small">(12:45:14)</span> <b>ivukotic:</b></span> i see cache: CVMFS in the new data.<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:46:20)</span> <b>dweitzel:</b></span> well, that just means it's pulling the data through the cache's HTTP interface.  Once the request hits a cache's HTTP interface, it's all XRootD after that.  No data goes through the CVMFS infrastructure, just the namespace.<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:46:27)</span> <b>rynge:</b></span> Do we need to look at the origin server maybe? Can we see what causes the UNL one being loaded?<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:47:39)</span> <b>dweitzel:</b></span> Since UNL's cache is mostly just IO loaded, and it's all outgoing, I would say it's probably not an origin problem.<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:48:33)</span> <b>dweitzel:</b></span> @lincoln any good monitoring on the Stashcache origin?  Which I think is <a href="http://stash.osgconnect.net">stash.osgconnect.net</a><br/>
<span style="color: #674b1b"><span style="font-size: small">(12:49:18)</span> <b>rynge:</b></span> Ok, can we get some idea what data it is serving out?<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:49:41)</span> <b>rynge:</b></span> (could be we/users are doing something silly, which would be good to know)<br/>
<span style="color: #235e5b"><span style="font-size: small">(12:54:57)</span> <b>dweitzel:</b></span> Working on that kind of monitoring now.  But right now, no, no centeralized method of doing that.<br/>
<span style="color: #674b1b"><span style="font-size: small">(13:01:18)</span> <b>rynge:</b></span> Well, can you get me a log or something so I can take a look? grepping is fine for now<br/>
<span style="color: #674b1b"><span style="font-size: small">(13:29:23)</span> <b>rynge:</b></span> I also see some of these:<br/>
<span style="color: #674b1b"><span style="font-size: small">(13:29:25)</span> <b>rynge:</b></span> 2018-02-20T12:20:10+0000 root         WARNING  XrdCP from cache failed on <a href="root://stashcache.grid.uchicago.edu">root://stashcache.grid.uchicago.edu</a>, pulling from main redirector<br/>
<span style="color: #674b1b"><span style="font-size: small">(13:29:45)</span> <b>rynge:</b></span> But that one was from cinestav so it could be just network issues in that case<br/>
<span style="color: #674b1b"><span style="font-size: small">(13:35:55)</span> <b>rynge:</b></span> @rynge uploaded a file: <a href="https://opensciencegrid.slack.com/files/U04ALFF47/F9C54MCHK/-.txt">Untitled</a><br/>
<span style="color: #674b1b"><span style="font-size: small">(13:36:03)</span> <b>rynge:</b></span> That is just number of accesses<br/>
<span style="color: #674b1b"><span style="font-size: small">(13:36:11)</span> <b>rynge:</b></span> No surprises<br/>
<span style="color: #674b1b"><span style="font-size: small">(13:36:31)</span> <b>rynge:</b></span> But the user who made me look at this, is the one with the most accesses<br/>
<span style="color: #53b759"><span style="font-size: small">(14:03:58)</span> <b>marian:</b></span> btw, I don't have any other in-depth monitoring like ganglia can provide for individual hosts ... we could ask site admins, though ... is it needed? all we have is check_mk status of xrootd which all seem green ... is there anything I could help here?<br/>
<span style="color: #235e5b"><span style="font-size: small">(14:09:53)</span> <b>dweitzel:</b></span> The issue isn't if it's working, the issue is, how well is it working?<br/>
<span style="color: #235e5b"><span style="font-size: small">(14:10:05)</span> <b>dweitzel:</b></span> I don't think we have the monitoring necessary to answer that question.<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:11:16)</span> <b>rynge:</b></span> 2018-02-20T11:59:31+0000 root         DEBUG    Trying geoip site of: <a href="http://cvmfs-egi.gridpp.rl.ac.uk:8000/cvmfs/oasis.opensciencegrid.org">http://cvmfs-egi.gridpp.rl.ac.uk:8000/cvmfs/oasis.opensciencegrid.org</a><br/>2018-02-20T11:59:31+0000 root         DEBUG    Querying for closest cache: <a href="http://cvmfs-egi.gridpp.rl.ac.uk:8000/cvmfs/oasis.opensciencegrid.org/api/v1.0/geo/stashcp/xrd-cache-1.t2.ucsd.edu,mwt2-stashcache.campuscluster.illinois.edu,hcc-stash.unl.edu,osgxroot.usatlas.bnl.gov,its-condor-xrootd1.syr.edu,stashcache.grid.uchicago.edu">http://cvmfs-egi.gridpp.rl.ac.uk:8000/cvmfs/oasis.opensciencegrid.org/api/v1.0/geo/stashcp/xrd-cache-1.t2.ucsd.edu,mwt2-stashcache.campuscluster.illinois.edu,hcc-stash.unl.edu,osgxroot.usatlas.bnl.gov,its-condor-xrootd1.syr.edu,stashcache.grid.uchicago.edu</a><br/>2018-02-20T11:59:32+0000 root         DEBUG    URL error: HTTP Error 404: Not Found<br/>
<span style="color: #235e5b"><span style="font-size: small">(14:11:46)</span> <b>dweitzel:</b></span> Does it move onto another geoip URL?<br/>
<span style="color: #235e5b"><span style="font-size: small">(14:11:59)</span> <b>dweitzel:</b></span> it should randomize the list at the beginning, then go through all of them until 1 of them works.<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:12:15)</span> <b>rynge:</b></span> 2018-02-20T11:59:32+0000 root         DEBUG    Trying geoip site of: <a href="http://klei.nikhef.nl:8000/cvmfs/oasis.opensciencegrid.org">http://klei.nikhef.nl:8000/cvmfs/oasis.opensciencegrid.org</a><br/>2018-02-20T11:59:32+0000 root         DEBUG    Querying for closest cache: <a href="http://klei.nikhef.nl:8000/cvmfs/oasis.opensciencegrid.org/api/v1.0/geo/stashcp/xrd-cache-1.t2.ucsd.edu,mwt2-stashcache.campuscluster.illinois.edu,hcc-stash.unl.edu,osgxroot.usatlas.bnl.gov,its-condor-xrootd1.syr.edu,stashcache.grid.uchicago.edu">http://klei.nikhef.nl:8000/cvmfs/oasis.opensciencegrid.org/api/v1.0/geo/stashcp/xrd-cache-1.t2.ucsd.edu,mwt2-stashcache.campuscluster.illinois.edu,hcc-stash.unl.edu,osgxroot.usatlas.bnl.gov,its-condor-xrootd1.syr.edu,stashcache.grid.uchicago.edu</a><br/>
<span style="color: #674b1b"><span style="font-size: small">(14:12:21)</span> <b>rynge:</b></span> Ok so that worked<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:49:27)</span> <b>rynge:</b></span> I have found two interesting things:<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:50:17)</span> <b>rynge:</b></span> 1) Whenever <a href="http://cvmfsrep.grid.sinica.edu.tw:8000/cvmfs/oasis.opensciencegrid.org">http://cvmfsrep.grid.sinica.edu.tw:8000/cvmfs/oasis.opensciencegrid.org</a> is used for the geoip, it returns the wrong closest cache (maybe it always returns <a href="http://stashcache.grid.uchicago.edu">stashcache.grid.uchicago.edu</a><br/>
<span style="color: #674b1b"><span style="font-size: small">(14:51:33)</span> <b>rynge:</b></span> 2) I have been running stashcp tests at UCSD, and see the transfers take way longer if I use stashcp vs xrdcp. For the test file, it takes ~1m12s, and for the xrdcp 9s. Is that expected?<br/>
<span style="color: #235e5b"><span style="font-size: small">(15:03:07)</span> <b>dweitzel:</b></span> no, not at all.  At UCSD, is it using CVMFS?<br/>
<span style="color: #235e5b"><span style="font-size: small">(15:03:40)</span> <b>dweitzel:</b></span> Can you send the full debug output?  So I can look at the timestamps of things?<br/>
<span style="color: #674b1b"><span style="font-size: small">(15:04:08)</span> <b>rynge:</b></span> @rynge uploaded a file: <a href="https://opensciencegrid.slack.com/files/U04ALFF47/F9C2M7A20/-.txt">Untitled</a><br/>
<span style="color: #235e5b"><span style="font-size: small">(15:07:38)</span> <b>dweitzel:</b></span> hum... yeah, the copy alone takes that full minute.  Any chance you can try running that stashcp twice?<br/>
<span style="color: #674b1b"><span style="font-size: small">(15:08:48)</span> <b>rynge:</b></span> Yeah, just one after another?<br/>
<span style="color: #674b1b"><span style="font-size: small">(15:10:51)</span> <b>rynge:</b></span> (and in other news, seems like my users has a bug in the jobs, making it pull in the same data in two different formats for each job!)<br/>
<span style="color: #235e5b"><span style="font-size: small">(15:12:50)</span> <b>dweitzel:</b></span> yeah, right after another.<br/>
<span style="color: #674b1b"><span style="font-size: small">(15:15:40)</span> <b>rynge:</b></span> @rynge uploaded a file: <a href="https://opensciencegrid.slack.com/files/U04ALFF47/F9BGN0C9X/-.txt">Untitled</a><br/>
<span style="color: #674b1b"><span style="font-size: small">(15:15:53)</span> <b>rynge:</b></span> And you get the geoip problem for free!<br/>
</body>
</html>

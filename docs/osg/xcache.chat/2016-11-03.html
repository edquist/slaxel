<!DOCTYPE html>
<html>
<head>
<title>Thu Nov 3, 2016 : #xcache (osg)</title>
</head>
<body>
<h3>Thu Nov 3, 2016 : #xcache (osg)</h3>
<span style="color: #9e3997"><span style="font-size: small">(12:59:31)</span> <b>bbockelm:</b></span> @lincoln - can you kick the xrootd server @ uchicago for stash over?<br/>
<span style="color: #e96699"><span style="font-size: small">(13:00:11)</span> <b>lincoln:</b></span> the origin or cache?<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:00:26)</span> <b>bbockelm:</b></span> origin<br/>
<span style="color: #e96699"><span style="font-size: small">(13:00:55)</span> <b>lincoln:</b></span> i can, it looks like it’s doin stuff in the logs. not dead or anything<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:01:07)</span> <b>bbockelm:</b></span> hmm..<br/>
<span style="color: #e96699"><span style="font-size: small">(13:01:15)</span> <b>lincoln:</b></span> <pre><br/>161103 13:01:07 20142 ?:22@its-condor-net4.syr.edu XrdPoll: FD 22 attached to poller 1; num=3<br/>161103 13:01:07 20142 XrootdXeq: osg.4150598:22@its-condor-net4.syr.edu pub IPv4 login<br/>161103 13:01:07 20142 osg.4150598:22@its-condor-net4.syr.edu ofs_stat:  fn=/user/ldunwoo/public/kinc-1477602035/00/00/Hsapiens-9606-20161012-204-Quantile-NCBI_GBM_NormalBrain_Parkinson-CufflinksHisat-v1.tar.gz<br/>161103 13:01:08 20142 XrootdXeq: osg.4150598:22@its-condor-net4.syr.edu disc 0:00:01<br/>161103 13:01:08 20142 osg.4150598:22@its-condor-net4.syr.edu XrdPoll: FD 22 detached from poller 1; num=2<br/>161103 13:01:08 20135 XrdInet: Accepted connection from <a href="mailto:24@its-condor-net4.syr.edu">24@its-condor-net4.syr.edu</a><br/>161103 13:01:08 20126 XrdSched: running main accept inq=0<br/>161103 13:01:08 20135 XrdProtocol: matched protocol xrootd<br/>161103 13:01:08 20135 ?:24@its-condor-net4.syr.edu XrdPoll: FD 24 attached to poller 1; num=3<br/>161103 13:01:08 20135 XrootdXeq: osg.4150608:24@its-condor-net4.syr.edu pub IPv4 login<br/>161103 13:01:08 20135 osg.4150608:24@its-condor-net4.syr.edu ofs_stat:  fn=/user/ldunwoo/public/kinc-1477602035/00/00/Hsapiens-9606-20161012-204-Quantile-NCBI_GBM_NormalBrain_Parkinson-CufflinksHisat-v1.tar.gz<br/>161103 13:01:08 20135 XrootdXeq: osg.4150608:24@its-condor-net4.syr.edu disc 0:00:00<br/>161103 13:01:08 20135 osg.4150608:24@its-condor-net4.syr.edu XrdPoll: FD 24 detached from poller 1; num=2<br/></pre><br/>
<span style="color: #e96699"><span style="font-size: small">(13:01:17)</span> <b>lincoln:</b></span> etc etc<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:01:40)</span> <b>bbockelm:</b></span> do you see stuff coming from <tt><a href="http://hcc-cvmfs-repo.unl.edu">hcc-cvmfs-repo.unl.edu</a></tt>?<br/>
<span style="color: #e96699"><span style="font-size: small">(13:02:43)</span> <b>lincoln:</b></span> no, because I have it iptables banned because the spider was causing too much load or at least was while I was debugging other issues.<br/>
<span style="color: #e96699"><span style="font-size: small">(13:02:54)</span> <b>lincoln:</b></span> I’ll unban it if you promise not to kill my metadata server :slightly_smiling_face:<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:13:34)</span> <b>bbockelm:</b></span> errmm<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:13:43)</span> <b>bbockelm:</b></span> I promise I won't do anything different.<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:25:58)</span> <b>bbockelm:</b></span> oh wait, I'm a dummy.  I have a concurrency knob for metadata queries.<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:33:39)</span> <b>bbockelm:</b></span> @lincoln - metdata queries reduced from 4 threads to 1.<br/>
<span style="color: #e96699"><span style="font-size: small">(13:33:51)</span> <b>lincoln:</b></span> ok<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:33:53)</span> <b>bbockelm:</b></span> I can haz unblock?<br/>
<span style="color: #e96699"><span style="font-size: small">(13:33:53)</span> <b>lincoln:</b></span> i unbanned it<br/>
<span style="color: #e96699"><span style="font-size: small">(13:35:05)</span> <b>lincoln:</b></span> but only if you give me those sweet sweet docker universe / job router configs.<br/>
<span style="color: #e96699"><span style="font-size: small">(13:35:25)</span> <b>lincoln:</b></span> i assume these slides are still relevant? <a href="https://research.cs.wisc.edu/htcondor/HTCondorWeek2016/presentations/WedWeitzel_DockerGridJobs.pdf">https://research.cs.wisc.edu/htcondor/HTCondorWeek2016/presentations/WedWeitzel_DockerGridJobs.pdf</a><br/>
<span style="color: #9e3997"><span style="font-size: small">(13:36:16)</span> <b>bbockelm:</b></span> and, since the meeting finished early, I can turn back to the Ceph-native processor.  What's the VM hostname again?<br/>
<span style="color: #e96699"><span style="font-size: small">(13:36:47)</span> <b>lincoln:</b></span> good question i don’t remember<br/>
<span style="color: #e96699"><span style="font-size: small">(13:36:49)</span> <b>lincoln:</b></span> let me find it<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:36:54)</span> <b>bbockelm:</b></span> yeah, those slides are still relevant.  Let's switch to the osg-investigations forum and I'll copy/paste some configs.<br/>
<span style="color: #e96699"><span style="font-size: small">(13:37:01)</span> <b>lincoln:</b></span> ok<br/>
<span style="color: #e96699"><span style="font-size: small">(13:39:01)</span> <b>lincoln:</b></span> <a href="http://stash-cvmfs.osgconnect.net">stash-cvmfs.osgconnect.net</a><br/>
<span style="color: #9e3997"><span style="font-size: small">(13:46:17)</span> <b>bbockelm:</b></span> Do you have puppet magic to enable OSG repos for that host?<br/>
<span style="color: #e96699"><span style="font-size: small">(13:46:26)</span> <b>lincoln:</b></span> mm, yeah i think so<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:47:32)</span> <b>bbockelm:</b></span> <pre><br/>[bbockelm@stash-cvmfs cvmfs-sync]$ vim stash-async <br/>-bash: vim: command not found<br/></pre><br/>IS THIS EVEN A REAL HOST<br/>
<span style="color: #e96699"><span style="font-size: small">(13:47:49)</span> <b>lincoln:</b></span> i only install @core nowadays<br/>
<span style="color: #e96699"><span style="font-size: small">(13:47:55)</span> <b>lincoln:</b></span> does ‘vi’ work?<br/>
<span style="color: #e96699"><span style="font-size: small">(13:48:05)</span> <b>lincoln:</b></span> b/c i think EL7, vim-lite only gives you a /bin/vi<br/>
<span style="color: #e96699"><span style="font-size: small">(13:48:07)</span> <b>lincoln:</b></span> or something dumb<br/>
<span style="color: #e96699"><span style="font-size: small">(13:48:16)</span> <b>lincoln:</b></span> i can install the full vim but i hate pulling in the x11 dependencies<br/>
<span style="color: #e96699"><span style="font-size: small">(13:48:24)</span> <b>lincoln:</b></span> (RH’s build for vim is pretty much the worst)<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:48:48)</span> <b>bbockelm:</b></span> :slightly_smiling_face: nah, I have sudo. I can make it work.<br/>
<span style="color: #e96699"><span style="font-size: small">(13:48:57)</span> <b>lincoln:</b></span> hah. ok<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:55:44)</span> <b>bbockelm:</b></span> @lincoln:<br/><pre><br/>Nov  3 19:53:28 stash-cvmfs kernel: stash-ceph      D ffff880210fbe960     0  1690    852 0x00000084<br/>Nov  3 19:53:28 stash-cvmfs kernel: ffff880210ea3b00 0000000000000082 ffff8802141b5080 ffff880210ea3fd8<br/>Nov  3 19:53:28 stash-cvmfs kernel: ffff880210ea3fd8 ffff880210ea3fd8 ffff8802141b5080 ffff880210fbe960<br/>Nov  3 19:53:28 stash-cvmfs kernel: ffff8800363ba800 ffff880210ea3b30 ffff880210fbea40 ffff880210fbe960<br/>Nov  3 19:53:28 stash-cvmfs kernel: Call Trace:<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffff8163ba09&gt;] schedule+0x29/0x70<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffffa025e53d&gt;] __fuse_request_send+0x13d/0x2c0 [fuse]<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffff810a6b80&gt;] ? wake_up_atomic_t+0x30/0x30<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffffa025e6d2&gt;] fuse_request_send+0x12/0x20 [fuse]<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffffa026472a&gt;] fuse_lookup_name+0x13a/0x2d0 [fuse]<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffffa026490e&gt;] fuse_lookup+0x4e/0x130 [fuse]<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffffa0264ac2&gt;] fuse_atomic_open+0xd2/0x100 [fuse]<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffff811ec8a1&gt;] do_last+0xa11/0x1270<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffff811eede2&gt;] path_openat+0xc2/0x490<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffff810e2860&gt;] ? futex_wake+0x80/0x160<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffff811f05ab&gt;] do_filp_open+0x4b/0xb0<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffff811fd147&gt;] ? __alloc_fd+0xa7/0x130<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffff811ddf53&gt;] do_sys_open+0xf3/0x1f0<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffff811de084&gt;] SyS_openat+0x14/0x20<br/>Nov  3 19:53:28 stash-cvmfs kernel: [&lt;ffffffff81646a09&gt;] system_call_fastpath+0x16/0x1b<br/>Nov  3 19:53:28 stash-cvmfs kernel: INFO: task stash-ceph:1691 blocked for more than 120 seconds.<br/></pre><br/>
<span style="color: #e96699"><span style="font-size: small">(14:55:52)</span> <b>lincoln:</b></span> ya someone broke ceph<br/>
<span style="color: #e96699"><span style="font-size: small">(14:55:54)</span> <b>lincoln:</b></span> was it you? :slightly_smiling_face:<br/>
<span style="color: #e96699"><span style="font-size: small">(14:56:01)</span> <b>lincoln:</b></span> im working on it, one sec<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:56:51)</span> <b>bbockelm:</b></span> i don't ... think it was me.<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:57:25)</span> <b>bbockelm:</b></span> looks like it happened like 15 minutes ago?<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:59:17)</span> <b>bbockelm:</b></span> I see one terminal that is doing ~15 downloads via Xrootd and the other is starting to spider the FS.  The latter only processed about 100 directories.<br/>
<span style="color: #9e3997"><span style="font-size: small">(15:16:03)</span> <b>bbockelm:</b></span> (is it ok to poke again?)<br/>
<span style="color: #9e3997"><span style="font-size: small">(15:52:30)</span> <b>bbockelm:</b></span> @lincoln alrighty: I have the initial script up and running on <a href="http://stash-cvmfs.opensciencegrid.org">stash-cvmfs.opensciencegrid.org</a>.  Ganglia plots should be fairly interesting there for the next few days.<br/>
<span style="color: #9e3997"><span style="font-size: small">(15:59:53)</span> <b>bbockelm:</b></span> (I can haz 10Gbps VM?)<br/>
<span style="color: #e96699"><span style="font-size: small">(16:00:59)</span> <b>lincoln:</b></span> yeah itssafetopokenow<br/>
<span style="color: #e96699"><span style="font-size: small">(16:01:22)</span> <b>lincoln:</b></span> all of our hypervisors are 10Gbps :slightly_smiling_face: your VM has to share!<br/>
<span style="color: #e96699"><span style="font-size: small">(16:01:34)</span> <b>lincoln:</b></span> also my spacebaris broken on this server room laptop :disappointed:<br/>
<span style="color: #e96699"><span style="font-size: small">(16:05:00)</span> <b>lincoln:</b></span> are you seeing a bottleneck?<br/>
<span style="color: #e96699"><span style="font-size: small">(16:05:13)</span> <b>lincoln:</b></span> the FUSE mount has a limit of about 100MB/s<br/>
<span style="color: #e96699"><span style="font-size: small">(16:05:18)</span> <b>lincoln:</b></span> in my experience anyhow<br/>
<span style="color: #9e3997"><span style="font-size: small">(19:50:32)</span> <b>bbockelm:</b></span> Yeah - that seems about right.  It's done only ~700GB in the meantime<br/>
<span style="color: #9e3997"><span style="font-size: small">(19:50:46)</span> <b>bbockelm:</b></span> what's the total in the public directories?  Know the leading-order number of rthat?<br/>
<span style="color: #e96699"><span style="font-size: small">(19:51:12)</span> <b>lincoln:</b></span> hm, i don’t offhand.<br/>
<span style="color: #e96699"><span style="font-size: small">(19:51:16)</span> <b>lincoln:</b></span> lets see<br/>
<span style="color: #e96699"><span style="font-size: small">(19:54:10)</span> <b>lincoln:</b></span> for all of /stash/user we have ceph.dir.rsubdirs=”11703649"<br/>
<span style="color: #e96699"><span style="font-size: small">(19:54:22)</span> <b>lincoln:</b></span> i don’t have a good way to separate out just public, i imagine significantly smaller<br/>
<span style="color: #9e3997"><span style="font-size: small">(19:55:14)</span> <b>bbockelm:</b></span> meh.  I guess we wait.<br/>
<span style="color: #e96699"><span style="font-size: small">(19:55:25)</span> <b>lincoln:</b></span> i can do some awk but .. lazy<br/>
<span style="color: #9e3997"><span style="font-size: small">(19:55:34)</span> <b>bbockelm:</b></span> How does the MDS load look?  I officially have two spiders running now<br/>
<span style="color: #e96699"><span style="font-size: small">(19:55:51)</span> <b>lincoln:</b></span> nubmer of ops is very low<br/>
<span style="color: #e96699"><span style="font-size: small">(19:56:14)</span> <b>lincoln:</b></span> i increased the mds cache size and moved it to hardware a week or two ago, so maybe the problems we were seeing before are gone<br/>
<span style="color: #e96699"><span style="font-size: small">(19:56:27)</span> <b>lincoln:</b></span> we had the cache sized too small so aggressive spidering was causing a ton of churn<br/>
<span style="color: #9e3997"><span style="font-size: small">(20:21:29)</span> <b>bbockelm:</b></span> Yeah - even with the FUSE client, it's going to be tricky to get the xattr logic correct.<br/>
<span style="color: #9e3997"><span style="font-size: small">(20:22:05)</span> <b>bbockelm:</b></span> I think I'll probably do a full sync once a day, then just sync deltas every hour.<br/>
</body>
</html>

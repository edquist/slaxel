<!DOCTYPE html>
<html>
<head>
<title>Fri Apr 21, 2017 : #gracc (osg)</title>
</head>
<body>
<h3>Fri Apr 21, 2017 : #gracc (osg)</h3>
<span style="color: #9e3997"><span style="font-size: small">(08:07:00)</span> <b>bbockelm:</b></span> Did anyone actually get a copy of the APEL upload scripts yesterday?<br/>
<span style="color: #3c8c69"><span style="font-size: small">(08:21:43)</span> <b>kretzke:</b></span> @bbockelm yeah, been meaning to move stuff to osg docker hub... who controls that? @dweitzel I think you said you can add me?<br/>
<span style="color: #9e3997"><span style="font-size: small">(08:22:45)</span> <b>bbockelm:</b></span> Looks like I have the appropriate permissions.<br/>
<span style="color: #9e3997"><span style="font-size: small">(08:23:04)</span> <b>bbockelm:</b></span> <tt>retzkek</tt>, right?<br/>
<span style="color: #3c8c69"><span style="font-size: small">(08:23:09)</span> <b>kretzke:</b></span> yep<br/>
<span style="color: #9e3997"><span style="font-size: small">(08:23:50)</span> <b>bbockelm:</b></span> done.<br/>
<span style="color: #3c8c69"><span style="font-size: small">(08:24:07)</span> <b>kretzke:</b></span> thanks!<br/>
<span style="color: #3c8c69"><span style="font-size: small">(08:24:34)</span> <b>kretzke:</b></span> in other news, ceph is hosing elasticsearch again… I’m learning to hate friday mornings :disappointed:<br/>
<span style="color: #9e3997"><span style="font-size: small">(08:27:09)</span> <b>bbockelm:</b></span> what URL do you look at for that?<br/>
<span style="color: #9e3997"><span style="font-size: small">(08:28:37)</span> <b>bbockelm:</b></span> <a href="http://opensciencegrid.github.io/gracc/dev-docs/install-grace-db/">http://opensciencegrid.github.io/gracc/dev-docs/install-grace-db/</a> :confused: this page still needs some love.<br/>
<span style="color: #3c8c69"><span style="font-size: small">(08:29:09)</span> <b>kretzke:</b></span> <a href="https://gracc.opensciencegrid.org/dashboard/db/grace-elasticsearch-monitor?refresh=1m&amp;orgId=1">https://gracc.opensciencegrid.org/dashboard/db/grace-elasticsearch-monitor?refresh=1m&amp;orgId=1</a><br/>
<span style="color: #3c8c69"><span style="font-size: small">(08:31:29)</span> <b>kretzke:</b></span> yep sure does<br/>
<span style="color: #9e3997"><span style="font-size: small">(08:43:16)</span> <b>bbockelm:</b></span> so... we're just waiting for all the data nodes to re-initialize?<br/>
<span style="color: #3c8c69"><span style="font-size: small">(08:54:18)</span> <b>kretzke:</b></span> assuming none of them drop out again. gotten three alerts about -4 in the last 15 minutes though<br/>
<span style="color: #3c8c69"><span style="font-size: small">(08:57:30)</span> <b>kretzke:</b></span> although that is likely because the nodes are so busy recovering indices that the monitoring queries are timing out<br/>
<span style="color: #3c8c69"><span style="font-size: small">(08:59:50)</span> <b>kretzke:</b></span> but at this point, no manual intervention required. except maybe praying to the storage god that all the primary shards are recovered ok<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:09:18)</span> <b>bbockelm:</b></span> hm - it at least went back to yellow (primary shards are recovered!).<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:09:25)</span> <b>bbockelm:</b></span> when does it start accepting records again?<br/>
<span style="color: #3c8c69"><span style="font-size: small">(09:15:38)</span> <b>kretzke:</b></span> it has been, albeit slowly. I think it will start indexing once the primary is recovered<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:19:59)</span> <b>bbockelm:</b></span> how can you tell?  Everything looks flatlined still...<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:20:17)</span> <b>bbockelm:</b></span> ah - I see the 'index rate'<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:22:07)</span> <b>bbockelm:</b></span> so it probably has a of some sort inside of logstash before it'll start reading off RabbitMQ again?<br/>
<span style="color: #3c8c69"><span style="font-size: small">(09:22:12)</span> <b>kretzke:</b></span> might have to kick the stash agents, logs stopped recording retries at 13:06<br/>
<span style="color: #3c8c69"><span style="font-size: small">(09:23:07)</span> <b>kretzke:</b></span> @marian around? good exercise<br/>
<span style="color: #53b759"><span style="font-size: small">(09:26:12)</span> <b>marian:</b></span> I'll be available within 15mins if can wait<br/>
<span style="color: #3c8c69"><span style="font-size: small">(09:28:00)</span> <b>kretzke:</b></span> rabbitmq won’t explode for 16 minutes, so I think it’s ok :wink:<br/>
<span style="color: #3c8c69"><span style="font-size: small">(09:31:43)</span> <b>kretzke:</b></span> probably can handle a couple more hours of records. speaking of that, we still have a task to work out the typical ingest rate so we can tell Marina how big the rabbitmq disk needs to be<br/>
<span style="color: #3c8c69"><span style="font-size: small">(09:32:24)</span> <b>kretzke:</b></span> other part of that equation is response time<br/>
<span style="color: #3c8c69"><span style="font-size: small">(09:39:56)</span> <b>kretzke:</b></span> roughly 250 kB/s. so 1 GB/hr? <a href="https://gracc.opensciencegrid.org/dashboard/db/rabbitmq-queues?refresh=1m&amp;panelId=4&amp;fullscreen&amp;orgId=1">https://gracc.opensciencegrid.org/dashboard/db/rabbitmq-queues?refresh=1m&amp;panelId=4&amp;fullscreen&amp;orgId=1</a><br/>
<span style="color: #9e3997"><span style="font-size: small">(09:44:47)</span> <b>bbockelm:</b></span> fwiw - ES has a big backlog of pending tasks, which might be the reason logstash is "stuck".  I wouldn't restart it until the shards are all assigned.<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:45:13)</span> <b>bbockelm:</b></span> At the current rate, that'll be about 10:30.<br/>
<span style="color: #674b1b"><span style="font-size: small">(10:22:29)</span> <b>rynge:</b></span> I thought CreateTime would have milliseconds. Is that not right, or am I just looking on data where that is not true?<br/>
<span style="color: #3c8c69"><span style="font-size: small">(10:25:17)</span> <b>kretzke:</b></span> no, I don’t think any times have more than second precision<br/>
<span style="color: #3c8c69"><span style="font-size: small">(10:26:59)</span> <b>kretzke:</b></span> I take that back, the <tt>@received</tt> timestamp (when the record was processed and put into elasticsearch) has millis. I haven’t seen millis in any raw records though that I can recall<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:38:06)</span> <b>bbockelm:</b></span> @kretzke - OK, I think it could use an expert eyeball again.  We've plateaued at a certain level of records ... pulling in FIFE-batch now though<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:39:02)</span> <b>bbockelm:</b></span> is it possible to kick over perhaps logstash for osg-itb?  What's the preferred way to restart logstash?<br/>
<span style="color: #3c8c69"><span style="font-size: small">(10:40:35)</span> <b>kretzke:</b></span> I was going to work @marian though that… I do <pre>cd /etc/gracc/docker/gracc.osg-itb<br/>docker-compose restart gracc-stash-raw</pre><br/>
<span style="color: #53b759"><span style="font-size: small">(10:41:10)</span> <b>marian:</b></span> ok, you did so or should I do so<br/>
<span style="color: #53b759"><span style="font-size: small">(10:41:50)</span> <b>marian:</b></span> how about gracc-stash-summary?<br/>
<span style="color: #53b759"><span style="font-size: small">(10:42:36)</span> <b>marian:</b></span> ok, do so, got it<br/>
<span style="color: #3c8c69"><span style="font-size: small">(10:42:42)</span> <b>kretzke:</b></span> I haven’t done anything<br/>
<span style="color: #3c8c69"><span style="font-size: small">(10:43:12)</span> <b>kretzke:</b></span> also, to see logs, there’s two options: <pre>sudo journalctl CONTAINER_NAME=graccosg_gracc-stash-raw_1 -n 5<br/>-- Logs begin at Fri 2017-03-10 17:28:38 UTC, end at Fri 2017-04-21 15:42:17 UTC. --<br/>Apr 21 13:06:33 gratiav2-1.novalocal dockerd[23184]: [503] {"error":{"root_cause":[{"type":"cluster_block_exception","reason":"blocked by: [SERVICE_UNAVAILABLE/2/no master];"}],"type":"cluster_block_excep<br/>Apr 21 13:06:35 gratiav2-1.novalocal dockerd[23184]: Attempted to send a bulk request to Elasticsearch configured at '["<a href="http://localhost:9200">http://localhost:9200</a>"]', but an error occurred and it failed! Are you sure you can <br/>Apr 21 13:06:35 gratiav2-1.novalocal dockerd[23184]: [503] {"error":{"root_cause":[{"type":"cluster_block_exception","reason":"blocked by: [SERVICE_UNAVAILABLE/2/no master];"}],"type":"cluster_block_excep<br/>Apr 21 13:06:37 gratiav2-1.novalocal dockerd[23184]: Attempted to send a bulk request to Elasticsearch configured at '["<a href="http://localhost:9200">http://localhost:9200</a>"]', but an error occurred and it failed! Are you sure you can <br/>Apr 21 13:06:37 gratiav2-1.novalocal dockerd[23184]: [503] {"error":{"root_cause":[{"type":"cluster_block_exception","reason":"blocked by: [SERVICE_UNAVAILABLE/2/no master];"}],"type":"cluster_block_excep<br/></pre><br/>
<span style="color: #3c8c69"><span style="font-size: small">(10:44:00)</span> <b>kretzke:</b></span> or <pre>cd /etc/gracc/docker/gracc.osg-itb<br/>docker-compose logs --tail 5 gracc-stash-raw</pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(10:45:31)</span> <b>bbockelm:</b></span> @kretzke - the top of the task queue is this:<br/><pre><br/>  "tasks" : [<br/>    {<br/>      "insert_order" : 156594,<br/>      "priority" : "HIGH",<br/>      "source" : "shard-failed",<br/>      "executing" : true,<br/>      "time_in_queue_millis" : 8831435,<br/>      "time_in_queue" : "2.4h"<br/>    }<br/></pre><br/>
<span style="color: #3c8c69"><span style="font-size: small">(10:46:22)</span> <b>kretzke:</b></span> ah, interesting!<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:47:03)</span> <b>bbockelm:</b></span> I'd rather get these tasks un-stuck first before we restart.<br/>
<span style="color: #53b759"><span style="font-size: small">(10:49:21)</span> <b>marian:</b></span> for gracc.osg-itb isn't is container_name=graccosgitb_gracc-stash-raw_1<br/>
<span style="color: #3c8c69"><span style="font-size: small">(10:50:51)</span> <b>kretzke:</b></span> yes<br/>
<span style="color: #53b759"><span style="font-size: small">(10:51:09)</span> <b>marian:</b></span> ok<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:01:56)</span> <b>bbockelm:</b></span> @kretzke I note the number of nodes is 6 ... is that too high?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:02:41)</span> <b>bbockelm:</b></span> ah, the client.<br/>
<span style="color: #3c8c69"><span style="font-size: small">(11:02:43)</span> <b>kretzke:</b></span> no, there’s two client nodes on -1<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:06:15)</span> <b>rynge:</b></span> @kretzke So I need something fairly unique to see what has not been processed yet. We used to use the dbid in Gratia for this. How is receivedtime different from createtime?<br/>
<blockquote>
<span style="color: #3c8c69"><span style="font-size: small">(11:07:08)</span> <b>kretzke:</b></span> rynge: CreateTime is when the probe created the record. You want @received, which is when the record was put into elasticsearch<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:09:40)</span> <b>rynge:</b></span> Ok, thanks<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:11:05)</span> <b>rynge:</b></span> So is the @ something special? Any ideas how it maps in dsl?<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:11:08)</span> <b>rynge:</b></span> s = Search(using=self._establish_client(), index=index) \<br/>                .query("wildcard", ProbeName=wildcardProbeNameq) \<br/>                .filter("wildcard", ProjectName=wildcardProjectName) \<br/>                .filter("range", @received={"gt": start_time}) \<br/>                .filter("term", ResourceType="Payload")<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:21:44)</span> <b>rynge:</b></span> Nevermind, I figured it out:<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:21:47)</span> <b>rynge:</b></span> s = Search(using=self._establish_client(), index=index) \<br/>                .query("wildcard", ProbeName=wildcardProbeNameq) \<br/>                .filter("wildcard", ProjectName=wildcardProjectName) \<br/>                .filter("range", ** {'@received': {'gt': start_time}}) \<br/>                .filter("term", ResourceType="Payload")<br/>
<span style="color: #3c8c69"><span style="font-size: small">(11:24:46)</span> <b>kretzke:</b></span> ah, cool<br/>
</blockquote>
<span style="color: #9e3997"><span style="font-size: small">(11:06:19)</span> <b>bbockelm:</b></span> well... I'm stumped.  Any ideas?<br/>
<span style="color: #3c8c69"><span style="font-size: small">(11:08:21)</span> <b>kretzke:</b></span> this is a little old in elasticsearch terms <a href="https://t37.net/how-to-fix-your-elasticsearch-cluster-stuck-in-initializing-shards-mode.html">https://t37.net/how-to-fix-your-elasticsearch-cluster-stuck-in-initializing-shards-mode.html</a><br/>
<span style="color: #3c8c69"><span style="font-size: small">(11:09:34)</span> <b>kretzke:</b></span> most are stuck on -4 <pre>[kretzke@gratiav2-1 gracc.osg-itb]$ curl -s localhost:9200/_cat/shards | grep INIT<br/>gracc.osg.raw3-2017.04           2 r INITIALIZING 12543656   18.8gb 10.71.101.64 gratia2-2<br/>osg-2016-03-14                   4 r INITIALIZING    59213  189.1mb 10.71.101.66 gratia2-4<br/>osg-2016-03-10                   3 r INITIALIZING    36785  122.2mb 10.71.101.66 gratia2-4<br/>osg-2016-03-11                   4 r INITIALIZING    64373  220.9mb 10.71.101.66 gratia2-4<br/>osg-2016-03-11                   3 r INITIALIZING    63943  192.7mb 10.71.101.66 gratia2-4<br/>gracc-osg-1970.01.01             2 r INITIALIZING     3084    3.3mb 10.71.101.66 gratia2-4<br/>gracc-osg-1970.01.01             3 r INITIALIZING     3058    3.5mb 10.71.101.66 gratia2-4<br/>gracc.osg-itb.raw3-2017.04       1 r INITIALIZING  1771494    2.3gb 10.71.101.64 gratia2-2<br/>osg-2016-03-12                   2 r INITIALIZING    63438  221.7mb 10.71.101.66 gratia2-4<br/>osg-2016-03-12                   3 r INITIALIZING    63611  192.3mb 10.71.101.66 gratia2-4<br/>osg-2016-03-13                   2 r INITIALIZING    70243  224.4mb 10.71.101.66 gratia2-4<br/>osg-2016-03-13                   0 r INITIALIZING    70439    210mb 10.71.101.66 gratia2-4</pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(11:14:07)</span> <b>bbockelm:</b></span> nothing horribly interesting there.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:21:36)</span> <b>bbockelm:</b></span> Ah!  <tt>gracc.osg.raw3-2017.04</tt>  that's a big honkin' one.<br/>
<span style="color: #3c8c69"><span style="font-size: small">(11:43:24)</span> <b>kretzke:</b></span> -4 seems to have a higher share of indexing than the others. afaik indexing takes priority over replicas. could try taking down the client node to stop ingest, may give it a chance to finish<br/>
<span style="color: #3c8c69"><span style="font-size: small">(11:45:16)</span> <b>kretzke:</b></span> that’s my suggestion, anyways. if that fails, may have to cycle elasticsearch on -4<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:49:43)</span> <b>bbockelm:</b></span> yeah, I think I'm going to cycle ES on -4.<br/>
<span style="color: #3c8c69"><span style="font-size: small">(11:51:19)</span> <b>kretzke:</b></span> it’s risky, with unassigned shards. not sure how easy it is, but could reassign all primaries off it first<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:56:38)</span> <b>bbockelm:</b></span> there's a hella lot of GC on -5.<br/>
<span style="color: #3c8c69"><span style="font-size: small">(11:57:59)</span> <b>kretzke:</b></span> yeah, I saw that… it’s the master fwiw<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:59:26)</span> <b>bbockelm:</b></span> hm - how to best stop the client?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:59:50)</span> <b>bbockelm:</b></span> just <tt>sytemctl stop elasticsearch</tt> on <tt>-1</tt>?<br/>
<span style="color: #3c8c69"><span style="font-size: small">(11:59:54)</span> <b>kretzke:</b></span> yep<br/>
<span style="color: #3c8c69"><span style="font-size: small">(12:00:10)</span> <b>kretzke:</b></span> can also stop elasticsearch-ro to stop searching<br/>
<span style="color: #3c8c69"><span style="font-size: small">(12:07:55)</span> <b>kretzke:</b></span> I have to get going soon. If it comes to it, before cycling -4 I’d suggest trying to force assign the unassigned shards as described in that link above<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:17:33)</span> <b>bbockelm:</b></span> yeah - I think I'm just going to leave it untouched for a bit.<br/>
<span style="color: #16569E"><span style="font-size: small">(13:47:12)</span> <b>edquist:</b></span> @bbockelm, @blin was wondering what the consequences are / how bad it is if we miss a day or three of submitting apel reports (in the event of something unexpected).  I don't know the answer; do you?<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:06:12)</span> <b>bbockelm:</b></span> It's not too bad - they are cumulative over the whole month.  Just don't miss it around the end of the month.<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:06:22)</span> <b>bbockelm:</b></span> @kretzke - after much trickery, I nursed it back to health.<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:06:36)</span> <b>bbockelm:</b></span> basically, once recovery was done, there were so many tasks piled up that the master froze.<br/>
<span style="color: #235e5b"><span style="font-size: small">(14:08:31)</span> <b>dweitzel:</b></span> What did you do to clear the tasks?<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:11:31)</span> <b>bbockelm:</b></span> uh.  The master froze, so I tipped over the process.<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:12:12)</span> <b>bbockelm:</b></span> however, there was less to reload, so it went faster.<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:12:33)</span> <b>bbockelm:</b></span> I did crank up various concurrency settings since Ceph is not a single spinning disk.<br/>
<span style="color: #9e3997"><span style="font-size: small">(14:21:33)</span> <b>bbockelm:</b></span> backlog logged!<br/>
<span style="color: #3c8c69"><span style="font-size: small">(14:55:25)</span> <b>kretzke:</b></span> Cool. I've wondered at times if we should try dedicated masters.<br/>
<span style="color: #9e3997"><span style="font-size: small">(15:41:38)</span> <b>bbockelm:</b></span> Yeah - probably a dedicated master and another data node would help.<br/>
<span style="color: #9e3997"><span style="font-size: small">(15:42:02)</span> <b>bbockelm:</b></span> Piece by piece, I plan on having @marian rebuild the entire seutp.<br/>
<span style="color: #53b759"><span style="font-size: small">(15:44:07)</span> <b>marian:</b></span> noted<br/>
<span style="color: #16569E"><span style="font-size: small">(16:16:08)</span> <b>edquist:</b></span> @marian, how should i get you my ssh private key?  and, where will it live?<br/>
<span style="color: #53b759"><span style="font-size: small">(16:16:41)</span> <b>marian:</b></span> private?<br/>
<span style="color: #16569E"><span style="font-size: small">(16:16:54)</span> <b>edquist:</b></span> yes<br/>
<span style="color: #16569E"><span style="font-size: small">(16:16:57)</span> <b>edquist:</b></span> i mean<br/>
<span style="color: #53b759"><span style="font-size: small">(16:17:10)</span> <b>marian:</b></span> I thought I'd first put in your public that you can access host<br/>
<span style="color: #16569E"><span style="font-size: small">(16:17:10)</span> <b>edquist:</b></span> the private key is needed for this<br/>
<span style="color: #16569E"><span style="font-size: small">(16:17:18)</span> <b>edquist:</b></span> nope<br/>
<span style="color: #16569E"><span style="font-size: small">(16:17:25)</span> <b>edquist:</b></span> the public key is not needed for anything<br/>
<span style="color: #53b759"><span style="font-size: small">(16:17:27)</span> <b>marian:</b></span> yeah, you're beyond my thinking, you know :slightly_smiling_face:<br/>
<span style="color: #16569E"><span style="font-size: small">(16:17:28)</span> <b>edquist:</b></span> just the private key<br/>
<span style="color: #16569E"><span style="font-size: small">(16:18:29)</span> <b>edquist:</b></span> at least, my understanding was that the docker host would have (in a reasonably safe place?) a copy of my private key (for now), which would get bind-mounted into the docker container when it ran<br/>
<span style="color: #16569E"><span style="font-size: small">(16:18:42)</span> <b>edquist:</b></span> the private key is what's needed to access my user on jump<br/>
<span style="color: #16569E"><span style="font-size: small">(16:19:01)</span> <b>edquist:</b></span> the public key is the one on jump<br/>
<span style="color: #53b759"><span style="font-size: small">(16:19:50)</span> <b>marian:</b></span> I see ... but dont' we just cut off jump part when we finally deploy to docker? for now we can run in a mode you have, or?<br/>
<span style="color: #16569E"><span style="font-size: small">(16:20:15)</span> <b>edquist:</b></span> the public keys are the ones you add to the <tt>authorized_keys</tt> file on a host that you want to log into<br/>
<span style="color: #53b759"><span style="font-size: small">(16:20:25)</span> <b>marian:</b></span> yep<br/>
<span style="color: #16569E"><span style="font-size: small">(16:20:45)</span> <b>edquist:</b></span> ah yeah i guess<br/>
<span style="color: #16569E"><span style="font-size: small">(16:20:53)</span> <b>edquist:</b></span> for now it can run as is,<br/>
<span style="color: #16569E"><span style="font-size: small">(16:21:12)</span> <b>edquist:</b></span> i just didn't know what the timing was for taking over the whole ssm submit process<br/>
<span style="color: #16569E"><span style="font-size: small">(16:25:41)</span> <b>edquist:</b></span> we'll probably worry about the rest next week; but yeah for now it'll run as-is<br/>
<span style="color: #16569E"><span style="font-size: small">(16:26:08)</span> <b>edquist:</b></span> New Catch Phrase<br/>
<span style="color: #16569E"><span style="font-size: small">(16:26:16)</span> <b>edquist:</b></span> It's Accounting Season<br/>
<span style="color: #16569E"><span style="font-size: small">(16:26:28)</span> <b>edquist:</b></span> Have you gotten your graccination yet?<br/>
<span style="color: #16569E"><span style="font-size: small">(16:26:40)</span> <b>edquist:</b></span> lol<br/>
<span style="color: #53b759"><span style="font-size: small">(16:31:19)</span> <b>marian:</b></span> heh<br/>
<span style="color: #53b759"><span style="font-size: small">(16:31:23)</span> <b>marian:</b></span> I need one<br/>
<span style="color: #53b759"><span style="font-size: small">(16:31:48)</span> <b>marian:</b></span> gracc shot ... Tuesday, no worries :wink:<br/>
</body>
</html>

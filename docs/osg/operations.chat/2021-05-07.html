<!DOCTYPE html>
<html>
<head>
<title>Fri May 7, 2021 : #operations (osg)</title>
</head>
<body>
<h3>Fri May 7, 2021 : #operations (osg)</h3>
<span style="color: #de5f24"><span style="font-size: small">(11:04:59)</span> <b>tannenba:</b></span> @rynge or @jdost321 or @bbockelm or whomever....  I am trying to locate the HTCondor logs from an execute node in OSG (i.e. the pilot logs).  I know submit machine (<a href="http://login05.osgconnect.net">login05.osgconnect.net</a>), I know the job id I care about (9340813.0), I know where it ran (Purdue-Geddes).   I am looking at xd-login:/local-scratch/gfaclogs/sdsc where apparently the pilot logs are rsynced, but have no idea how to find the logs I want.  How do I "map" the submit machine to a factory glideinWMS entry?  Please help.<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:24:30)</span> <b>rynge:</b></span> The docker container logs do not come back<br/>
<span style="color: #674b1b"><span style="font-size: small">(11:24:37)</span> <b>rynge:</b></span> We will have to ask the site for those<br/>
<span style="color: #43761b"><span style="font-size: small">(11:37:39)</span> <b>blin:</b></span> @tannenba what Mats said: the software team can help with that<br/>
<span style="color: #de5f24"><span style="font-size: small">(11:51:24)</span> <b>tannenba:</b></span> Thanks @blin and @rynge.  BrianL, could you  or someone in your team get the log(s) for me?  I can provide additional details (i.e. specific hosts etc).  One such instance I'd like to see is:<br/><pre>05/07/21 10:25:06 (9340813.0) (2713351): Request to run on slot1@Purdue-Geddes-bd7ec70ec7e6 &lt;172.17.0.2:43814?CCBID=192.170.227.251:9881%3faddrs%3d192.170.227.251-9881+[2605-9a00-10-400d-7686-7aff-fedd-d118]-9881%26alias%3dflock.opensciencegrid.org#1518842&amp;PrivNet=Purdue-Geddes-bd7ec70ec7e6&amp;addrs=172.17.0.2-43814&amp;alias=Purdue-Geddes-bd7ec70ec7e6&gt; was ACCEPTED</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(11:52:01)</span> <b>blin:</b></span> the best thing we can do atm is facilitate a conversation. i'll open a ticket and CC you on it<br/>
<span style="color: #43761b"><span style="font-size: small">(11:52:40)</span> <b>blin:</b></span> which logs are you looking for exactly?<br/>
<span style="color: #43761b"><span style="font-size: small">(11:55:18)</span> <b>blin:</b></span> @tannenba also, what do you need them for?<br/>
<span style="color: #674b1b"><span style="font-size: small">(12:03:36)</span> <b>rynge:</b></span> @blin Slightly related - I wonder if we could expose some tag/version inside the container and get it advertised in HTCondor. I'm curious if these sites ever pull new versions of the container.<br/>
<span style="color: #43761b"><span style="font-size: small">(12:04:23)</span> <b>blin:</b></span> my guess is they generally don't<br/>
<span style="color: #43761b"><span style="font-size: small">(12:04:27)</span> <b>blin:</b></span> but yeah, we should definitely do that<br/>
<span style="color: #43761b"><span style="font-size: small">(12:15:38)</span> <b>blin:</b></span> ticket with purdue: <a href="https://support.opensciencegrid.org/a/tickets/67232">https://support.opensciencegrid.org/a/tickets/67232</a><br/>
<span style="color: #43761b"><span style="font-size: small">(12:22:33)</span> <b>blin:</b></span> advertising container tag in the startd ticket here: <a href="https://opensciencegrid.atlassian.net/browse/SOFTWARE-4602">https://opensciencegrid.atlassian.net/browse/SOFTWARE-4602</a><br/>
<span style="color: #674b1b"><span style="font-size: small">(12:24:25)</span> <b>rynge:</b></span> Thanks!<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:00:45)</span> <b>tannenba:</b></span> @rynge @blin Thanks for the logs.  Can I inspect this Docker container that we have Purdue running?  Is it in DockerHub or ?   The problem with the jobs at Purdue is the USER_JOB_WRAPPER fails to launch.  Wondering if this docker container includes /bin/bash ....<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:03:49)</span> <b>rynge:</b></span> <a href="https://github.com/opensciencegrid/osgvo-docker-pilot">https://github.com/opensciencegrid/osgvo-docker-pilot</a><br/>
<span style="color: #43761b"><span style="font-size: small">(14:03:51)</span> <b>blin:</b></span> <a href="https://hub.docker.com/r/opensciencegrid/osgvo-docker-pilot">https://hub.docker.com/r/opensciencegrid/osgvo-docker-pilot</a><br/>
<span style="color: #43761b"><span style="font-size: small">(14:04:00)</span> <b>blin:</b></span> :checkered_flag:<br/>
<span style="color: #43761b"><span style="font-size: small">(14:04:11)</span> <b>blin:</b></span> docs to deploy it here: <a href="https://opensciencegrid.org/docs/resource-sharing/os-backfill-containers/#running-the-container-with-docker">https://opensciencegrid.org/docs/resource-sharing/os-backfill-containers/#running-the-container-with-docker</a><br/>
<span style="color: #674b1b"><span style="font-size: small">(14:04:15)</span> <b>rynge:</b></span> Note that we are running it other places and it works fine<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:04:40)</span> <b>rynge:</b></span> This is also why I was curious about if it was a recent pull or not<br/>
<span style="color: #43761b"><span style="font-size: small">(14:05:02)</span> <b>blin:</b></span> it's somewhat recent, they're at least running with supervisord<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:05:11)</span> <b>rynge:</b></span> Ok, good<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:06:19)</span> <b>rynge:</b></span> So we got a log from Purdue?<br/>
<span style="color: #43761b"><span style="font-size: small">(14:06:31)</span> <b>blin:</b></span> ya out of band<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:06:56)</span> <b>rynge:</b></span> Ok, also it seems other jobs are running fine there<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:11:35)</span> <b>rynge:</b></span> It is also interesting that the user requests 36 GB RAM, wants singularity but does not specify an image<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:17:50)</span> <b>tannenba:</b></span> So here is the starter dying:<br/><pre>05/07/21 15:23:46 (pid:19486) Set filetransfer runtime ads to /tmp/osgvo-pilot-23648/lib/condor/execute/dir_19486/.<a href="http://job.ad">job.ad</a> and /tmp/osgvo-pilot-23648/lib/condor/execute/dir_19486/.<a href="http://machine.ad">machine.ad</a>.<br/>05/07/21 15:23:47 (pid:19486) File transfer completed successfully.<br/>05/07/21 15:23:48 (pid:19486) Job 9049408.6963 set to execute immediately<br/>05/07/21 15:23:48 (pid:19486) Starting a VANILLA universe job with ID: 9049408.6963<br/>05/07/21 15:23:48 (pid:19486) IWD: /tmp/osgvo-pilot-23648/lib/condor/execute/dir_19486<br/>05/07/21 15:23:48 (pid:19486) Renice expr "0" evaluated to 0<br/>05/07/21 15:23:48 (pid:19486) Running job as user same uid as parent: personal condor<br/>05/07/21 15:23:48 (pid:19486) Cannot find/execute USER_JOB_WRAPPER file /tmp/osgvo-pilot-23648/user-job-wrapper.sh<br/>05/07/21 15:23:48 (pid:19486) Failed to start job, exiting<br/>05/07/21 15:23:48 (pid:19486) ShutdownFast all jobs.<br/>05/07/21 15:23:48 (pid:19486) Failed to open '.<a href="http://update.ad">update.ad</a>' to read update ad: No such file or directory (2).<br/>05/07/21 15:23:48 (pid:19486) All jobs have exited... starter exiting</pre><br/>
<span style="color: #43761b"><span style="font-size: small">(14:20:34)</span> <b>blin:</b></span> that's pretty bizarre<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:21:00)</span> <b>tannenba:</b></span> The container above has HTCondor v9.0.0 in it, but all the Purdue nodes showing up in the OSP collector claim they are v8.9.11<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:21:58)</span> <b>tannenba:</b></span> So with this container....  it already has HTCondor in it.... does that mean glideinWMS does not pull in HTCondor via curl on the worker node like it does everywhere else?  I am confused about the workflow here<br/>
<span style="color: #43761b"><span style="font-size: small">(14:22:46)</span> <b>blin:</b></span> it does not, glideinwms isn't really involved with these containers<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:24:07)</span> <b>tannenba:</b></span> So where does the user_job_wrapper.sh come from?<br/>
<span style="color: #43761b"><span style="font-size: small">(14:26:15)</span> <b>blin:</b></span> that file is created at runtime <a href="https://github.com/opensciencegrid/osgvo-docker-pilot/blob/master/10-setup-htcondor.sh#L106-L114direct">https://github.com/opensciencegrid/osgvo-docker-pilot/blob/master/10-setup-htcondor.sh#L106-L114direct</a><br/>
<span style="color: #43761b"><span style="font-size: small">(14:26:50)</span> <b>blin:</b></span> and <tt>/usr/sbin/osgvo-user-add-wrapper</tt> is pulled from github at build time <a href="https://github.com/opensciencegrid/osgvo-docker-pilot/blob/master/Dockerfile#L21">https://github.com/opensciencegrid/osgvo-docker-pilot/blob/master/Dockerfile#L21</a><br/>
<span style="color: #e0a729"><span style="font-size: small">(14:27:18)</span> <b>gthain:</b></span> What would it take to be able to start this container locally, <tt>docker exec</tt> into it, and accept jobs from the failing VO?<br/>
<span style="color: #43761b"><span style="font-size: small">(14:28:12)</span> <b>blin:</b></span> hold your horses, i don't think OSG is a failing VO :slightly_smiling_face:<br/>
<span style="color: #43761b"><span style="font-size: small">(14:29:21)</span> <b>blin:</b></span> you need an IDTOKEN from the CM (fastest thing here would be to get one from @rynge) then you follow the instructions here <a href="https://opensciencegrid.org/docs/resource-sharing/os-backfill-containers/#running-the-container-with-docker">https://opensciencegrid.org/docs/resource-sharing/os-backfill-containers/#running-the-container-with-docker</a><br/>
<span style="color: #e0a729"><span style="font-size: small">(14:32:05)</span> <b>gthain:</b></span> So, when this container runs, does it accept payload jobs directly?<br/>
<span style="color: #e0a729"><span style="font-size: small">(14:32:17)</span> <b>gthain:</b></span> or does it accept pilot jobs which then themselves run payloads?<br/>
<span style="color: #43761b"><span style="font-size: small">(14:32:54)</span> <b>blin:</b></span> payloads directly<br/>
<span style="color: #43761b"><span style="font-size: small">(14:33:14)</span> <b>blin:</b></span> the container ~ a normal OSG VO pilot<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:37:52)</span> <b>tannenba:</b></span> What is setting config knob USER_JOB_WRAPPER?  the glidein startup script ?<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:38:50)</span> <b>tannenba:</b></span> Never mind, see it in the config<br/>
<span style="color: #e0a729"><span style="font-size: small">(14:39:32)</span> <b>gthain:</b></span> that doesn't match the error, though<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:39:41)</span> <b>tannenba:</b></span> So in this container, USER_JOB_WRAPPER = /usr/sbin/osgvo-user-job-wrapper, but when the job runs it is using USER_JOB_WRAPPER file /tmp/osgvo-pilot-23648/user-job-wrapper.sh<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:41:17)</span> <b>tannenba:</b></span> Are we sure any jobs run at this site ?<br/>
<span style="color: #43761b"><span style="font-size: small">(14:42:01)</span> <b>blin:</b></span> looks like a fair amount <a href="https://gracc.opensciencegrid.org/d/000000037/payload-jobs-summary?orgId=1&amp;var-ReportableVOName=All&amp;var-Project=All&amp;var-Facility=Purdue%20Geddes&amp;var-User=All&amp;var-ExitCode=All&amp;var-Probe=All&amp;var-interval=1d&amp;var-Organization=All">https://gracc.opensciencegrid.org/d/000000037/payload-jobs-summary?orgId=1&amp;var-Reportable[…]e=All&amp;var-Probe=All&amp;var-interval=1d&amp;var-Organization=All</a><br/>
<span style="color: #43761b"><span style="font-size: small">(14:47:01)</span> <b>blin:</b></span> that's pretty weird. i'm a little fuzzy on all the different condor configs so I think @rynge would have to weigh in on that<br/>
<span style="color: #43761b"><span style="font-size: small">(14:47:17)</span> <b>blin:</b></span> oh lol, i think i see<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:47:40)</span> <b>tannenba:</b></span> File 10-setup-htcondor.sh<br/>
<span style="color: #43761b"><span style="font-size: small">(14:47:46)</span> <b>blin:</b></span> <a href="https://github.com/opensciencegrid/osgvo-docker-pilot/blob/master/10-setup-htcondor.sh#L116">https://github.com/opensciencegrid/osgvo-docker-pilot/blob/master/10-setup-htcondor.sh#L116</a><br/>
<span style="color: #43761b"><span style="font-size: small">(14:47:47)</span> <b>blin:</b></span> yeah<br/>
<span style="color: #de5f24"><span style="font-size: small">(14:52:16)</span> <b>tannenba:</b></span> Currently suspicious that in this container, MOUNT_UNDER_SCRATCH = /tmp, and yet the execute directory is also in /tmp.  This is bad.  Normally it would not matter for a non-root pilot (since this does mount namespaces), but I note in the container startup instructions it says  to use<tt>--cap-add=SYS_ADMIN</tt><br/>
<span style="color: #674b1b"><span style="font-size: small">(16:10:00)</span> <b>rynge:</b></span> We have like 3000 jobs on the site currently<br/>
<span style="color: #de5f24"><span style="font-size: small">(16:22:24)</span> <b>tannenba:</b></span> Looks like not all the Purdue nodes, but some?  Like right now happening continuously with node Purdue-Geddes-4b8e9a127e46<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:24:49)</span> <b>rynge:</b></span> Interesting<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:31:00)</span> <b>rynge:</b></span> It might be a little bit safer to put the job wrapper user /usr/sbin or something like that<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:31:34)</span> <b>rynge:</b></span> There is no need to have it under the "dynamic" pilot directory<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:32:51)</span> <b>rynge:</b></span> I can really explain why it is missing, but it could either be some thing that went wrong during the initial startup, or maybe a script or a job escaped and was able to remove files outside the sandbox<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:33:31)</span> <b>rynge:</b></span> It is personal condor, so ownership is the same between that dynamically created directory and the job<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:34:28)</span> <b>rynge:</b></span> @blin We might also have to rethink the auto restart of HTCondor here<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:34:46)</span> <b>rynge:</b></span> Or limit it somehow so the container does not live forever in a crappy state<br/>
<span style="color: #43761b"><span style="font-size: small">(16:35:34)</span> <b>blin:</b></span> i'm fine with getting rid of it and letting the container die<br/>
<span style="color: #de5f24"><span style="font-size: small">(16:38:20)</span> <b>tannenba:</b></span> It would be safer to not have Local Dir and thus EXECUTE under /tmp<br/>
<span style="color: #de5f24"><span style="font-size: small">(16:38:20)</span> <b>tannenba:</b></span> It would be safer to not have EXECUTE on /tmp<br/>
<span style="color: #43761b"><span style="font-size: small">(16:40:35)</span> <b>blin:</b></span> hrm, i'm not sure where we can put it. these containers are also supposed to be startable through singularity<br/>
<span style="color: #43761b"><span style="font-size: small">(16:41:19)</span> <b>blin:</b></span> i guess we can create the necessary dirs under <tt>/</tt>  directly as long as the dir's world writable?<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:41:20)</span> <b>rynge:</b></span> Well we have had that discussion before - this is supposed to be like a glidein - a user should be able to start it from their own user account<br/>
<span style="color: #43761b"><span style="font-size: small">(16:45:51)</span> <b>blin:</b></span> <a href="https://opensciencegrid.atlassian.net/browse/SOFTWARE-4609">https://opensciencegrid.atlassian.net/browse/SOFTWARE-4609</a><br/>
<span style="color: #de5f24"><span style="font-size: small">(16:46:28)</span> <b>tannenba:</b></span> It is happening on a lot of Purdue nodes.  I cannot explain why it is not happening on all of them.<br/>
<span style="color: #de5f24"><span style="font-size: small">(16:53:19)</span> <b>tannenba:</b></span> Here is a pretty good way to detect them - a condor_status that says show me machines that are currently not running any jobs, and yet have tried to start a bunch of jobs<br/>
<span style="color: #de5f24"><span style="font-size: small">(16:53:36)</span> <b>tannenba:</b></span> <pre>[tannenba@login05 condor]$ condor_status -cons 'Cpus==DetectedCpus &amp;&amp; JobStarts &gt; 5' -af Machine JobStarts <br/>Purdue-Geddes-01632fc33c9a 1203<br/>Purdue-Geddes-4b8e9a127e46 466<br/>Purdue-Geddes-4c31c656fb98 1538<br/>Purdue-Geddes-17ec965090d0 2052<br/>Purdue-Geddes-43e31ff267ca 260<br/>Purdue-Geddes-96fcc261bbe5 62<br/>Purdue-Geddes-285c1f84602e 115<br/>Purdue-Geddes-479a69694b95 951<br/>Purdue-Geddes-834d34a2d4eb 448<br/>Purdue-Geddes-3840f2ab1a08 231<br/>Purdue-Geddes-23903fde8ab1 275<br/>Purdue-Geddes-92190fc49c7a 2223<br/>Purdue-Geddes-1626201a213c 918<br/>Purdue-Geddes-a9a5d7136b94 904<br/>Purdue-Geddes-a49586de970c 2189<br/>Purdue-Geddes-b9b36682c007 1009<br/>Purdue-Geddes-ba139a0d76fa 407<br/>Purdue-Geddes-ba859199653e 167<br/>Purdue-Geddes-bd7ec70ec7e6 250<br/>Purdue-Geddes-bfa4d992d63d 290<br/>Purdue-Geddes-d22555278269 167<br/>Purdue-Geddes-da53b8d686dd 1236<br/>Purdue-Geddes-daa373c4ab06 994<br/>Purdue-Geddes-f09cdea7b29a 300<br/>Purdue-Geddes-fede3929fa54 3761</pre><br/>
<span style="color: #de5f24"><span style="font-size: small">(16:54:00)</span> <b>tannenba:</b></span> You can see some of these machines have suckered in thousands of jobs!<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:54:21)</span> <b>rynge:</b></span> Huh, the job wrapper situation is odd! <tt>/usr/sbin/osgvo-user-job-wrapper</tt> is the one we need for things like Singularity to work, <tt>$LOCAL_DIR/user-job-wrapper.sh</tt> is the one used for local site config<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:54:49)</span> <b>rynge:</b></span> I think we are running the wrong wrapper<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:55:01)</span> <b>rynge:</b></span> Which means all Singularity jobs would fail<br/>
<span style="color: #43761b"><span style="font-size: small">(16:55:35)</span> <b>blin:</b></span> the latter calls the former, though, doesn't it?<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:55:51)</span> <b>rynge:</b></span> Ah, it does<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:56:07)</span> <b>rynge:</b></span> Ok, that makes sense<br/>
<span style="color: #674b1b"><span style="font-size: small">(16:56:11)</span> <b>rynge:</b></span> I think<br/>
<span style="color: #674b1b"><span style="font-size: small">(17:06:17)</span> <b>rynge:</b></span> Well, the only thing I can come up with quickly is removing the restart in supervisord<br/>
<span style="color: #674b1b"><span style="font-size: small">(17:06:34)</span> <b>rynge:</b></span> At least that would prevent broken containers from coming back up<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:07:44)</span> <b>tannenba:</b></span> Do we know why this only happens on some nodes and not others?<br/>
<span style="color: #674b1b"><span style="font-size: small">(17:07:52)</span> <b>rynge:</b></span> No clue<br/>
<span style="color: #674b1b"><span style="font-size: small">(17:09:58)</span> <b>rynge:</b></span> We can try recreating it locally, but it could take time<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:10:32)</span> <b>tannenba:</b></span> It seems specific to Purdue nodes that have 20 cores and 64GB RAM<br/>
<span style="color: #674b1b"><span style="font-size: small">(17:15:07)</span> <b>rynge:</b></span> Did you get full logs from Purdue? I mean all the way from when the container started?<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:17:03)</span> <b>tannenba:</b></span> I just forwarded you the one execute node logs files we got to your email.  Hope you can receive a 25MB attachment :slightly_smiling_face:.<br/>
<span style="color: #de5f24"><span style="font-size: small">(17:18:55)</span> <b>tannenba:</b></span> Looks to me like we just have the HTCondor "log" directory from inside the container<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:24:46)</span> <b>bbockelm:</b></span> @ayounts @goughes ^^^ I know you guys are likely out for the day, but in the offchance you get bored this evening, would be curious in seeing what's "special" about the containers Todd references above.<br/>
</body>
</html>

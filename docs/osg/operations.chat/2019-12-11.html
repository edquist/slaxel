<!DOCTYPE html>
<html>
<head>
<title>Wed Dec 11, 2019 : #operations (osg)</title>
</head>
<body>
<h3>Wed Dec 11, 2019 : #operations (osg)</h3>
<span style="color: #9e3997"><span style="font-size: small">(09:05:30)</span> <b>bbockelm:</b></span> @lincoln - does uchicago run the hardware for <a href="http://flock.opensciencegrid.org">flock.opensciencegrid.org</a>?<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:06:42)</span> <b>bbockelm:</b></span> Basically, from UWM, we're seeing weird issues with accessing that host - some files hang indefinitely and others go smoothly.<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:08:46)</span> <b>bbockelm:</b></span> it seems to depend a lot on filesize - files &gt; 32KB hang but smaller ones are OK.<br/>
<span style="color: #235e5b"><span style="font-size: small">(09:14:13)</span> <b>dweitzel:</b></span> (sorry for multiple threads) @marco.mascheroni I don't see any gracc records from hosted-ce13, though fermilab says they have over a thousand cores are running currently.  Who can access that host and check the gratia probe?<br/>
<span style="color: #e96699"><span style="font-size: small">(09:23:36)</span> <b>lincoln:</b></span> @bbockelm yes<br/>
<span style="color: #e96699"><span style="font-size: small">(09:24:28)</span> <b>lincoln:</b></span> Hm it's not an MTU issue somewhere is it ?<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:24:45)</span> <b>bbockelm:</b></span> Smells like an MTU issue!<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:25:48)</span> <b>bbockelm:</b></span> @marco.mascheroni was able to show that downloading large files from CERN was OK.<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:25:53)</span> <b>bbockelm:</b></span> What's the MTU for flock?<br/>
<span style="color: #e96699"><span style="font-size: small">(09:50:43)</span> <b>lincoln:</b></span> 9k<br/>
<span style="color: #e96699"><span style="font-size: small">(09:51:07)</span> <b>lincoln:</b></span> do we have any packet loss between UWM and UC?<br/>
<span style="color: #e96699"><span style="font-size: small">(09:51:12)</span> <b>lincoln:</b></span> I don't suppose they have a perfSonar node?<br/>
<span style="color: #e23f99"><span style="font-size: small">(09:54:18)</span> <b>marco.mascheroni:</b></span> @dweitzel I can access it, I will do it in a bit<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:54:32)</span> <b>bbockelm:</b></span> As a test, can we crank down the MTU on just the flock host?<br/>
<span style="color: #e96699"><span style="font-size: small">(09:54:48)</span> <b>lincoln:</b></span> we can<br/>
<span style="color: #e96699"><span style="font-size: small">(09:55:19)</span> <b>lincoln:</b></span> let me first check that i have console access incase i completely bust it :slightly_smiling_face:<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:55:39)</span> <b>bbockelm:</b></span> They have at least part of the infrastructure working -- the submit host for the UWM cluster can access the larger files but the worker nodes can't.<br/>
<span style="color: #e96699"><span style="font-size: small">(09:56:00)</span> <b>lincoln:</b></span> :thinking_face:<br/>
<span style="color: #e96699"><span style="font-size: small">(09:56:16)</span> <b>lincoln:</b></span> traceroute job from a worker to flock? :slightly_smiling_face:<br/>
<span style="color: #e96699"><span style="font-size: small">(09:56:44)</span> <b>lincoln:</b></span> i have a meeting right now but lets come back to this in a bit<br/>
<span style="color: #9e3997"><span style="font-size: small">(09:56:49)</span> <b>bbockelm:</b></span> it's in the ticket... hold on.<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:01:14)</span> <b>bbockelm:</b></span> Freshdesk #63060<br/>
<span style="color: #43761b"><span style="font-size: small">(10:04:50)</span> <b>blin:</b></span> public URL: <a href="https://support.opensciencegrid.org/public/tickets/3d14ba4df1010c50ae304f13fa51c4789927eed63356cf15d42e6eaf1f87c6bf">https://support.opensciencegrid.org/public/tickets/3d14ba4df1010c50ae304f13fa51c4789927eed63356cf15d42e6eaf1f87c6bf</a><br/>
<span style="color: #e23f99"><span style="font-size: small">(11:44:38)</span> <b>marco.mascheroni:</b></span> @dweitzel about ce13 not reporting I am udating the gratia packages that seemed a bit old. I'll run osg-configure after that. This is what I had:<br/><pre>gratia-probe-common.x86_64                                                          1.20.8-1.osg34.el7                                                     @osg-testing<br/>gratia-probe-htcondor-ce.x86_64                                                     1.19.0-1.osg34.el7                                                     @osg        <br/>osg-configure-gratia.noarch                                                         2.2.3-1.osg34.el7                                                      @osg</pre><br/>
<span style="color: #235e5b"><span style="font-size: small">(11:45:17)</span> <b>dweitzel:</b></span> it should have been reporting something, can you make sure the service is running?<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:45:27)</span> <b>dweitzel:</b></span> anything in the gratia logs, /var/log/gratia/...<br/>
<blockquote>
<span style="color: #e23f99"><span style="font-size: small">(15:02:28)</span> <b>marco.mascheroni:</b></span> @dweitzel the service was running, before the yum update of the packages I had:<br/><pre>11:27:00 CST Gratia: Info: suppressing record with GlobalJobId <a href="http://condor.hosted-ce13.grid.uchicago.edu#55590.0#1575736035">condor.hosted-ce13.grid.uchicago.edu#55590.0#1575736035</a> due to Grid == Local<br/>11:27:00 CST Gratia: Moving transient input file: /var/lib/gratia/data/history.55590.0 to quarantine in /var/lib/gratia/data/quarantine<br/>11:27:00 CST Gratia: OK: No unsuppressed usage records in this packet: not sending<br/>11:27:00 CST Gratia: ***********************************************************<br/>11:27:00 CST Gratia: Creating a UsageRecord 2019-12-11T17:27:00Z<br/>11:27:00 CST Gratia: ArbritaryList: ['']<br/>11:27:00 CST Gratia: Registering transient input file: /var/lib/gratia/data/history.55592.0<br/>11:27:00 CST Gratia: ***********************************************************<br/>11:27:00 CST Gratia: ERROR: unable to find valid certinfo file for job 55592<br/>11:27:00 CST Gratia: Saved record to /var/lib/gratia/tmp/gratiafiles/subdir.htcondor-ce_<a href="http://hosted-ce13.grid.uchicago.edu">hosted-ce13.grid.uchicago.edu</a>_<a href="http://gratia-osg-prod.opensciencegrid.org">gratia-osg-prod.opensciencegrid.org</a>_80/outbox/r.1<br/>813673.gratia.xml__QF7TmpuQlf<br/>11:27:00 CST Gratia: Deleting transient input file: /var/lib/gratia/data/history.55592.0<br/>11:27:00 CST Gratia: OK - Record added to bundle (2/100)<br/>11:27:00 CST Gratia: ***********************************************************<br/>11:27:00 CST Gratia: Processing bundle file: <br/>11:27:00 CST Gratia: Processing bundle file: /var/lib/gratia/tmp/gratiafiles/subdir.htcondor-ce_<a href="http://hosted-ce13.grid.uchicago.edu">hosted-ce13.grid.uchicago.edu</a>_<a href="http://gratia-osg-prod.opensciencegrid.org">gratia-osg-prod.opensciencegrid.org</a>_80/ou<br/>tbox/r.1813673.gratia.xml__QF7TmpuQlf<br/>11:27:00 CST Gratia: Connection via HTTP to: <a href="http://gratia-osg-prod.opensciencegrid.org:80">gratia-osg-prod.opensciencegrid.org:80</a><br/>11:27:01 CST Gratia: Bundle response indicates success, /var/lib/gratia/tmp/gratiafiles/subdir.htcondor-ce_<a href="http://hosted-ce13.grid.uchicago.edu">hosted-ce13.grid.uchicago.edu</a>_gratia-osg-prod.opensciencegri<br/><a href="http://d.org">d.org</a>_80/outbox/r.1813673.gratia.xml__QF7TmpuQlf will be deleted<br/>11:27:01 CST Gratia: OK - Processed bundle with 2 records:  OK<br/>11:27:01 CST Gratia: ***********************************************************<br/>11:27:01 CST Gratia: Removing log files older than 31 days from /var/log/gratia<br/>11:27:01 CST Gratia: /var/log/gratia uses 0.068% and there is 51% free<br/>11:27:01 CST Gratia: Removing incomplete data files older than 31 days from /var/lib/gratia/data/<br/>11:27:01 CST Gratia: Removing quarantines data files older than 31 days from /var/lib/gratia/data/quarantine<br/>11:27:01 CST Gratia: /var/lib/gratia/data/quarantine uses 0.050% and there is 51% free<br/>11:27:01 CST Gratia: End of execution summary: new records sent successfully: 1<br/>11:27:01 CST Gratia:                           new records suppressed: 1<br/>11:27:01 CST Gratia:                           new records failed: 0<br/>11:27:01 CST Gratia:                           records reprocessed successfully: 0<br/>11:27:01 CST Gratia:                           reprocessed records failed: 0<br/>11:27:01 CST Gratia:                           handshake records sent successfully: 1<br/>11:27:01 CST Gratia:                           handshake records failed: 0<br/>11:27:01 CST Gratia:                           bundle of records sent successfully: 1<br/>11:27:01 CST Gratia:                           bundle of records failed: 0<br/>11:27:01 CST Gratia:                           outstanding records: 0<br/>11:27:01 CST Gratia:                           outstanding staged records: 0<br/>11:27:01 CST Gratia:                           outstanding records tar files: 0<br/>11:27:01 CST Gratia: End-of-execution disconnect ...</pre><br/>
<span style="color: #e23f99"><span style="font-size: small">(15:02:39)</span> <b>marco.mascheroni:</b></span> Waiting to see if the update helped<br/>
<span style="color: #235e5b"><span style="font-size: small">(15:03:04)</span> <b>dweitzel:</b></span> Are there jobs running there?<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:03:23)</span> <b>marco.mascheroni:</b></span> also, I believe the topology and local99.ini data are not correct, I need to fix them,<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:03:33)</span> <b>marco.mascheroni:</b></span> yes, there are running jobs<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:03:52)</span> <b>marco.mascheroni:</b></span> [root@hosted-ce13 hosted-ce-config]# condor_ce_q<br/><br/><br/>-- Schedd: <a href="http://hosted-ce13.grid.uchicago.edu">hosted-ce13.grid.uchicago.edu</a> : &lt;128.135.158.238:5617&gt; @ 12/11/19 15:03:40<br/>OWNER BATCH_NAME                 SUBMITTED   DONE   RUN    IDLE  TOTAL JOB_IDS<br/>uscms CMD: glidein_startup.sh  12/7  10:48      _     44    556    600 55594.0 ... 56199.0<br/><br/>600 jobs; 0 completed, 0 removed, 556 idle, 44 running, 0 held, 0 suspended<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:04:25)</span> <b>marco.mascheroni:</b></span> And there were more: <a href="http://fermifactory02.fnal.gov:8319/monitor/factoryStatus.html?entry=CMSHTPC_T3_US_Bridges">http://fermifactory02.fnal.gov:8319/monitor/factoryStatus.html?entry=CMSHTPC_T3_US_Bridges</a><br/>
<span style="color: #235e5b"><span style="font-size: small">(15:05:17)</span> <b>dweitzel:</b></span> Seems odd that there has never been any records in gracc from that node. <br/>
<span style="color: #e23f99"><span style="font-size: small">(15:12:36)</span> <b>marco.mascheroni:</b></span> I guess nobody ever checked?<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:14:29)</span> <b>marco.mascheroni:</b></span> anyway, would it be ok if I changed topology filenames, 99local.ini configuration and factory (hepcloud) xml to make it consistent with other ces<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:14:30)</span> <b>marco.mascheroni:</b></span> ?<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:14:56)</span> <b>marco.mascheroni:</b></span> since there is nothing I guess we are not losing much<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:16:10)</span> <b>marco.mascheroni:</b></span> I'll do that tomorrow if it is ok<br/>
<span style="color: #235e5b"><span style="font-size: small">(15:20:55)</span> <b>dweitzel:</b></span> Sure<br/>
<span style="color: #235e5b"><span style="font-size: small">(15:54:08)</span> <b>dweitzel:</b></span> Do you know how many cores that specific entry point is suppose to allocate per glidein. <br/>
<span style="color: #235e5b"><span style="font-size: small">(20:14:00)</span> <b>dweitzel:</b></span> I found the hosted-ce13 records in the quarantine.  Lots of problems with them.  They hit this bug: <a href="https://opensciencegrid.atlassian.net/browse/SOFTWARE-3474">https://opensciencegrid.atlassian.net/browse/SOFTWARE-3474</a> and some didn't have some attributes that were required.   I wrote a custom one-off parser to clean them up and send them back into GRACC (<a href="https://github.com/opensciencegrid/gracc-tools/tree/master/gracc-oneoffs/gracc-reprocess-hostedce">https://github.com/opensciencegrid/gracc-tools/tree/master/gracc-oneoffs/gracc-reprocess-hostedce</a>).  BTW, I think I re-learned today that gratia records XML is not valid XML.<br/>
</blockquote>
<span style="color: #9e3997"><span style="font-size: small">(12:53:21)</span> <b>bbockelm:</b></span> @lincoln - time to come back to the UWM thing above?<br/>
<span style="color: #e96699"><span style="font-size: small">(12:53:27)</span> <b>lincoln:</b></span> sure<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:53:55)</span> <b>bbockelm:</b></span> basically, lemme know when you knock the MTU down and I can re-test the <tt>curl</tt> command at the UWM side.<br/>
<span style="color: #e96699"><span style="font-size: small">(12:54:14)</span> <b>lincoln:</b></span> okay<br/>
<span style="color: #e96699"><span style="font-size: small">(12:57:55)</span> <b>lincoln:</b></span> @bbockelm done<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:58:13)</span> <b>bbockelm:</b></span> works like magic!<br/>
<span style="color: #e96699"><span style="font-size: small">(12:58:30)</span> <b>lincoln:</b></span> sounds like they have an MTU issue somewhere in their path :slightly_smiling_face:<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:59:10)</span> <b>bbockelm:</b></span> indeed.  Could we hold flock at the lower MTU for awhile to help them debug?<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:59:32)</span> <b>bbockelm:</b></span> I noticed <a href="http://xd-login.opensciencegrid.org">xd-login.opensciencegrid.org</a> has the same issue; having two hosts with different MTUs in the same subnet would be helpful for A/B testing.<br/>
<span style="color: #e96699"><span style="font-size: small">(12:59:49)</span> <b>lincoln:</b></span> fine by me- @rynge?<br/>
<span style="color: #674b1b"><span style="font-size: small">(13:02:08)</span> <b>rynge:</b></span> Go for it<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:02:46)</span> <b>bbockelm:</b></span> gotta run -- Lincoln, I CC'd you on my response so you will know when things get resolved at UWM.<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:02:49)</span> <b>bbockelm:</b></span> (yay!  more email!)<br/>
<span style="color: #674b1b"><span style="font-size: small">(13:03:10)</span> <b>rynge:</b></span> . o O ( at some point we could join the 2000's and have IPv6 on it as well )<br/>
<span style="color: #e96699"><span style="font-size: small">(13:04:25)</span> <b>lincoln:</b></span> we have V6 whenever you want<br/>
<span style="color: #e96699"><span style="font-size: small">(13:04:27)</span> <b>lincoln:</b></span> :slightly_smiling_face:<br/>
<span style="color: #e96699"><span style="font-size: small">(13:04:38)</span> <b>lincoln:</b></span> I can set a V6 address on it right now if you like<br/>
<span style="color: #674b1b"><span style="font-size: small">(13:54:54)</span> <b>rynge:</b></span> No that is not the problem - it is more the HTCondor / CCB / ipv4 hosts interacting with ipv6 glideins I would worry about<br/>
<span style="color: #9e3997"><span style="font-size: small">(13:59:39)</span> <b>bbockelm:</b></span> CMS does it!<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:07:01)</span> <b>rynge:</b></span> Yeah, but they have a bigger stick than I do<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:07:59)</span> <b>rynge:</b></span> That is, they can probably make submit host and sites change if they need to<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:08:15)</span> <b>rynge:</b></span> I'm stuck with a lowest common denominator problem<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:08:58)</span> <b>rynge:</b></span> For example, does CMS have IPv6 only submit hosts? Or IPv6 only glideins?<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:09:06)</span> <b>rynge:</b></span> Or are they all dual stacked?<br/>
<span style="color: #674b1b"><span style="font-size: small">(14:09:35)</span> <b>rynge:</b></span> I suspect that if all the submit hosts are dual stacked, things would work pretty well<br/>
<span style="color: #235e5b"><span style="font-size: small">(20:14:00)</span> <b>dweitzel:</b></span> I found the hosted-ce13 records in the quarantine.  Lots of problems with them.  They hit this bug: <a href="https://opensciencegrid.atlassian.net/browse/SOFTWARE-3474">https://opensciencegrid.atlassian.net/browse/SOFTWARE-3474</a> and some didn't have some attributes that were required.   I wrote a custom one-off parser to clean them up and send them back into GRACC (<a href="https://github.com/opensciencegrid/gracc-tools/tree/master/gracc-oneoffs/gracc-reprocess-hostedce">https://github.com/opensciencegrid/gracc-tools/tree/master/gracc-oneoffs/gracc-reprocess-hostedce</a>).  BTW, I think I re-learned today that gratia records XML is not valid XML.<br/>
<blockquote>
<span style="color: #e23f99"><span style="font-size: small">(15:02:28)</span> <b>marco.mascheroni:</b></span> @dweitzel the service was running, before the yum update of the packages I had:<br/><pre>11:27:00 CST Gratia: Info: suppressing record with GlobalJobId <a href="http://condor.hosted-ce13.grid.uchicago.edu#55590.0#1575736035">condor.hosted-ce13.grid.uchicago.edu#55590.0#1575736035</a> due to Grid == Local<br/>11:27:00 CST Gratia: Moving transient input file: /var/lib/gratia/data/history.55590.0 to quarantine in /var/lib/gratia/data/quarantine<br/>11:27:00 CST Gratia: OK: No unsuppressed usage records in this packet: not sending<br/>11:27:00 CST Gratia: ***********************************************************<br/>11:27:00 CST Gratia: Creating a UsageRecord 2019-12-11T17:27:00Z<br/>11:27:00 CST Gratia: ArbritaryList: ['']<br/>11:27:00 CST Gratia: Registering transient input file: /var/lib/gratia/data/history.55592.0<br/>11:27:00 CST Gratia: ***********************************************************<br/>11:27:00 CST Gratia: ERROR: unable to find valid certinfo file for job 55592<br/>11:27:00 CST Gratia: Saved record to /var/lib/gratia/tmp/gratiafiles/subdir.htcondor-ce_<a href="http://hosted-ce13.grid.uchicago.edu">hosted-ce13.grid.uchicago.edu</a>_<a href="http://gratia-osg-prod.opensciencegrid.org">gratia-osg-prod.opensciencegrid.org</a>_80/outbox/r.1<br/>813673.gratia.xml__QF7TmpuQlf<br/>11:27:00 CST Gratia: Deleting transient input file: /var/lib/gratia/data/history.55592.0<br/>11:27:00 CST Gratia: OK - Record added to bundle (2/100)<br/>11:27:00 CST Gratia: ***********************************************************<br/>11:27:00 CST Gratia: Processing bundle file: <br/>11:27:00 CST Gratia: Processing bundle file: /var/lib/gratia/tmp/gratiafiles/subdir.htcondor-ce_<a href="http://hosted-ce13.grid.uchicago.edu">hosted-ce13.grid.uchicago.edu</a>_<a href="http://gratia-osg-prod.opensciencegrid.org">gratia-osg-prod.opensciencegrid.org</a>_80/ou<br/>tbox/r.1813673.gratia.xml__QF7TmpuQlf<br/>11:27:00 CST Gratia: Connection via HTTP to: <a href="http://gratia-osg-prod.opensciencegrid.org:80">gratia-osg-prod.opensciencegrid.org:80</a><br/>11:27:01 CST Gratia: Bundle response indicates success, /var/lib/gratia/tmp/gratiafiles/subdir.htcondor-ce_<a href="http://hosted-ce13.grid.uchicago.edu">hosted-ce13.grid.uchicago.edu</a>_gratia-osg-prod.opensciencegri<br/><a href="http://d.org">d.org</a>_80/outbox/r.1813673.gratia.xml__QF7TmpuQlf will be deleted<br/>11:27:01 CST Gratia: OK - Processed bundle with 2 records:  OK<br/>11:27:01 CST Gratia: ***********************************************************<br/>11:27:01 CST Gratia: Removing log files older than 31 days from /var/log/gratia<br/>11:27:01 CST Gratia: /var/log/gratia uses 0.068% and there is 51% free<br/>11:27:01 CST Gratia: Removing incomplete data files older than 31 days from /var/lib/gratia/data/<br/>11:27:01 CST Gratia: Removing quarantines data files older than 31 days from /var/lib/gratia/data/quarantine<br/>11:27:01 CST Gratia: /var/lib/gratia/data/quarantine uses 0.050% and there is 51% free<br/>11:27:01 CST Gratia: End of execution summary: new records sent successfully: 1<br/>11:27:01 CST Gratia:                           new records suppressed: 1<br/>11:27:01 CST Gratia:                           new records failed: 0<br/>11:27:01 CST Gratia:                           records reprocessed successfully: 0<br/>11:27:01 CST Gratia:                           reprocessed records failed: 0<br/>11:27:01 CST Gratia:                           handshake records sent successfully: 1<br/>11:27:01 CST Gratia:                           handshake records failed: 0<br/>11:27:01 CST Gratia:                           bundle of records sent successfully: 1<br/>11:27:01 CST Gratia:                           bundle of records failed: 0<br/>11:27:01 CST Gratia:                           outstanding records: 0<br/>11:27:01 CST Gratia:                           outstanding staged records: 0<br/>11:27:01 CST Gratia:                           outstanding records tar files: 0<br/>11:27:01 CST Gratia: End-of-execution disconnect ...</pre><br/>
<span style="color: #e23f99"><span style="font-size: small">(15:02:39)</span> <b>marco.mascheroni:</b></span> Waiting to see if the update helped<br/>
<span style="color: #235e5b"><span style="font-size: small">(15:03:04)</span> <b>dweitzel:</b></span> Are there jobs running there?<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:03:23)</span> <b>marco.mascheroni:</b></span> also, I believe the topology and local99.ini data are not correct, I need to fix them,<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:03:33)</span> <b>marco.mascheroni:</b></span> yes, there are running jobs<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:03:52)</span> <b>marco.mascheroni:</b></span> [root@hosted-ce13 hosted-ce-config]# condor_ce_q<br/><br/><br/>-- Schedd: <a href="http://hosted-ce13.grid.uchicago.edu">hosted-ce13.grid.uchicago.edu</a> : &lt;128.135.158.238:5617&gt; @ 12/11/19 15:03:40<br/>OWNER BATCH_NAME                 SUBMITTED   DONE   RUN    IDLE  TOTAL JOB_IDS<br/>uscms CMD: glidein_startup.sh  12/7  10:48      _     44    556    600 55594.0 ... 56199.0<br/><br/>600 jobs; 0 completed, 0 removed, 556 idle, 44 running, 0 held, 0 suspended<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:04:25)</span> <b>marco.mascheroni:</b></span> And there were more: <a href="http://fermifactory02.fnal.gov:8319/monitor/factoryStatus.html?entry=CMSHTPC_T3_US_Bridges">http://fermifactory02.fnal.gov:8319/monitor/factoryStatus.html?entry=CMSHTPC_T3_US_Bridges</a><br/>
<span style="color: #235e5b"><span style="font-size: small">(15:05:17)</span> <b>dweitzel:</b></span> Seems odd that there has never been any records in gracc from that node. <br/>
<span style="color: #e23f99"><span style="font-size: small">(15:12:36)</span> <b>marco.mascheroni:</b></span> I guess nobody ever checked?<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:14:29)</span> <b>marco.mascheroni:</b></span> anyway, would it be ok if I changed topology filenames, 99local.ini configuration and factory (hepcloud) xml to make it consistent with other ces<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:14:30)</span> <b>marco.mascheroni:</b></span> ?<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:14:56)</span> <b>marco.mascheroni:</b></span> since there is nothing I guess we are not losing much<br/>
<span style="color: #e23f99"><span style="font-size: small">(15:16:10)</span> <b>marco.mascheroni:</b></span> I'll do that tomorrow if it is ok<br/>
<span style="color: #235e5b"><span style="font-size: small">(15:20:55)</span> <b>dweitzel:</b></span> Sure<br/>
<span style="color: #235e5b"><span style="font-size: small">(15:54:08)</span> <b>dweitzel:</b></span> Do you know how many cores that specific entry point is suppose to allocate per glidein. <br/>
<span style="color: #235e5b"><span style="font-size: small">(20:14:00)</span> <b>dweitzel:</b></span> I found the hosted-ce13 records in the quarantine.  Lots of problems with them.  They hit this bug: <a href="https://opensciencegrid.atlassian.net/browse/SOFTWARE-3474">https://opensciencegrid.atlassian.net/browse/SOFTWARE-3474</a> and some didn't have some attributes that were required.   I wrote a custom one-off parser to clean them up and send them back into GRACC (<a href="https://github.com/opensciencegrid/gracc-tools/tree/master/gracc-oneoffs/gracc-reprocess-hostedce">https://github.com/opensciencegrid/gracc-tools/tree/master/gracc-oneoffs/gracc-reprocess-hostedce</a>).  BTW, I think I re-learned today that gratia records XML is not valid XML.<br/>
</blockquote>
</body>
</html>

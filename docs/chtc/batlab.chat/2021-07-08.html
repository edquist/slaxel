<!DOCTYPE html>
<html>
<head>
<title>Thu Jul 8, 2021 : #batlab (chtc)</title>
</head>
<body>
<h3>Thu Jul 8, 2021 : #batlab (chtc)</h3>
<span style="color: #bb86b7"><span style="font-size: small">(08:12:18)</span> <b>tim:</b></span> It appears the the procd on e2460 reports short reads fairly frequently:<br/>
<span style="color: #bb86b7"><span style="font-size: small">(08:15:07)</span> <b>tim:</b></span> <pre>--<br/>07/07/21 07:19:09 : ProcAPI: read 1420 pid entries out of 1482 total entries in /proc<br/>07/07/21 07:19:09 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 1420 is suddenly too much smaller than the previous read of 1622<br/>--<br/>07/07/21 14:03:52 : ProcAPI: read 2387 pid entries out of 2449 total entries in /proc<br/>07/07/21 14:03:52 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 2387 is suddenly too much smaller than the previous read of 2768<br/>--<br/>07/07/21 14:10:09 : ProcAPI: read 2968 pid entries out of 3030 total entries in /proc<br/>07/07/21 14:10:09 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 2968 is suddenly too much smaller than the previous read of 3349<br/>--<br/>07/07/21 14:23:02 : ProcAPI: read 3139 pid entries out of 3201 total entries in /proc<br/>07/07/21 14:23:02 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 3139 is suddenly too much smaller than the previous read of 3534<br/>--<br/>07/07/21 15:21:41 : ProcAPI: read 2186 pid entries out of 2248 total entries in /proc<br/>07/07/21 15:21:41 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 2186 is suddenly too much smaller than the previous read of 2476<br/>--<br/>07/07/21 15:24:16 : ProcAPI: read 1449 pid entries out of 1511 total entries in /proc<br/>07/07/21 15:24:16 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 1449 is suddenly too much smaller than the previous read of 2035<br/>--<br/>07/07/21 15:24:20 : ProcAPI: read 1208 pid entries out of 1270 total entries in /proc<br/>07/07/21 15:24:20 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 1208 is suddenly too much smaller than the previous read of 1449<br/>--<br/>07/07/21 15:32:09 : ProcAPI: read 2725 pid entries out of 2787 total entries in /proc<br/>07/07/21 15:32:09 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 2725 is suddenly too much smaller than the previous read of 3112<br/>--<br/>07/07/21 15:32:54 : ProcAPI: read 2684 pid entries out of 2746 total entries in /proc<br/>07/07/21 15:32:54 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 2684 is suddenly too much smaller than the previous read of 3373<br/>--<br/>07/07/21 15:34:10 : ProcAPI: read 2423 pid entries out of 2485 total entries in /proc<br/>07/07/21 15:34:10 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 2423 is suddenly too much smaller than the previous read of 2734<br/>--<br/>07/07/21 16:13:46 : ProcAPI: read 2924 pid entries out of 2986 total entries in /proc<br/>07/07/21 16:13:46 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 2924 is suddenly too much smaller than the previous read of 3634<br/>--<br/>07/07/21 16:27:54 : ProcAPI: read 3487 pid entries out of 3549 total entries in /proc<br/>07/07/21 16:27:54 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 3487 is suddenly too much smaller than the previous read of 3955<br/>--<br/>07/07/21 21:26:26 : ProcAPI: read 2578 pid entries out of 2640 total entries in /proc<br/>07/07/21 21:26:26 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 2578 is suddenly too much smaller than the previous read of 2961<br/>--<br/>07/08/21 00:40:57 : ProcAPI: read 2486 pid entries out of 2548 total entries in /proc<br/>07/08/21 00:40:57 : PROCAPI_RETRY_FRACTION = 0.900000 means that the current read of 2486 is suddenly too much smaller than the previous read of 2807</pre><br/>
<span style="color: #d58247"><span style="font-size: small">(08:17:20)</span> <b>gthain:</b></span> It does have a lot to read...<br/>
<span style="color: #bb86b7"><span style="font-size: small">(08:18:49)</span> <b>tim:</b></span> Nice work speeding things up. Now if we would properly leverage those 8 core slots on the Mac, we could build fast there as well.<br/>
<span style="color: #d58247"><span style="font-size: small">(08:21:31)</span> <b>gthain:</b></span> I have some ideas there...<br/>
<span style="color: #d58247"><span style="font-size: small">(10:18:17)</span> <b>gthain:</b></span> With Tim's help, we are now building all the dockerized platforms -j 8, and build times have dropped to about 6 minutes<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:18:40)</span> <b>tlmiller:</b></span> w00t!<br/>
<span style="color: #d58247"><span style="font-size: small">(10:18:52)</span> <b>gthain:</b></span> next step is that we are flocking to e2460, and it takes at least 5 minutes to get there via flocking...<br/>
<span style="color: #d58247"><span style="font-size: small">(10:19:08)</span> <b>gthain:</b></span> So, I want to configure e2460 to reverse-flock to the batlab pool, to knock this time off<br/>
<span style="color: #d58247"><span style="font-size: small">(10:19:24)</span> <b>gthain:</b></span> this requires some infrastructure help to configure the security of the batlab cm<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:20:19)</span> <b>tlmiller:</b></span> By "reverse flock", do you mean anything other than adding <a href="http://cm.batlab.org">cm.batlab.org</a> to its COLLECTOR_HOST?<br/>
<span style="color: #d58247"><span style="font-size: small">(10:20:30)</span> <b>gthain:</b></span> That's exactly what I mean<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:20:53)</span> <b>tlmiller:</b></span> What needs to be changed, security-wise?<br/>
<span style="color: #d58247"><span style="font-size: small">(10:21:11)</span> <b>gthain:</b></span> e2460 claims it needs a token authorized on the cm<br/>
<span style="color: #d58247"><span style="font-size: small">(10:21:29)</span> <b>gthain:</b></span> Dunno how to do that in a way that survives puppet<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:21:55)</span> <b>tlmiller:</b></span> I didn't think CHTC had moved to tokens yet.<br/>
<span style="color: #d58247"><span style="font-size: small">(10:22:19)</span> <b>gthain:</b></span> This is why I need help from infrastructure.  TimS says he should have time today<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:22:26)</span> <b>tlmiller:</b></span> My impression is that Puppet doesn't blow away files it doesn't know about, at least in some ...<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:22:35)</span> <b>tlmiller:</b></span> ... which means the message is almost certainly _not_ the actual problem ...<br/>
<span style="color: #d58247"><span style="font-size: small">(10:22:54)</span> <b>gthain:</b></span> Is the batlab pool all host-based?<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:23:00)</span> <b>tlmiller:</b></span> ... cases where I've worried about it.<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:23:53)</span> <b>tlmiller:</b></span> I don't actually recall.  But we should be able to set the security knobs in the .flightworthy file for now, and just put the signing key in the right place.<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:24:12)</span> <b>tlmiller:</b></span> If we can't, we can move the signing key to a wrong place that Puppet doesn't know about. :)<br/>
<span style="color: #d58247"><span style="font-size: small">(10:24:38)</span> <b>gthain:</b></span> Also, changed NEGOTIATOR_CYCLE_DELAY in the C3 condors from the default 20 seconds to 2, and the test suite got 5 minutes faster.  Now taking about 27 minutes on the master branch<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:25:58)</span> <b>tlmiller:</b></span> What do we think about the startd starting to blow right up after we started mixing builds in?<br/>
<span style="color: #d58247"><span style="font-size: small">(10:26:12)</span> <b>gthain:</b></span> Feels like a bug<br/>
<span style="color: #d58247"><span style="font-size: small">(10:26:39)</span> <b>gthain:</b></span> I condor_rm'd a bunch of running jobs sometime in there, that may have been the proximate cause<br/>
<span style="color: #d58247"><span style="font-size: small">(10:27:18)</span> <b>gthain:</b></span> Also, I think there is a missing dependency in cmake somewhere that it causing the Ubuntu 20 builds to fail when built in parallel occassionally<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:27:20)</span> <b>tlmiller:</b></span> I'd buy that the procd-related failure was probably caused CPU exhaustion, but I was thinking more about getting green tests...<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:27:47)</span> <b>tlmiller:</b></span> There's definitely a missing dependency; I've seen it happen every so often interactively.<br/>
<span style="color: #d58247"><span style="font-size: small">(10:27:57)</span> <b>gthain:</b></span> somewhere in the python bindings?<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:28:09)</span> <b>tlmiller:</b></span> Because it's one of the failures that resolves itself cleanly after re-running, I haven't tracked it down.<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:28:12)</span> <b>tlmiller:</b></span> That's possible.<br/>
<span style="color: #d58247"><span style="font-size: small">(10:28:23)</span> <b>gthain:</b></span> If it happens frequently, I'll track it down<br/>
<span style="color: #d58247"><span style="font-size: small">(10:29:19)</span> <b>gthain:</b></span> The parallel builds haven't seemed to impacted the tests much<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:29:44)</span> <b>tlmiller:</b></span> ... except for the two startd failures that did (or would have) wiped all the tests out?<br/>
<span style="color: #d58247"><span style="font-size: small">(10:30:12)</span> <b>gthain:</b></span> Well ... in terms of load-induced race conditions<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:30:36)</span> <b>tlmiller:</b></span> Which is nice, but not the end-all of stability. :)<br/>
<span style="color: #d58247"><span style="font-size: small">(10:31:23)</span> <b>gthain:</b></span> I submitted 6 builds in quick succession last night with the parallel build code, and all the tests passed on those platforms<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:31:49)</span> <b>tlmiller:</b></span> Good news.<br/>
<span style="color: #d58247"><span style="font-size: small">(11:18:55)</span> <b>gthain:</b></span> Do you know what, exactly the "queue time' field in the batlab results page measures?<br/>
<span style="color: #d58247"><span style="font-size: small">(11:19:39)</span> <b>gthain:</b></span> I'm still seeing 3-4 minutes of "queue time" for builds and tests, even when there is an available slot<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:21:06)</span> <b>tlmiller:</b></span> It probably includes all the on-disk manipulations necessary to prepare the job, but I'd have to check.<br/>
<span style="color: #d58247"><span style="font-size: small">(11:22:19)</span> <b>gthain:</b></span> Maybe the next step is to get an SSD for the submit machine<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:23:38)</span> <b>tlmiller:</b></span> It is the difference in time between the duration of <tt>platform_job</tt> and the duration of <tt>remote_task</tt>,<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:24:08)</span> <b>tlmiller:</b></span> which is probably no longer accurate, if it ever was, to refer to as entirely queueing time.<br/>
<span style="color: #d58247"><span style="font-size: small">(11:24:28)</span> <b>gthain:</b></span> But it is the time not spent on the execute side<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:25:23)</span> <b>tlmiller:</b></span> The <tt>remote_task</tt> does _not_ include, e.g, <tt>remote_pre</tt>, which takes a quite a while for some reason.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:25:30)</span> <b>tlmiller:</b></span> Look at the run-details page.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:25:59)</span> <b>tlmiller:</b></span> <tt>platform_job</tt> - <tt>remote_clock</tt> is much closer to the queuing delay, probably.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:28:02)</span> <b>tlmiller:</b></span> (IIRC, I added <tt>remote_clock</tt> because there wasn't a good way to measure remote-side-only time, otherwise.)<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:29:51)</span> <b>tlmiller:</b></span> It looks like there are up to several minutes of real queuing delays, ~60 seconds of remote-side stuff not in <tt>remote_post</tt>, and another minute or so of spin-up time internal to Metronome, maybe?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:30:02)</span> <b>tlmiller:</b></span> (Which would include untarring things.)<br/>
<span style="color: #bb86b7"><span style="font-size: small">(11:47:59)</span> <b>tim:</b></span> Does downloading a Docker image figure in there somewhere?<br/>
<span style="color: #9e3997"><span style="font-size: small">(12:00:35)</span> <b>tlmiller:</b></span> Downloading a Docker image would take place in the durational difference between <tt>platform_job</tt> and <tt>remote_clock</tt>, I guess; did we not tweak the size of the Docker image cache on this host?<br/>
<span style="color: #bb86b7"><span style="font-size: small">(13:19:44)</span> <b>tim:</b></span> It would be a 2 Gig download.<br/>
<span style="color: #d58247"><span style="font-size: small">(13:21:54)</span> <b>gthain:</b></span> We upped the docker image cache size significantly<br/>
</body>
</html>

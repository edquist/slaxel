<!DOCTYPE html>
<html>
<head>
<title>Mon Jun 14, 2021 : #batlab (chtc)</title>
</head>
<body>
<h3>Mon Jun 14, 2021 : #batlab (chtc)</h3>
<span style="color: #235e5b"><span style="font-size: small">(10:28:47)</span> <b>johnkn:</b></span> I'd also like to get to the bottom of this.  But for longer term. I think setting up the test machine to have 2 or 3 core slots is the way we want to go rather than running fewer tests in parallel per test run.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:31:01)</span> <b>johnkn:</b></span> We should also focus on fixing the leaking of condor_master processes by having C2 do a better job of cleanup after each test.<br/>Windows already does this by using a windows specific mechanism, so we can use a unix specific mechanism if one exists.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:34:04)</span> <b>johnkn:</b></span> Currently, we wait 30 seconds for the master to be "up".  and 2 minutes for the rest of the daemons.  With shared port now the default, maybe the master wait time should be a larger percentage of the total?<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:48:15)</span> <b>tlmiller:</b></span> I saw part of that Friday night.  I guess I was totally wrong about the big problem being i/o.  (It might still be a problem, but whatever is causing 70%+ system -- almost certainly the procd -- is clearly a bigger problem, at least for &gt; ~10 simultaneous tests.)<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:53:53)</span> <b>johnkn:</b></span> This is from one of the startup fails.   Condor master was started at 00:16:27:00 and not up within 30 seconds.  The MasterLog has this.<br/><pre>06/12/21 00:16:28.329 (D_ALWAYS) ******************************************************<br/>06/12/21 00:16:28.330 (D_ALWAYS) ** condor_master (CONDOR_MASTER) STARTING UP<br/>06/12/21 00:16:28.330 (D_ALWAYS) ** /var/lib/condor/execute/slot1/dir_122249/userdir/condor/usr/sbin/condor_master<br/>06/12/21 00:16:28.330 (D_ALWAYS) ** SubsystemInfo: name=MASTER type=MASTER(2) class=DAEMON(1)<br/>06/12/21 00:16:28.330 (D_ALWAYS) ** Configuration: subsystem:MASTER local:&lt;NONE&gt; class:DAEMON<br/>06/12/21 00:16:28.330 (D_ALWAYS) ** $CondorVersion: 9.0.2 Jun 11 2021 BuildID: Debian-9.0.2-0.544796 PackageID: 9.0.2-0.544796 Debian-9.0.2-0.544796 $<br/>06/12/21 00:16:28.330 (D_ALWAYS) ** $CondorPlatform: X86_64-Ubuntu_18.04 $<br/>06/12/21 00:16:28.330 (D_ALWAYS) ** PID = 95481<br/>06/12/21 00:16:28.330 (D_ALWAYS) ** Log last touched time unavailable (No such file or directory)<br/>06/12/21 00:16:28.330 (D_ALWAYS) ******************************************************<br/>06/12/21 00:16:28.330 (D_ALWAYS) Using config source: /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/condor_config<br/>06/12/21 00:16:28.330 (D_ALWAYS) Using local config sources: <br/>06/12/21 00:16:28.330 (D_ALWAYS)    /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/condor_config.local<br/>06/12/21 00:16:28.330 (D_ALWAYS) config Macros = 72, Sorted = 72, StringBytes = 2325, TablesBytes = 2640<br/>06/12/21 00:16:28.330 (D_ALWAYS) CLASSAD_CACHING is OFF<br/>06/12/21 00:16:28.330 (D_ALWAYS) Daemon Log is logging: D_ALWAYS D_ERROR<br/>06/12/21 00:16:28.330 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:28.330 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:28.331 (D_ALWAYS) SharedPortEndpoint: waiting for connections to named socket master_95481_9a4b<br/>06/12/21 00:16:28.331 (D_ALWAYS) SharedPortEndpoint: failed to open /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/shared_port_ad: No such file or directory<br/>06/12/21 00:16:28.331 (D_ALWAYS) SharedPortEndpoint: did not successfully find SharedPortServer address. Will retry in 60s.<br/>06/12/21 00:16:28.331 (D_ALWAYS) DaemonCore: private command socket at &lt;127.0.0.1:0?alias=condorauto-646393.0-e2460.chtc.wisc.edu&amp;sock=master_95481_9a4b&gt;<br/>06/12/21 00:16:30.778 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 2.446820 seconds.<br/>06/12/21 00:16:35.162 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 2.351740 seconds.<br/>06/12/21 00:16:35.861 (D_ALWAYS) Adding SHARED_PORT to DAEMON_LIST, because USE_SHARED_PORT=true (to disable this, set AUTO_INCLUDE_SHARED_PORT_IN_DAEMON_LIST=False)<br/>06/12/21 00:16:35.861 (D_ALWAYS) SHARED_PORT is in front of a COLLECTOR, so it will use the configured collector port<br/>06/12/21 00:16:35.861 (D_ALWAYS) Master restart (GRACEFUL) is watching /var/lib/condor/execute/slot1/dir_122249/userdir/condor/sbin/condor_master (mtime:1623471702)<br/>06/12/21 00:16:35.863 (D_ALWAYS) Cannot remove wait-for-startup file /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/shared_port_ad<br/>06/12/21 00:16:37.295 (D_ALWAYS) WARNING: forward resolution of localhost6 doesn't match 127.0.0.1!<br/>06/12/21 00:16:37.295 (D_ALWAYS) WARNING: forward resolution of localhost6.localdomain6 doesn't match 127.0.0.1!<br/>06/12/21 00:16:37.305 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:37.305 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:37.313 (D_ALWAYS) Started DaemonCore process "/var/lib/condor/execute/slot1/dir_122249/userdir/condor/libexec/condor_shared_port", pid and pgroup = 96195<br/>06/12/21 00:16:37.313 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/shared_port_ad to appear.<br/>06/12/21 00:16:37.414 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/shared_port_ad to appear.<br/>... this repeats every .1 sec for the until<br/>06/12/21 00:16:55.706 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/shared_port_ad to appear.<br/>06/12/21 00:16:55.807 (D_ALWAYS) Found /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/shared_port_ad.<br/>06/12/21 00:16:55.807 (D_ALWAYS) Cannot remove wait-for-startup file /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address<br/>06/12/21 00:16:55.808 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:55.808 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:55.817 (D_ALWAYS) Started DaemonCore process "/var/lib/condor/execute/slot1/dir_122249/userdir/condor/sbin/condor_collector", pid and pgroup = 96940<br/>06/12/21 00:16:55.818 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:55.918 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:56.018 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:56.119 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:56.219 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:56.320 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:56.420 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:56.520 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:56.620 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:56.721 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:56.822 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:56.922 (D_ALWAYS) Waiting for /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address to appear.<br/>06/12/21 00:16:57.023 (D_ALWAYS) Found /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/.collector_address.<br/>06/12/21 00:16:57.023 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:57.023 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:57.032 (D_ALWAYS) Started DaemonCore process "/var/lib/condor/execute/slot1/dir_122249/userdir/condor/sbin/condor_schedd", pid and pgroup = 96946<br/>06/12/21 00:16:57.032 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:57.032 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:57.042 (D_ALWAYS) Started DaemonCore process "/var/lib/condor/execute/slot1/dir_122249/userdir/condor/sbin/condor_negotiator", pid and pgroup = 96947<br/>06/12/21 00:16:57.042 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:57.042 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:57.052 (D_ALWAYS) Started DaemonCore process "/var/lib/condor/execute/slot1/dir_122249/userdir/condor/sbin/condor_startd", pid and pgroup = 96948<br/>06/12/21 00:16:57.052 (D_ALWAYS) Daemons::StartAllDaemons all daemons were started<br/>06/12/21 00:17:16.090 (D_ALWAYS) Setting ready state 'Ready' for STARTD<br/>06/12/21 01:16:37.447 (D_ALWAYS) Preen pid is 44552<br/>06/12/21 01:16:38.431 (D_ALWAYS) Preen (pid 44552) exited with status 4</pre><br/>
<span style="color: #235e5b"><span style="font-size: small">(10:55:14)</span> <b>johnkn:</b></span> The master polling every .1 second and logging eache is *maybe?* a problem... ?<br/>
<span style="color: #9e3997"><span style="font-size: small">(10:56:54)</span> <b>tlmiller:</b></span> Well...<br/>• 5-7 seconds of that failure was DNS.<br/>• It took two seconds for the collector address file to appear.  Not sure why that happened, could have been DNS again.<br/>• You're right, the extra logging is probably making the situation worse.<br/>• What was the startd doing that took 19 seconds?<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:57:10)</span> <b>johnkn:</b></span> I noticed that.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:57:18)</span> <b>johnkn:</b></span> and the master was *almost* up in time.<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:57:52)</span> <b>johnkn:</b></span> And TOKEN auth is still stupidly chatty about trying to access rootly directories<br/>
<span style="color: #235e5b"><span style="font-size: small">(10:59:52)</span> <b>johnkn:</b></span> SharedPortLog.<br/><pre>06/12/21 00:16:40.673 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 2.027953 seconds.<br/>06/12/21 00:16:40.673 (D_ALWAYS) Setting maximum file descriptors to 4096.<br/>06/12/21 00:16:40.673 (D_ALWAYS) ******************************************************<br/>06/12/21 00:16:40.673 (D_ALWAYS) ** condor_shared_port (CONDOR_SHARED_PORT) STARTING UP<br/>06/12/21 00:16:40.673 (D_ALWAYS) ** /var/lib/condor/execute/slot1/dir_122249/userdir/condor/usr/lib/condor/libexec/condor_shared_port<br/>06/12/21 00:16:40.673 (D_ALWAYS) ** SubsystemInfo: name=SHARED_PORT type=SHARED_PORT(11) class=DAEMON(1)<br/>06/12/21 00:16:40.673 (D_ALWAYS) ** Configuration: subsystem:SHARED_PORT local:&lt;NONE&gt; class:DAEMON<br/>06/12/21 00:16:40.673 (D_ALWAYS) ** $CondorVersion: 9.0.2 Jun 11 2021 BuildID: Debian-9.0.2-0.544796 PackageID: 9.0.2-0.544796 Debian-9.0.2-0.544796 $<br/>06/12/21 00:16:40.673 (D_ALWAYS) ** $CondorPlatform: X86_64-Ubuntu_18.04 $<br/>06/12/21 00:16:40.673 (D_ALWAYS) ** PID = 96195<br/>06/12/21 00:16:40.673 (D_ALWAYS) ** Log last touched time unavailable (No such file or directory)<br/>06/12/21 00:16:40.673 (D_ALWAYS) ******************************************************<br/>06/12/21 00:16:40.673 (D_ALWAYS) Using config source: /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/condor_config<br/>06/12/21 00:16:40.673 (D_ALWAYS) Using local config sources: <br/>06/12/21 00:16:40.673 (D_ALWAYS)    /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/condor_config.local<br/>06/12/21 00:16:40.673 (D_ALWAYS) config Macros = 73, Sorted = 73, StringBytes = 2382, TablesBytes = 2676<br/>06/12/21 00:16:40.673 (D_ALWAYS) CLASSAD_CACHING is ENABLED<br/>06/12/21 00:16:40.673 (D_ALWAYS) Daemon Log is logging: D_ALWAYS D_ERROR<br/>06/12/21 00:16:40.673 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:40.674 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:40.674 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:40.674 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:16:40.674 (D_ALWAYS) WARNING: Condor is running on a loopback address<br/>06/12/21 00:16:40.674 (D_ALWAYS)          of this machine, and may not visible to other hosts!<br/>06/12/21 00:16:40.674 (D_ALWAYS) Daemoncore: Listening at &lt;0.0.0.0:34189&gt; on TCP (ReliSock).<br/>06/12/21 00:16:40.674 (D_ALWAYS) DaemonCore: command socket at &lt;127.0.0.1:34189?addrs=127.0.0.1-34189&amp;alias=condorauto-646393.0-e2460.chtc.wisc.edu&amp;noUDP&gt;<br/>06/12/21 00:16:40.674 (D_ALWAYS) DaemonCore: private command socket at &lt;127.0.0.1:34189?addrs=127.0.0.1-34189&amp;alias=condorauto-646393.0-e2460.chtc.wisc.edu&gt;<br/>06/12/21 00:16:43.599 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 2.924786 seconds.<br/>06/12/21 00:16:46.320 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 2.720721 seconds.<br/>06/12/21 00:16:50.243 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 3.249237 seconds.<br/>06/12/21 00:16:53.330 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 3.086538 seconds.<br/>06/12/21 00:16:55.793 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 2.463091 seconds.<br/>06/12/21 00:16:55.793 (D_ALWAYS) main_init() called<br/>06/12/21 00:16:55.793 (D_ALWAYS) About to update statistics in shared_port daemon ad file at /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/log/shared_port_ad :</pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(11:01:14)</span> <b>tlmiller:</b></span> Of course, one might ask, "Why did we find it necessary to look up the hostname from scratch on six separate occasions?"<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:01:41)</span> <b>johnkn:</b></span> Startd was counting to 128.. :wink:<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:03:18)</span> <b>johnkn:</b></span> <pre>06/12/21 00:17:00.583 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 2.069319 seconds.<br/>06/12/21 00:17:00.584 (D_ALWAYS) ******************************************************<br/>06/12/21 00:17:00.584 (D_ALWAYS) ** condor_startd (CONDOR_STARTD) STARTING UP<br/>06/12/21 00:17:00.584 (D_ALWAYS) ** /var/lib/condor/execute/slot1/dir_122249/userdir/condor/usr/sbin/condor_startd<br/>06/12/21 00:17:00.584 (D_ALWAYS) ** SubsystemInfo: name=STARTD type=STARTD(7) class=DAEMON(1)<br/>06/12/21 00:17:00.584 (D_ALWAYS) ** Configuration: subsystem:STARTD local:&lt;NONE&gt; class:DAEMON<br/>06/12/21 00:17:00.584 (D_ALWAYS) ** $CondorVersion: 9.0.2 Jun 11 2021 BuildID: Debian-9.0.2-0.544796 PackageID: 9.0.2-0.544796 Debian-9.0.2-0.544796 $<br/>06/12/21 00:17:00.584 (D_ALWAYS) ** $CondorPlatform: X86_64-Ubuntu_18.04 $<br/>06/12/21 00:17:00.584 (D_ALWAYS) ** PID = 96948<br/>06/12/21 00:17:00.584 (D_ALWAYS) ** Log last touched time unavailable (No such file or directory)<br/>06/12/21 00:17:00.584 (D_ALWAYS) ******************************************************<br/>06/12/21 00:17:00.584 (D_ALWAYS) Using config source: /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/condor_config<br/>06/12/21 00:17:00.584 (D_ALWAYS) Using local config sources: <br/>06/12/21 00:17:00.584 (D_ALWAYS)    /var/lib/condor/execute/slot1/dir_122249/userdir/test-runs/job_core_bigenv_sched.saveme/pdir95140condor/condor_config.local<br/>06/12/21 00:17:00.584 (D_ALWAYS) config Macros = 73, Sorted = 73, StringBytes = 2372, TablesBytes = 2676<br/>06/12/21 00:17:00.584 (D_ALWAYS) CLASSAD_CACHING is ENABLED<br/>06/12/21 00:17:00.584 (D_ALWAYS) Daemon Log is logging: D_ALWAYS D_ERROR<br/>06/12/21 00:17:00.584 (D_ALWAYS) SharedPortEndpoint: waiting for connections to named socket startd_95481_9a4b<br/>06/12/21 00:17:00.585 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:17:00.585 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:17:00.585 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:17:00.585 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:17:00.586 (D_ALWAYS) DaemonCore: command socket at &lt;127.0.0.1:34189?addrs=127.0.0.1-34189&amp;alias=condorauto-646393.0-e2460.chtc.wisc.edu&amp;noUDP&amp;sock=startd_95481_9a4b&gt;<br/>06/12/21 00:17:00.586 (D_ALWAYS) DaemonCore: private command socket at &lt;127.0.0.1:34189?addrs=127.0.0.1-34189&amp;alias=condorauto-646393.0-e2460.chtc.wisc.edu&amp;noUDP&amp;sock=startd_95481_9a4b&gt;<br/>06/12/21 00:17:10.007 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 2.343910 seconds.<br/>06/12/21 00:17:15.929 (D_ALWAYS) VM universe will be tested to check if it is available<br/>06/12/21 00:17:15.929 (D_ALWAYS) History file rotation is enabled.<br/>06/12/21 00:17:15.929 (D_ALWAYS)   Maximum history file size is: 20971520 bytes<br/>06/12/21 00:17:15.929 (D_ALWAYS)   Number of rotated history files is: 2<br/>06/12/21 00:17:15.930 (D_ALWAYS) Allocating auto shares for slot type 0: Cpus: auto, Memory: auto, Swap: auto, Disk: auto<br/>06/12/21 00:17:15.930 (D_ALWAYS)   slot type 0: Cpus: 1.000000, Memory: 2012, Swap: 0.78%, Disk: 0.78%<br/>06/12/21 00:17:15.930 (D_ALWAYS)   slot type 0: Cpus: 1.000000, Memory: 2012, Swap: 0.78%, Disk: 0.78%<br/>06/12/21 00:17:15.930 (D_ALWAYS)   slot type 0: Cpus: 1.000000, Memory: 2012, Swap: 0.78%, Disk: 0.78%<br/>06/12/21 00:17:15.930 (D_ALWAYS)   slot type 0: Cpus: 1.000000, Memory: 2012, Swap: 0.78%, Disk: 0.78%<br/>.... repeat a lot ...<br/>06/12/21 00:17:15.931 (D_ALWAYS)   slot type 0: Cpus: 1.000000, Memory: 2012, Swap: 0.78%, Disk: 0.78%<br/>06/12/21 00:17:15.931 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:17:15.931 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:17:15.931 (D_ALWAYS) slot1: New machine resource allocated<br/>06/12/21 00:17:15.931 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:17:15.931 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>... repeat a lot ...<br/>06/12/21 00:17:15.968 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/12/21 00:17:15.968 (D_ALWAYS) slot128: New machine resource allocated<br/>06/12/21 00:17:15.968 (D_ALWAYS) Running: /usr/bin/docker container prune -f --filter=label=org.htcondorproject=True<br/>06/12/21 00:17:15.970 (D_ALWAYS) Failed to read results from '/usr/bin/docker container prune -f --filter=label=org.htcondorproject=True': 'No such file or directory' (2)<br/>06/12/21 00:17:15.970 (D_ALWAYS) CronJobList: Adding job 'kflops'<br/>06/12/21 00:17:15.970 (D_ALWAYS) CronJobList: Adding job 'mips'<br/>06/12/21 00:17:15.970 (D_ALWAYS) CronJob: Initializing job 'kflops' (/var/lib/condor/execute/slot1/dir_122249/userdir/condor/libexec/condor_kflops)<br/>06/12/21 00:17:15.970 (D_ALWAYS) CronJob: Initializing job 'mips' (/var/lib/condor/execute/slot1/dir_122249/userdir/condor/libexec/condor_mips)<br/>06/12/21 00:17:15.974 (D_ALWAYS) ResAttribute detected we don't have unpriv user namespaces:  return is  is 1<br/>06/12/21 00:17:16.022 (D_ALWAYS) Unable to calculate keyboard/mouse idle time due to them both being USB or not present, assuming infinite idle time for these devices.<br/>06/12/21 00:17:16.028 (D_ALWAYS) slot1: State change: IS_OWNER is false<br/>06/12/21 00:17:16.028 (D_ALWAYS) slot1: Changing state: Owner -&gt; Unclaimed<br/>--- repeat a lot ...</pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(11:04:16)</span> <b>tlmiller:</b></span> I find the following 10-second gap to highly alarming:<br/><pre>06/12/21 00:17:00.586 (D_ALWAYS) DaemonCore: private command socket at &lt;127.0.0.1:34189?addrs=127.0.0.1-34189&amp;alias=condorauto-646393.0-e2460.chtc.wisc.edu&amp;noUDP&amp;sock=startd_95481_9a4b&gt;<br/>06/12/21 00:17:10.007 (D_ALWAYS) WARNING: Saw slow DNS query, which may impact entire system: getaddrinfo(<a href="http://condorauto-646393.0-e2460.chtc.wisc.edu">condorauto-646393.0-e2460.chtc.wisc.edu</a>) took 2.343910 seconds.</pre><br/><br/>
<span style="color: #235e5b"><span style="font-size: small">(11:04:25)</span> <b>johnkn:</b></span> yeah.<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:04:40)</span> <b>johnkn:</b></span> what was the other 7 ish seconds?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:04:53)</span> <b>tlmiller:</b></span> Oh, wait, that's immediately followed by another six-second gap.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:06:02)</span> <b>tlmiller:</b></span> (Tell me more about the Windows-specific mechanism for better C2 clean-ups.)<br/>
<span style="color: #d58247"><span style="font-size: small">(11:06:28)</span> <b>gthain:</b></span> Is there one condor job per test running in the C2 condor?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:06:34)</span> <b>tlmiller:</b></span> (I suspect the C2 leaking problem is that the masters are deliberately destroying the environment data we use to do process tracking.)<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:06:57)</span> <b>tlmiller:</b></span> Yes.  The C2 condor is running a DAG (bag) of tests.<br/>
<span style="color: #d58247"><span style="font-size: small">(11:07:35)</span> <b>gthain:</b></span> Gotcha.  So, I'm also curious why the C2 isn't killing orphans like a good Dickensian villian<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:07:46)</span> <b>johnkn:</b></span> On windows, tests are wrapped in timed_cmd, which will cleanup all child processes using windows Job objects<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:07:53)</span> <b>tlmiller:</b></span> See prior parenthetical remark. ;)<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:07:59)</span> <b>tlmiller:</b></span> Ah, right.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:08:04)</span> <b>tlmiller:</b></span> Thanks, I'd forgotten about that.<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:08:17)</span> <b>johnkn:</b></span> before that we leaked *a lot*<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:08:19)</span> <b>tlmiller:</b></span> Can the C2 use cgroups?<br/>
<span style="color: #d58247"><span style="font-size: small">(11:09:05)</span> <b>gthain:</b></span> ToddT claims that the master, like the other condor daemons, should kill itself if its parent pid goes away<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:09:25)</span> <b>tlmiller:</b></span> Its parent PID can't go away?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:09:35)</span> <b>tlmiller:</b></span> It's a child of init?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:09:42)</span> <b>tlmiller:</b></span> (I'm confused...)<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:10:02)</span> <b>johnkn:</b></span> we start it with <tt>-f &amp;</tt><br/>
<span style="color: #235e5b"><span style="font-size: small">(11:10:17)</span> <b>johnkn:</b></span> who is the parent of that?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:10:52)</span> <b>tlmiller:</b></span> Do we think leaking processes is contributing to the procd problem?<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:11:12)</span> <b>johnkn:</b></span> Is there a procd problem in the test suite?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:11:54)</span> <b>tlmiller:</b></span> It depends on what, exactly, <tt>-f</tt> does, but yeah, maybe that makes the parent PID the process starting it.<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:12:22)</span> <b>johnkn:</b></span> -f disables the forking, and also keeps the console open.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:12:26)</span> <b>tlmiller:</b></span> ... we're not getting to 70% "system" utilization blocking on i/o.<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:13:28)</span> <b>johnkn:</b></span> you think the procd is the reason for 70% system?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:13:37)</span> <b>tlmiller:</b></span> And leaking a giant pile of processes is bad for memory usage but shouldn't actually be causing the tests problems.<br/>
<span style="color: #d58247"><span style="font-size: small">(11:13:55)</span> <b>gthain:</b></span> leaking a giant pile of processes is bad for the procds<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:14:06)</span> <b>johnkn:</b></span> I should say (you think the 100s of procds)<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:14:12)</span> <b>tlmiller:</b></span> Sure, Greg, but is it bad enough?<br/>
<span style="color: #d58247"><span style="font-size: small">(11:14:52)</span> <b>gthain:</b></span> I supposed I should know this, but how does DNS work inside docker?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:15:05)</span> <b>tlmiller:</b></span> I think -- possibly because of GregT's malign influence -- the reason for 70% system time is that the Linux kernel has a spinlock in the wrong place.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:15:36)</span> <b>tlmiller:</b></span> IIRC, there's a some serious magic to make sure the magic hostnames resolve; TimT would know more about that.<br/>
<span style="color: #d58247"><span style="font-size: small">(11:16:00)</span> <b>gthain:</b></span> Seems criminal that it takes  multiple seconds to resolve our own hostname<br/>
<span style="color: #d58247"><span style="font-size: small">(11:16:11)</span> <b>gthain:</b></span> that should be cached in a bunch of different places<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:16:44)</span> <b>tlmiller:</b></span> Concur; including, I thought, in HTCondor itself.<br/>
<span style="color: #d58247"><span style="font-size: small">(11:17:10)</span> <b>gthain:</b></span> Yeah, in HTCondor, in /etc/hosts, in whatever dns resolver linux uses this week<br/>
<span style="color: #d58247"><span style="font-size: small">(11:17:29)</span> <b>gthain:</b></span> Do the tests need (external) networking?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:17:41)</span> <b>tlmiller:</b></span> I would hope not.<br/>
<span style="color: #d58247"><span style="font-size: small">(11:18:05)</span> <b>gthain:</b></span> I've wonder if they'd run if we submit the test job to the c1 with <tt>docker_network_type = none</tt><br/>
<span style="color: #9e3997"><span style="font-size: small">(11:18:24)</span> <b>tlmiller:</b></span> Depends on what that does to DNS.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:18:33)</span> <b>tlmiller:</b></span> (We're super-touchy about that.)<br/>
<span style="color: #d58247"><span style="font-size: small">(11:18:48)</span> <b>gthain:</b></span> (This would also make me feel better about all the tests that set the various security settings to <tt>*</tt><br/>
<span style="color: #9e3997"><span style="font-size: small">(11:21:19)</span> <b>tlmiller:</b></span> (Of course, if the procd is having problems, that could also lead to a stacking-up of masters, right?)<br/>
<span style="color: #235e5b"><span style="font-size: small">(11:21:52)</span> <b>johnkn:</b></span> I would expect a cascaded of fail, yes.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:21:55)</span> <b>tlmiller:</b></span> Can unprivileged processes make (sub)cgroups?<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:22:56)</span> <b>tlmiller:</b></span> -- I'm wondering about spending less time looking at every directory in <tt>/proc</tt>.<br/>
<span style="color: #d58247"><span style="font-size: small">(11:30:48)</span> <b>gthain:</b></span> Yes, on newer kernels<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:32:56)</span> <b>tlmiller:</b></span> So the good news is that running 90 copies of <tt>ps</tt> in parallel will also spike system usage &gt; 70%.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:33:33)</span> <b>tlmiller:</b></span> (To help make this noticeable and maybe better-simulate the procd, it's 90 copies in parallel, each one running ten times.)<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:36:43)</span> <b>tlmiller:</b></span> <pre>$ time seq -w 0 89 | parallel ./psauxw.sh<br/>...<br/>real    0m7.948s<br/>user    0m20.498s<br/>sys     10m58.046s</pre><br/>but<br/><pre>time seq -w 0 44 | parallel ./psauxw.sh<br/>real    0m3.394s<br/>user    0m11.190s<br/>sys     2m9.465s</pre><br/>
<span style="color: #d58247"><span style="font-size: small">(11:37:40)</span> <b>gthain:</b></span> I assume that if psaxuww.sh contains<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:37:58)</span> <b>tlmiller:</b></span> <pre>$ cat psauxw.sh<br/>#!/bin/bash<br/>ps auxw 2&gt;&amp;1 &gt; /dev/null<br/>ps auxw 2&gt;&amp;1 &gt; /dev/null<br/>ps auxw 2&gt;&amp;1 &gt; /dev/null<br/>ps auxw 2&gt;&amp;1 &gt; /dev/null<br/><br/>ps auxw 2&gt;&amp;1 &gt; /dev/null<br/>ps auxw 2&gt;&amp;1 &gt; /dev/null<br/>ps auxw 2&gt;&amp;1 &gt; /dev/null<br/>ps auxw 2&gt;&amp;1 &gt; /dev/null<br/><br/>ps auxw 2&gt;&amp;1 &gt; /dev/null<br/>ps auxw 2&gt;&amp;1 &gt; /dev/null</pre><br/><br/>
<span style="color: #d58247"><span style="font-size: small">(11:38:09)</span> <b>gthain:</b></span> <tt>unshare -Urpf --mount-proc /bin/ps auxww</tt><br/>
<span style="color: #d58247"><span style="font-size: small">(11:38:16)</span> <b>gthain:</b></span> things get much better?<br/>
<span style="color: #d58247"><span style="font-size: small">(11:38:30)</span> <b>gthain:</b></span> (presuming the mythical newer kernel)<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:38:39)</span> <b>tlmiller:</b></span> I can try that...<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:41:06)</span> <b>tlmiller:</b></span> <pre>$ time seq -w 0 89 | parallel ./psauxw-unshared.sh<br/>...<br/>real    0m0.853s<br/>user    0m0.995s<br/>sys     0m16.016s</pre><br/><br/>
<span style="color: #9e3997"><span style="font-size: small">(11:41:20)</span> <b>tlmiller:</b></span> Of course, each <tt>ps auxw</tt> is only seeing 1 process at that point.<br/>
<span style="color: #d58247"><span style="font-size: small">(11:41:33)</span> <b>gthain:</b></span> Still, theory seems to hold<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:42:29)</span> <b>tlmiller:</b></span> Well, given the exponential take-off in system time, I'm now wondering if there's just a scaling issue in the kernel/the <tt>/proc</tt> interface (rather than or possibly in addition to a spinlock).<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:42:46)</span> <b>tlmiller:</b></span> <pre>$ uname -a<br/>Linux <a href="http://e2460.chtc.wisc.edu">e2460.chtc.wisc.edu</a> 5.0.8-1.el7.elrepo.x86_64 #1 SMP Wed Apr 17 10:11:44 EDT 2019 x86_64 x86_64 x86_64 GNU/Linux</pre><br/>incidentally.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:44:09)</span> <b>tlmiller:</b></span> <pre>$ time seq -w 0 179 | parallel ./psauxw.sh<br/>...<br/>real    0m31.879s<br/>user    1m24.432s<br/>sys     51m12.440s</pre><br/><br/>
<span style="color: #9e3997"><span style="font-size: small">(11:46:13)</span> <b>tlmiller:</b></span> I guess a good follow-up experiment would be fork 45 sleeps jobs off in the background and make sure that 45 <tt>psauxw.sh</tt> jobs don't take appreciably longer.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:48:53)</span> <b>tlmiller:</b></span> Interesting.  It took noticeably longer, but nearly as much longer as 45 extra ps's:<br/><pre>real    0m5.571s<br/>user    0m21.026s<br/>sys     3m31.017s</pre><br/>(run again:)<br/><pre>real    0m5.008s<br/>user    0m21.350s<br/>sys     3m8.112s</pre><br/>... of course, some of this could interference from jobs.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:49:34)</span> <b>tlmiller:</b></span> ... yeah, it is.  Tests after the sleep jobs complete also take ~5 seconds.<br/>
<span style="color: #9e3997"><span style="font-size: small">(11:50:13)</span> <b>tlmiller:</b></span> I'm going to go have lunch.<br/>
<span style="color: #d58247"><span style="font-size: small">(15:44:43)</span> <b>gthain:</b></span> this looks suspicious:<br/>
<span style="color: #d58247"><span style="font-size: small">(15:44:46)</span> <b>gthain:</b></span> <pre>Removing environment config overides : _CONDOR_BIN _CONDOR_JOB_AD _CONDOR_HIGHPORT _CONDOR_CHIRP_CONFIG _CONDOR_SCRATCH_DIR _CONDOR_SLOT _CONDOR_ANCESTOR_22 _CONDOR_MACHINE_AD _CONDOR_LOWPORT _CONDOR_JOB_IWD _CONDOR_ANCESTOR_29 _CONDOR_JOB_PIDS _CONDOR_ANCESTOR_61882</pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(15:46:07)</span> <b>tlmiller:</b></span> Ayup.<br/>
<span style="color: #9e3997"><span style="font-size: small">(15:46:27)</span> <b>tlmiller:</b></span> Is that from the master or from something in Metronome or the test glue?<br/>
<span style="color: #d58247"><span style="font-size: small">(15:46:45)</span> <b>gthain:</b></span> remote_task.pl<br/>
<span style="color: #d58247"><span style="font-size: small">(15:46:57)</span> <b>gthain:</b></span> in the condor test glue<br/>
<span style="color: #9e3997"><span style="font-size: small">(15:47:12)</span> <b>tlmiller:</b></span> OK.  We may want to be a little more careful there. :)<br/>
<span style="color: #d58247"><span style="font-size: small">(15:48:23)</span> <b>gthain:</b></span> I assume that <tt>_CONDOR_ANCESTOR_*</tt> is the only pattern we need to keep<br/>
<span style="color: #9e3997"><span style="font-size: small">(15:48:49)</span> <b>tlmiller:</b></span> Probably?<br/>
<span style="color: #9e3997"><span style="font-size: small">(15:49:02)</span> <b>tlmiller:</b></span> (I don't know what <tt>_CONDOR_JOB_PIDS</tt> is for.)<br/>
<span style="color: #d58247"><span style="font-size: small">(15:49:54)</span> <b>gthain:</b></span> I believe it is used so that condor_ssh_to_job can say "your jobs are running with pids XXX" as a convenience for debugging<br/>
<span style="color: #9e3997"><span style="font-size: small">(15:50:14)</span> <b>tlmiller:</b></span> Ah, OK.<br/>
<span style="color: #d58247"><span style="font-size: small">(16:19:55)</span> <b>gthain:</b></span> The master shutdown hang is even stupider than I thought:<br/>
<span style="color: #d58247"><span style="font-size: small">(16:20:30)</span> <b>gthain:</b></span> <pre>06/14/21 12:00:41.189 (D_ALWAYS) Got SIGQUIT.  Performing fast shutdown.<br/>06/14/21 12:00:41.189 (D_ALWAYS) Sent SIGQUIT to COLLECTOR (pid 62219)<br/>06/14/21 12:00:41.189 (D_ALWAYS) Sent SIGQUIT to NEGOTIATOR (pid 62260)<br/>06/14/21 12:00:41.189 (D_ALWAYS) Sent SIGQUIT to SCHEDD (pid 62256)<br/>06/14/21 12:00:41.189 (D_ALWAYS) Sent SIGQUIT to STARTD (pid 62264)<br/>06/14/21 12:00:41.190 (D_ALWAYS) AllReaper unexpectedly called on pid 62260, status 0.<br/>06/14/21 12:00:41.190 (D_ALWAYS) The NEGOTIATOR (pid 62260) exited with status 0<br/>06/14/21 12:00:41.198 (D_ALWAYS) AllReaper unexpectedly called on pid 62219, status 0.<br/>06/14/21 12:00:41.198 (D_ALWAYS) The COLLECTOR (pid 62219) exited with status 0<br/>06/14/21 12:00:41.203 (D_ALWAYS) AllReaper unexpectedly called on pid 62264, status 0.<br/>06/14/21 12:00:41.203 (D_ALWAYS) The STARTD (pid 62264) exited with status 0<br/>06/14/21 12:00:41.366 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/14/21 12:00:41.366 (D_ALWAYS) Can't open directory "/etc/condor/passwords.d" as PRIV_ROOT, errno: 13 (Permission denied)<br/>06/14/21 12:05:18.261 (D_ALWAYS) condor_write(): Socket closed when trying to write 2083 bytes to collector 127.0.0.1:0, fd is 10<br/>06/14/21 12:05:18.261 (D_ALWAYS) Buf::write(): condor_write() failed<br/>06/14/21 12:05:18.268 (D_ALWAYS) condor_write(): Socket closed when trying to write 2091 bytes to collector 127.0.0.1:0, fd is 10</pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(16:23:20)</span> <b>tlmiller:</b></span> We really wait for _minutes_ before we notice that the socket was closed?<br/>(Also, why is reaper call "unexpected" if we just sent the collector a SIGQUIT?)<br/>
<span style="color: #d58247"><span style="font-size: small">(16:23:32)</span> <b>gthain:</b></span> We fear the reaper<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:23:41)</span> <b>tlmiller:</b></span> When does the collector log indicate that it actually finished dying?<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:23:49)</span> <b>tlmiller:</b></span> Dieing?<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:23:58)</span> <b>tlmiller:</b></span> Dyeing?<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:24:01)</span> <b>tlmiller:</b></span> Dying.<br/>
<blockquote>
<span style="color: #d58247"><span style="font-size: small">(16:25:24)</span> <b>gthain:</b></span> EXITING<br/>
</blockquote>
<span style="color: #d58247"><span style="font-size: small">(16:24:30)</span> <b>gthain:</b></span> <pre>06/14/21 12:00:41.189 (D_ALWAYS) Got SIGQUIT.  Performing fast shutdown.<br/>06/14/21 12:00:41.190 (D_ALWAYS) **** condor_collector (condor_COLLECTOR) pid 62219 EXITING WITH STATUS 0</pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(16:26:25)</span> <b>tlmiller:</b></span> That's... just dumb.<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:27:27)</span> <b>tlmiller:</b></span> Isn't the maximum TCP timeout actually two minutes?<br/>
<span style="color: #d58247"><span style="font-size: small">(16:30:39)</span> <b>gthain:</b></span> I think we start an async update right before killing the collector<br/>
<span style="color: #235e5b"><span style="font-size: small">(16:35:31)</span> <b>johnkn:</b></span> Yeah, I said this last week.  the master doesn't "know" that the COLLECTOR_HOST is the collector it's going to kill, so it sends an update to let the collector know it's going away.<br/>
<span style="color: #d58247"><span style="font-size: small">(16:40:22)</span> <b>gthain:</b></span> It sends the update before killing, though<br/>
<span style="color: #d58247"><span style="font-size: small">(16:40:38)</span> <b>gthain:</b></span> Which the collector got, in this case<br/>
<span style="color: #235e5b"><span style="font-size: small">(16:42:06)</span> <b>johnkn:</b></span> so you're saying that the master is sending more bytes *after* it removes it's ad from the collector?<br/>
<span style="color: #d58247"><span style="font-size: small">(16:42:44)</span> <b>gthain:</b></span> That's what it looks like<br/>
<span style="color: #235e5b"><span style="font-size: small">(16:43:31)</span> <b>johnkn:</b></span> hmm.  something messed up in the internal machinery.<br/>
<span style="color: #9e3997"><span style="font-size: small">(16:47:47)</span> <b>tlmiller:</b></span> Does the master send the ad before killing the collector, or does daemon core?  I'm wondering if maybe daemon core sends the collector an ad on the way out, because it always does.<br/>
<span style="color: #d58247"><span style="font-size: small">(16:52:15)</span> <b>gthain:</b></span> master sends the ad<br/>
<span style="color: #235e5b"><span style="font-size: small">(18:23:40)</span> <b>johnkn:</b></span> Oh, our byzantine ways.<br/>1. master receives a condor_off -master command<br/>2. calls daemoncore::Send_Signal<br/>3. which sees that the destination is itself, so it converts that to a call to HandleSig()<br/>4. HandleSig calls daemoncore's handler - handle_dc_sigquit()<br/>5. handle_dc_sigquit() calls the the main_shutdown_fast handler that the master registered<br/>6. which<br/>    a. invalidates the master ad<br/>    b. sets a flag to indicate it's in shutdown mode<br/>    c. kills child daemons in (sort of) order.  startds first, shared port last.<br/>    d. stops the procd<br/>    e. exits<br/>
<span style="color: #235e5b"><span style="font-size: small">(18:23:54)</span> <b>johnkn:</b></span> Step 6.c. can take a while.<br/>
<span style="color: #d58247"><span style="font-size: small">(18:25:53)</span> <b>gthain:</b></span> Where does it sort the order of the daemons it kills -- note the log above -- first collector, negotiator, schedd then startd<br/>
<span style="color: #d58247"><span style="font-size: small">(18:27:16)</span> <b>gthain:</b></span> Note that we only use the flag in restart<br/>
<span style="color: #235e5b"><span style="font-size: small">(18:27:22)</span> <b>johnkn:</b></span> I think that's probably startup order, set when it builds the internal daemon list.<br/>
<span style="color: #d58247"><span style="font-size: small">(18:27:55)</span> <b>gthain:</b></span> at boot time, it shuffles the collector to the beginning of the list, but I think it uses that order at shutdown<br/>
<span style="color: #235e5b"><span style="font-size: small">(18:31:00)</span> <b>johnkn:</b></span> most likely, with the machinery to ignore the daemon list order for a few daemons<br/>
<span style="color: #235e5b"><span style="font-size: small">(18:32:25)</span> <b>johnkn:</b></span> huh.<br/>
<span style="color: #235e5b"><span style="font-size: small">(18:32:36)</span> <b>johnkn:</b></span> looks like it's alphabetical.<br/>
<span style="color: #235e5b"><span style="font-size: small">(18:33:04)</span> <b>johnkn:</b></span> StopFastAllDaemons iterates a <tt>std::map&lt;name, daemon&gt;</tt><br/>
<span style="color: #235e5b"><span style="font-size: small">(18:35:49)</span> <b>johnkn:</b></span> So clearly if we put VIEW_COLLECTOR in the list instead of COLLECTOR.  we fix the shutdown order :slightly_smiling_face:<br/>
</body>
</html>

<!DOCTYPE html>
<html>
<head>
<title>Tue Aug 3, 2021 : #batlab (chtc)</title>
</head>
<body>
<h3>Tue Aug 3, 2021 : #batlab (chtc)</h3>
<span style="color: #bb86b7"><span style="font-size: small">(17:24:10)</span> <b>tim:</b></span> The most recent build on BaTLab has issues with mulitple Docker jobs failing to launch. It looked like 6 jobs were downloading Docker image and probably timed out for some reason. I would have expected those image to already be in the local cache. Do we have to make an adjustment there?<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:28:41)</span> <b>tlmiller:</b></span> Sounds like.<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:30:54)</span> <b>tlmiller:</b></span> There seems to be plenty of space, and the <tt>docker images</tt> output looks pretty complete, though.<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:31:32)</span> <b>tlmiller:</b></span> (Has it really been five months since the last change to the Fedora32 and CentOS7/8 images?)<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:31:44)</span> <b>tlmiller:</b></span> ... so I don't know what the change would be.<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:33:02)</span> <b>tlmiller:</b></span> ... let me look at the Metronome logs for a bit.  I'm not sure those jobs ever even tried to start...<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:34:51)</span> <b>tlmiller:</b></span> ... although DAGMan thinks it did, there's no platform_job log:<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:35:09)</span> <b>tlmiller:</b></span> <pre>08/03/21 16:32:41 (D_ALWAYS) Event: ULOG_EXECUTE for HTCondor Node platform_pre.nmi-zdev:x86_64_Ubuntu20 (759158.0.0) {08/03/21 16:32:36}<br/>08/03/21 16:32:41 (D_ALWAYS) Number of idle job procs: 2<br/>08/03/21 16:32:41 (D_ALWAYS) Event: ULOG_JOB_TERMINATED for HTCondor Node platform_pre.nmi-zdev:x86_64_Ubuntu20 (759158.0.0) {08/03/21 16:32:36}</pre><br/><br/>
<span style="color: #9e3997"><span style="font-size: small">(17:35:30)</span> <b>tlmiller:</b></span> Ack!  That's the wrong job.  Please ignore.<br/>
<span style="color: #bb86b7"><span style="font-size: small">(17:37:44)</span> <b>tim:</b></span> I released those jobs. The hold messages were:<br/>
<span style="color: #bb86b7"><span style="font-size: small">(17:38:25)</span> <b>tim:</b></span> <pre>-- Schedd: <a href="http://batlabsubmit0001.chtc.wisc.edu">batlabsubmit0001.chtc.wisc.edu</a> : &lt;128.104.100.94:9618?... @ 08/03/21 17:11:11<br/> ID        OWNER          HELD_SINCE  HOLD_REASON<br/>759155.0   condorauto      8/3  16:33 Error from <a href="mailto:slot1_3@e2460.chtc.wisc.edu">slot1_3@e2460.chtc.wisc.edu</a>: Unable to find image 'htcondor/nmi-zdev:x86_64_CentOS8-09010005' locally x86_64_CentOS8-09010005: Pulling from htcondor/nmi-zdev 7a0437f04f83: Already exists 1d2031000890: Pulling fs layer c82cd0e8e81d: Pulling fs layer 767e6133c6c7: Pulling fs layer ee57f163b2e9: Pulling fs layer a5e04f42e11b: Pulling fs layer 38e370936535: Pulling fs layer e30a014e339c: Pulling fs layer 41e58f2c2da7: Pulling fs layer fda5631a3174: Pulling fs layer 21aa6325029e: Pulling fs layer 0df892c859bb: Pulling fs layer c6227215c149<br/>759156.0   condorauto      8/3  16:33 Error from <a href="mailto:slot1_4@e2460.chtc.wisc.edu">slot1_4@e2460.chtc.wisc.edu</a>: Unable to find image 'htcondor/nmi-zdev:x86_64_Debian10-09010005' locally x86_64_Debian10-09010005: Pulling from htcondor/nmi-zdev 0bc3020d05f1: Pulling fs layer 74fb1e6bb17b: Pulling fs layer 1a2f914077f6: Pulling fs layer 23c8650045e8: Pulling fs layer eb39f5e07e1e: Pulling fs layer a7e572f64f6c: Pulling fs layer 0bc3020d05f1: Waiting 733c37f777d3: Pulling fs layer 74fb1e6bb17b: Waiting 5c675e544063: Pulling fs layer 1a2f914077f6: Waiting e2dbc18c1f5e: Pulling fs layer 78dac0c5db97: Pulling fs layer a7e5<br/>759159.0   condorauto      8/3  16:33 Error from <a href="mailto:slot1_5@e2460.chtc.wisc.edu">slot1_5@e2460.chtc.wisc.edu</a>: Unable to find image 'htcondor/nmi-zdev:x86_64_Debian9-09000100' locally x86_64_Debian9-09000100: Pulling from htcondor/nmi-zdev 199ebcd83264: Pulling fs layer fb7a89091a82: Pulling fs layer 68207ab9e18c: Pulling fs layer ad5b266ed2b7: Pulling fs layer c6d414de0e34: Pulling fs layer 1ae6d0e8d901: Pulling fs layer 0dc9ce1d372b: Pulling fs layer 428d8d3d423c: Pulling fs layer 7aaf106b6a1c: Pulling fs layer 53c5f75ad8fa: Pulling fs layer b09f45017dbb: Pulling fs layer fb7a89091a82: Waiting e1f8ea4c5613: Pulli<br/>759160.0   condorauto      8/3  16:33 Error from <a href="mailto:slot1_6@e2460.chtc.wisc.edu">slot1_6@e2460.chtc.wisc.edu</a>: Unable to find image 'htcondor/nmi-zdev:x86_64_Fedora32-09010005' locally x86_64_Fedora32-09010005: Pulling from htcondor/nmi-zdev de2fa1c557bb: Pulling fs layer 4c81f8e659ac: Pulling fs layer 920ed7d8e153: Pulling fs layer 1090bb89dd37: Pulling fs layer 4cf44d295b7f: Pulling fs layer a41ce158fa18: Pulling fs layer 383051e9cdd6: Pulling fs layer 12d1b824b84c: Pulling fs layer 5e88c7ddd30a: Pulling fs layer 3320c0cf8fed: Pulling fs layer 920ed7d8e153: Waiting 2be7b666a268: Pulling fs layer 4c81f8e659ac: Wai<br/>759163.0   condorauto      8/3  16:33 Error from <a href="mailto:slot1_7@e2460.chtc.wisc.edu">slot1_7@e2460.chtc.wisc.edu</a>: Unable to find image 'htcondor/nmi-zdev:x86_64_Ubuntu18-09010005' locally x86_64_Ubuntu18-09010005: Pulling from htcondor/nmi-zdev 25fa05cd42bd: Pulling fs layer c55489fb95f0: Pulling fs layer ac14d9dc4ef8: Pulling fs layer 7db9785ab870: Pulling fs layer 25fa05cd42bd: Waiting 7f82d5d63f59: Pulling fs layer c55489fb95f0: Waiting 3a39095f4930: Pulling fs layer 84fdbc6c0760: Pulling fs layer 87314052f91a: Pulling fs layer 88e58f8e9cd4: Pulling fs layer 7db9785ab870: Waiting 3a39095f4930: Waiting 525cf591dda3:<br/>759164.0   condorauto      8/3  16:33 Error from <a href="mailto:slot1_8@e2460.chtc.wisc.edu">slot1_8@e2460.chtc.wisc.edu</a>: Unable to find image 'htcondor/nmi-zdev:x86_64_Ubuntu20-09010005' locally x86_64_Ubuntu20-09010005: Pulling from htcondor/nmi-zdev c549ccf8d472: Pulling fs layer 2f2c2aa4bf43: Pulling fs layer 3e949366015d: Pulling fs layer 9989d1c2c367: Pulling fs layer 2426135dbc22: Pulling fs layer 02dad9b3c444: Pulling fs layer 0edc48275e3a: Pulling fs layer 6b9aee254e40: Pulling fs layer 34cc397069f3: Pulling fs layer f34867b112d3: Pulling fs layer 3e949366015d: Waiting 324f747f5b6a: Pulling fs layer 9989d1c2c367: Wai<br/><br/>6 jobs; 0 completed, 0 removed, 0 idle, 0 running, 6 held, 0 suspended</pre><br/>
<span style="color: #9e3997"><span style="font-size: small">(17:38:26)</span> <b>tlmiller:</b></span> &gt;  08/03/21 16:33:06 (pid:72008) (D_ALWAYS) Found 49 entries in docker image cache.<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:38:36)</span> <b>tlmiller:</b></span> Apparently I'm looking in the wrong place, since I only saw, like 8...<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:38:58)</span> <b>tlmiller:</b></span> ... that's one heck of a hold message.<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:39:31)</span> <b>tlmiller:</b></span> Found the following in the starter log:<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:39:34)</span> <b>tlmiller:</b></span> <pre>08/03/21 16:33:38 (pid:72008) (D_ALWAYS) Docker invocation '/usr/bin/docker kill --signal 9 HTCJob759164_0_slot1_8_PID72008' failed, printing first few lines of output.<br/>08/03/21 16:33:38 (pid:72008) (D_ALWAYS) Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</pre><br/><br/>
<span style="color: #9e3997"><span style="font-size: small">(17:40:00)</span> <b>tlmiller:</b></span> (I believe that 759164 is the proc ID of the Ubuntu 20 job in question.)<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:40:23)</span> <b>tlmiller:</b></span> Let me check slot1_3...<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:41:15)</span> <b>tlmiller:</b></span> <pre>08/03/21 16:33:33 (pid:64801) (D_ALWAYS) Got SIGQUIT.  Performing fast shutdown.<br/>08/03/21 16:33:33 (pid:64801) (D_ALWAYS) ShutdownFast all jobs.<br/>08/03/21 16:33:33 (pid:64801) (D_ALWAYS) DockerProc::ShutdownFast() container 'HTCJob759155_0_slot1_3_PID64801'<br/>08/03/21 16:33:38 (pid:64801) (D_ALWAYS) Docker invocation '/usr/bin/docker kill --signal 9 HTCJob759155_0_slot1_3_PID64801' failed, printing first few lines of output.<br/>08/03/21 16:33:38 (pid:64801) (D_ALWAYS) Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?</pre><br/><br/>
<span style="color: #bb86b7"><span style="font-size: small">(17:41:28)</span> <b>tim:</b></span> Well there were 6 hold message there.<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:41:43)</span> <b>tlmiller:</b></span> Wonder why the startd was telling the starter to shut down?<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:41:49)</span> <b>tlmiller:</b></span> ... I'm going blind, too, apparently.<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:41:50)</span> <b>tlmiller:</b></span> *Sigh*<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:42:44)</span> <b>tlmiller:</b></span> ...<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:42:47)</span> <b>tlmiller:</b></span> <pre>08/03/21 16:33:04 (D_ALWAYS) slot1_8: Changing activity: Idle -&gt; Busy<br/>08/03/21 16:33:10 (D_ALWAYS) OfflineUniverses = {}<br/>08/03/21 16:33:11 (D_ALWAYS) OfflineUniverses = {}<br/>08/03/21 16:33:33 (D_ALWAYS) Got SIGQUIT.  Performing fast shutdown.</pre><br/><br/>
<span style="color: #9e3997"><span style="font-size: small">(17:43:56)</span> <b>tlmiller:</b></span> and from the MasterLog:<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:43:58)</span> <b>tlmiller:</b></span> <pre>08/03/21 16:33:33 (D_ALWAYS) Got SIGQUIT.  Performing fast shutdown.</pre><br/><br/>
<span style="color: #9e3997"><span style="font-size: small">(17:44:33)</span> <b>tlmiller:</b></span> Ah.<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:44:37)</span> <b>tlmiller:</b></span> Machine's only been up 51 minutes.<br/>
<span style="color: #bb86b7"><span style="font-size: small">(17:46:41)</span> <b>tim:</b></span> Ok a reboot problem. Still seems a good idea to expand the Docker cache.<br/>
<span style="color: #9e3997"><span style="font-size: small">(17:47:58)</span> <b>tlmiller:</b></span> Concur.  I have no idea how to change that, or even to check it, though. :/<br/>
</body>
</html>

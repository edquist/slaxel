<!DOCTYPE html>
<html>
<head>
<title>Fri Aug 20, 2021 : #operations (chtc)</title>
</head>
<body>
<h3>Fri Aug 20, 2021 : #operations (chtc)</h3>
<span style="color: #9f69e7"><span style="font-size: small">(09:53:07)</span> <b>lmichael:</b></span> @tjslauson I’ll make a ticket once we’ve settled on the details, but first:<br/>1. Can you confirm our current per-user running cores limit on the HPC Cluster? (and that I’m correct that our setting is a per-user limit, across all/any partitions)<br/>2. If we wanted to give members of the Szlufarska group the ability to run across more cores in their own partitions, is there a way to do that without allowing them to run with that many cores across all partitions? Can we set a higher limit for specific users? Specific partitions?<br/>Thanks! User’s question is here: <a href="https://crt.cs.wisc.edu/rt/Ticket/Display.html?id=103961">https://crt.cs.wisc.edu/rt/Ticket/Display.html?id=103961</a><br/>
<blockquote>
<span style="color: #4bbe2e"><span style="font-size: small">(10:57:45)</span> <b>tjslauson:</b></span> re #1, here's the relevant info from your own Slurm association (this info is public to anyone with an account on the cluster):<br/>
<span style="color: #4bbe2e"><span style="font-size: small">(10:57:54)</span> <b>tjslauson:</b></span> <br/>
<span style="color: #4bbe2e"><span style="font-size: small">(10:58:48)</span> <b>tjslauson:</b></span> those values are typical for most of our HPC users (users we haven't done any customization on).  by default we set a limit of 720 running cores on each individual user association, raised from 600 a couple years ago.  You have the ability to override this and set any value for cpu per-user in the userapp.  Note that the Partition field in the output above is empty, because most associations we've made aren't constrained to a particular partition.  That is, they're able to apply regardless of which partition a user with such an association selects.<br/>
<span style="color: #4bbe2e"><span style="font-size: small">(10:59:51)</span> <b>tjslauson:</b></span> re #2, the way I'm thinking of would probably work, but I'd want to test it out before making any stronger claim (it could be a user testing it, I'm fine w/ that)<br/>
<span style="color: #4bbe2e"><span style="font-size: small">(11:00:52)</span> <b>tjslauson:</b></span> Slurm associations are uniquely identified by a 4-tuple: (cluster, bankaccount, user, partition).  For us "cluster" is always "hpc", "user" is always a Slurm (same as Linux) username, and "bankaccount" usually corresponds to a lowercase accounting group.  We usually only specify the first 3 parts of the tuple, letting the partition component be a free variable.  If you make a new Slurm association specifying partition as well, you could specify a different CPU limit for that association.<br/>
<span style="color: #4bbe2e"><span style="font-size: small">(11:01:24)</span> <b>tjslauson:</b></span> the part I'm unsure of without reading/testing more is what precedence that more-specific association would have.  I'd assume the most specific association would be used if it was the best match for the submitted job, but there's also the concept of a default association that could throw a wrench into that, etc etc.<br/>
<span style="color: #4bbe2e"><span style="font-size: small">(11:03:48)</span> <b>tjslauson:</b></span> it may also be possible to specify (cluster, bankaccount, partition) and omit the user component to catch all the Szlufarska users using just one new association.  I haven't worked much with non-hierarchical Slurm associations yet, so there could be fairshare implications I haven't thought about.<br/>
<span style="color: #4bbe2e"><span style="font-size: small">(11:04:36)</span> <b>tjslauson:</b></span> @lmichael ^^<br/>
<span style="color: #9f69e7"><span style="font-size: small">(12:51:07)</span> <b>lmichael:</b></span> Okay. I read the above to mean: 1) we can increase the limit per-user, but that would allow the same user to run on that many cores across partitions (so even just within the ‘univ2’ partition). and 2) you’d need to dig a bit more to limit just certain users for certain partitions. Part of what I’m worried about is that the same user could have (both) 720 cores in use in ‘univ2’ AND have a higher number of cores across ‘szlufarska’ partitions, which would REALLY game the system.<br/><br/>*Maybe ideal*: if they have ANY jobs running in a shared partition, then they are limited to 720 total. If they only have jobs running in their own partition, they’re unlimited. Can you do that/look into it?<br/>
<span style="color: #9f69e7"><span style="font-size: small">(2021-08-23 12:56:29)</span> <b>lmichael:</b></span> ping for when you get back to this thread, @tjslauson<br/>
<span style="color: #4bbe2e"><span style="font-size: small">(2021-08-23 13:45:16)</span> <b>tjslauson:</b></span> yup, it's still on my radar.  I just got a bunch of non-HPC stuff in the last couple hours.<br/>
<span style="color: #4bbe2e"><span style="font-size: small">(2021-08-23 13:45:23)</span> <b>tjslauson:</b></span> I won't be able to do anything with this today<br/>
</blockquote>
<span style="color: #e7392d"><span style="font-size: small">(14:16:00)</span> <b>ckoch5:</b></span> why does this error happen?<br/><pre>Failed to get response to CREATE_JOB_OWNER_SEC_SESSION from starter</pre><br/>
<span style="color: #684b6c"><span style="font-size: small">(14:27:51)</span> <b>bbockelman:</b></span> My estimates:<br/>• 20% chance it's because of some network disconnect to the starter.<br/>• 80% chance it's an "authorization denied" because authorization denied is indistinguishable from a network disconnect.<br/>
<span style="color: #e7392d"><span style="font-size: small">(14:29:45)</span> <b>ckoch5:</b></span> I’ll note that this happened to me 1-2x on Tues when trying to do an interactive job on the GPU nodes, and that’s what’s happening to this user also.<br/>
<span style="color: #e7392d"><span style="font-size: small">(14:30:00)</span> <b>ckoch5:</b></span> My sense is that it’s transient and you should just try again. make sense?<br/>
<span style="color: #684b6c"><span style="font-size: small">(14:33:30)</span> <b>bbockelman:</b></span> ah, if it's not a glidein, my probability estimates change.  There's something likely interesting in the starter log then.<br/>
<span style="color: #e7392d"><span style="font-size: small">(14:41:50)</span> <b>ckoch5:</b></span> but is my recommendation still good then? just try again?<br/>
<span style="color: #684b6c"><span style="font-size: small">(14:43:21)</span> <b>bbockelman:</b></span> yes, that's fine.  But we should also dig up the underlying cause if it's only a minute or so of work.<br/>
<span style="color: #5a4592"><span style="font-size: small">(15:59:32)</span> <b>tannenba:</b></span> @blin @bbockelman @wiscmoate Hey folks....  Where can I find startd/starter logs for CHTC Glidein nodes?  (E.g., the stderr files from the factory for GLOW pilot jobs).  Same place as for the OSPool on xd-login or somewhere else?  I am guessing the latter... Thanks!<br/>
<span style="color: #5a4592"><span style="font-size: small">(15:59:47)</span> <b>tannenba:</b></span> (tracking down a problem for @lmichael)<br/>
<span style="color: #4cc091"><span style="font-size: small">(16:35:55)</span> <b>blin:</b></span> hrm, i'm not sure if they're rsync'ed over. Jeff Dost would know for sure<br/>
<span style="color: #3c989f"><span style="font-size: small">(16:42:13)</span> <b>wiscmoate:</b></span> @blin @tannenba <tt><a href="http://glidein-cm.chtc.wisc.edu">glidein-cm.chtc.wisc.edu</a>:/var/log/gfaclogs</tt><br/>
<span style="color: #5a4592"><span style="font-size: small">(16:42:33)</span> <b>tannenba:</b></span> Thanks!<br/>
<span style="color: #3c989f"><span style="font-size: small">(16:43:01)</span> <b>wiscmoate:</b></span> <pre>  # Freshdesk ticket #66784: Create service to rsync GLOW factory logs to CHTC<br/>  # <a href="https://support.opensciencegrid.org/a/tickets/66784">https://support.opensciencegrid.org/a/tickets/66784</a><br/>  # ticket #102870 glidein-cm out of disk space<br/>  # <a href="https://crt.cs.wisc.edu/rt/Ticket/Display.html?id=102870">https://crt.cs.wisc.edu/rt/Ticket/Display.html?id=102870</a></pre><br/>
<span style="color: #3c989f"><span style="font-size: small">(16:43:02)</span> <b>wiscmoate:</b></span> :wink:<br/>
</body>
</html>
